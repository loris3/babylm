{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# scp -r loriss21cs@slurm01.vda.univie.ac.at:/srv/home/users/loriss21cs/babylm/mean_influence mean_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = \"./mean_influence_divided_by_len_test/OLMo-2-1124-7B-SFT\"\n",
    "# for dir in tqdm(list(os.listdir(input))):\n",
    "#     dataset_name = None\n",
    "#     split = None\n",
    "#     print(dir)\n",
    "#     l = dir.split(\"]_\")[-1]\n",
    "  \n",
    "#     if \"train\" in l:\n",
    "#         l = l.split(\"_train[\")\n",
    "#         dataset_name = \"anasedova/\"+l[0]\n",
    "#         split = \"train[\"+l[1]\n",
    "#     else:\n",
    "#         l = l.split(\"_test[\")\n",
    "#         dataset_name = \"anasedova/\"+l[0]\n",
    "#         split = \"test[\"+l[1]\n",
    "\n",
    "    \n",
    "#     d = load_dataset(dataset_name, split=split)\n",
    "#     print(dir, dataset_name, split, len(d))\n",
    "#     path_in = os.path.join(input, dir, \"main\")\n",
    "#     path_out = os.path.join(\"./mean_influence/OLMo-2-1124-7B-SFT\", dir, \"main\")\n",
    "#     os.makedirs(os.path.dirname(path_out))\n",
    "#     torch.save(torch.load(path_in,weights_only=True,map_location=\"cpu\") * len(d), path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"./mean_influence/OLMo-2-1124-7B-SFT/\"\n",
    "\n",
    "\n",
    "empty_subfolders = [\n",
    "    os.path.join(root, d) for root, dirs, files in os.walk(directory) for d in dirs\n",
    "    if not os.listdir(os.path.join(root, d)) \n",
    "]\n",
    "\n",
    "for folder in empty_subfolders:\n",
    "    if \"dev\" not in folder:\n",
    "        print(\"rmdir\",folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./mean_influence/OLMo-2-1124-7B-SFT/\"\n",
    "prefix = \"tulu-3-sft-olmo-2-mixture_train[26%:27%]_\"\n",
    "test_sets = [(\"anasedova/\"+\"_\".join(dir.replace(prefix,\"\").split(\"_\")[0:-1]), dir.replace(prefix,\"\").split(\"_\")[-1]) for dir in  os.listdir(directory) if prefix in dir]\n",
    "\n",
    "test_sets.append((\"anasedova/olmes_tulu_3_unseen\", \"test[0%:100%]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "schema = datasets.load_from_disk(\"./verb_confidence_exp_t/retrieved_documents/nq_llama8b-sft_test\").features\n",
    "\n",
    "folder = \"./verb_confidence_exp_t/retrieved_documents\"\n",
    "candidate_schema = None \n",
    "for candidate_dataset in os.listdir(folder):\n",
    "    candidate_schema = datasets.load_from_disk(os.path.join(folder,candidate_dataset))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = datasets.load_from_disk(\"./verb_confidence_exp_t/retrieved_documents/nq_llama8b-sft_test\").features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query_col in [key for key in schema.keys() if \"_query\" in key]:\n",
    "#     schema[query_col] = schema[\"response_query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = datasets.load_from_disk(\"./verb_confidence_exp_t/retrieved_documents/nq_llama8b-sft_test\")\n",
    "# dataset = dataset.map(features.encode_example, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(error).difference(fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(fine).difference(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del test_sets[test_sets.index(('anasedova/tulu_3_no_errors', 'train[0%100%]'))]\n",
    "# del test_sets[test_sets.index(('anasedova/tulu-3-sft-mixture', 'train[0%100%]'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sets = [t for t in test_sets if \"_tulu_3_formatting_errors_train\" not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "checkpoint = \"main\"\n",
    "\n",
    "df_all_errors = load_dataset(\"anasedova/tulu_3_all_errors_upd\", split=\"train\").select_columns([\"ordinal_id\", \"noise_type\"])\n",
    "dff = load_dataset(\"allenai/tulu-3-sft-olmo-2-mixture\", split=\"train[0%:100%]\").select_columns([\"id\"]).to_pandas()\n",
    "def load_data(train_dataset_name, dataset_train_split, test_dataset_name, dataset_test_split, model_name, influence_output_dir=\"./mean_influence\", checkpoint=\"main\"):\n",
    "\n",
    "    assert train_dataset_name == \"allenai/tulu-3-sft-olmo-2-mixture\", \"move loading inside function\"\n",
    "    influence_output_dir = os.path.join(influence_output_dir, os.path.basename(model_name))\n",
    " \n",
    "   # dataset = load_dataset(train_dataset_name, split=dataset_train_split).select_columns([\"id\"])\n",
    "    # len_dataset_test = len(load_dataset(test_dataset_name, split=dataset_test_split))\n",
    "    \n",
    "    pattern = re.compile(rf\"{re.escape(os.path.basename(train_dataset_name))}_.*_{re.escape(os.path.basename(test_dataset_name))}\")\n",
    "\n",
    "    train_splits = [dataset_train_split.split(\"[\")[0] + f\"[{i}%:{i + 100 // 100}%]\" for i in range(0, 100, 100 // 100)]\n",
    "\n",
    "    files = [file for file in os.listdir(influence_output_dir) if pattern.search(file)]\n",
    "\n",
    "    # verify all files are ready\n",
    "    for train_split in train_splits:\n",
    "        if not(any([os.path.basename(train_dataset_name) + \"_\" + train_split in file for file in files])):\n",
    "            print(f\"python process_gradients.py {model_name} {train_dataset_name} {0} --dataset_train_split={train_split} --dataset_test={test_dataset_name} --dataset_test_split={dataset_test_split} --mode=mean  --batch_size=10\")\n",
    "            raise FileNotFoundError\n",
    "        \n",
    "        \n",
    "\n",
    "    for result_superbatch in files:\n",
    "        try:\n",
    "            torch.load(os.path.join(influence_output_dir,result_superbatch, checkpoint),weights_only=True,map_location=\"cpu\")\n",
    "        except:\n",
    "            print(result_superbatch)\n",
    "    \n",
    "\n",
    "    idx, results_superbatches = zip(*sorted([(int(re.search(r\"\\[(\\d*)\\%\", result_superbatch).group(1)), torch.load(os.path.join(influence_output_dir,result_superbatch, checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten()) for result_superbatch in files], key=lambda x: x[0]))\n",
    "\n",
    "\n",
    "\n",
    "    influence_scores = np.concat(results_superbatches)\n",
    "    # influence_scores = influence_scores * len_dataset_test # TODO hotfix\n",
    "\n",
    "    df = pd.DataFrame(influence_scores)\n",
    "    df.columns = [\"influence\"]\n",
    "\n",
    "    df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "\n",
    "    assert len(dff) == len(df)\n",
    "\n",
    "\n",
    "\n",
    "    df[dff.columns] = dff  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # TODO hotfix as old influence datasets lack noise_type col\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    if train_dataset_name == \"allenai/tulu-3-sft-olmo-2-mixture\":\n",
    "        # assuming \"ds\" is 1:1 allenai/tulu-3-sft-olmo-2-mixture  \n",
    "        a = [\"no\"] * len(dff)\n",
    "        for row in df_all_errors:\n",
    "            a[row[\"ordinal_id\"]] = row[\"noise_type\"]\n",
    "        ds = ds.add_column(\"noise_type\", a)\n",
    "    return ds\n",
    "\n",
    "dfs = []\n",
    "for i, (test_dataset_name, test_split) in enumerate(test_sets):\n",
    "    print(i)\n",
    "    ds = load_data(\n",
    "        \"allenai/tulu-3-sft-olmo-2-mixture\", \n",
    "        \"train[0%:100%]\",\n",
    "        test_dataset_name,\n",
    "        test_split,\n",
    "        \"allenai/OLMo-2-1124-7B-SFT\",\n",
    "        )\n",
    "    df = ds.to_pandas()\n",
    "    df[\"test_dataset\"] = test_dataset_name\n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "grouped = df.groupby([\"test_dataset\", \"noise_type\"])[\"influence\"].mean().reset_index()\n",
    "\n",
    "\n",
    "sns.barplot(data=grouped, x='test_dataset', y='influence', hue='noise_type', palette=\"Set2\")\n",
    "\n",
    "plt.title(\"Mean Influence by Noise Type and Test Dataset\")\n",
    "plt.xlabel(\"Test Dataset\")\n",
    "plt.ylabel(\"Mean Influence of category\")\n",
    "plt.legend(title='Noise Type')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"test_dataset\"] == \"anasedova/tulu_3_incorrect_output_errors\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "unique_datasets = df[\"test_dataset\"].unique()\n",
    "num_datasets = len(unique_datasets)\n",
    "\n",
    "fig, axes = plt.subplots(num_datasets, 1, figsize=(12, 3 * num_datasets), sharex=True)\n",
    "\n",
    "\n",
    "for ax, (name, df_) in zip(axes, df.groupby(by=\"test_dataset\")):\n",
    "    df_.groupby(by=\"noise_type\")[\"influence\"].hist(ax=ax, legend=True, bins=500, density=True, alpha=0.5)\n",
    "    ax.set_title(f\"Test Dataset: {name}\")\n",
    "    ax.set_xlim(df[\"influence\"].min(), df[\"influence\"].max() * 0.3)\n",
    "    ax.set_ylim(0,100000)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afdyfrefdf.groupby(by=\"noise_type\")[\"influence\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=\"noise_type\")[\"influence\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model = smf.ols(\"influence ~ C(noise_type)\", data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(df[\"influence\"], df[\"noise_type\"], alpha=0.05)\n",
    "\n",
    "print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "for name, df_ in df.groupby(by=\"test_dataset\"):\n",
    "    tukey_results = pairwise_tukeyhsd(df_[\"influence\"], df_[\"noise_type\"], alpha=0.05)\n",
    "    print(name)\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "\n",
    "ks_results = []\n",
    "for a, b in itertools.combinations( df[\"group\"].unique(), 2):\n",
    "    group1 = df[df[\"group\"] == a][\"influence\"]\n",
    "    group2 = df[df[\"group\"] == b][\"influence\"]\n",
    "    # print(len(group1), len(group2))\n",
    "    ks_stat, p_value = ks_2samp(group1, group2)\n",
    "    ks_results.append([a, b, ks_stat, p_value, p_value < 0.05])\n",
    "pd.DataFrame(ks_results, columns=[\"group1\", \"group2\", \"KS-stat\", \"p-adj\", \"reject\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can now access the mean influence of any noise_category on any of the datasets, e.g.,\n",
    "\n",
    "- \"the mean influence underspecified_input[123] has on examples in olmes\"\n",
    "- \"the mean influence examples from underspecified_input has on examples in olmes\" `df.groupby(\"noise_type\").mean()`\n",
    "- \"the mean influence of examples in tulu if prompted with examples from underspecified_input\"\n",
    "- etc\n",
    "\n",
    "What combinations do you want (there are 7*7)?\n",
    "Should I just provide you one big dataframe with \"influence\", \"example a\", \"noise type a\", \"example b\", \"noise type b\" (noise_type is \"no\" if not present in datasets or from olmes)?\n",
    "So that you can compare e.g. group means?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
