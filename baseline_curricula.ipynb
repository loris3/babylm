{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "DEBUG = False\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "from baseline_curricula import upload, split_into_epochs, repeat_for_ten_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./difficulty_ngram/1/babylm_2024_10m_curriculum\n",
      "Skipping ./difficulty_ngram/1/babylm_2024_10m_curriculum, already calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "from baseline_curricula import get_perplexity_ngram\n",
    "\n",
    "for dataset_name in config.datasets:\n",
    "    perplexity_df = get_perplexity_ngram(dataset_name)\n",
    "    upload(\"perplexity_increasing.pt\", repeat_for_ten_epochs(perplexity_df[\"perplexity\"].sort_values(ascending=True).index.to_list()), dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./difficulty_mattr/babylm_2024_10m_curriculum\n",
      "Skipping ./difficulty_mattr/babylm_2024_10m_curriculum, already calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "from baseline_curricula import get_mattr\n",
    "\n",
    "\n",
    "for dataset_name in config.datasets:\n",
    "    mattr_df = get_mattr(dataset_name, n_proc=128)\n",
    " #   upload(\"perplexity_decreasing.pt\", split_into_epochs(perplexity_df[\"perplexity\"].sort_values(ascending=False).index.to_list()), dataset)\n",
    "    upload(\"mattr_increasing.pt\", repeat_for_ten_epochs(mattr_df[\"mattr\"].sort_values(ascending=True).index.to_list()), dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    }
   ],
   "source": [
    "from baseline_curricula import get_mattr\n",
    "import torch\n",
    "\n",
    "\n",
    "for dataset_name in config.datasets:\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    random_order = [torch.randperm(len(dataset[\"train\"])) for _ in range(0,EPOCHS)]\n",
    "    \n",
    "    upload(\"random.pt\", random_order, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import validate_training_duration_limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1: Child Directed Speech\n",
      "0 2839591\n",
      "1 5679182\n",
      "2 8518773\n",
      "3 11358364\n",
      "4 14197955\n",
      "5 17037546\n",
      "6 19877137\n",
      "7 20000002\n",
      "C2: Unscripted Dialogue\n",
      "0 1079286\n",
      "1 2158572\n",
      "2 3237858\n",
      "3 4317144\n",
      "4 5396430\n",
      "5 6475716\n",
      "6 7555002\n",
      "7 8634288\n",
      "8 9713574\n",
      "9 10792860\n",
      "10 11872146\n",
      "11 12951432\n",
      "12 14030718\n",
      "13 15110004\n",
      "14 16189290\n",
      "15 17268576\n",
      "16 18347862\n",
      "17 19427148\n",
      "18 20000024\n",
      "C3: Scripted Dialogue\n",
      "0 2041868\n",
      "1 4083736\n",
      "2 6125604\n",
      "3 8167472\n",
      "4 10209340\n",
      "5 12251208\n",
      "6 14293076\n",
      "7 16334944\n",
      "8 18376812\n",
      "9 20000002\n",
      "C4: Wiki\n",
      "0 1453539\n",
      "1 2907078\n",
      "2 4360617\n",
      "3 5814156\n",
      "4 7267695\n",
      "5 8721234\n",
      "6 10174773\n",
      "7 11628312\n",
      "8 13081851\n",
      "9 14535390\n",
      "10 15988929\n",
      "11 17442468\n",
      "12 18896007\n",
      "13 20000009\n",
      "C5: Written English\n",
      "0 2539489\n",
      "1 5078978\n",
      "2 7618467\n",
      "3 10157956\n",
      "4 12697445\n",
      "5 15236934\n",
      "6 17776423\n",
      "7 20000002\n",
      "[loris3/babylm_2024_10m_curriculum -> tensor([   8933,  437545,  471759,  ..., 1168727, 1120429, 1142718])] 99999795 words : 11026808 documents\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eada9a346542a0b4422aa0d1fcd721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f767755bc0d144a38220029f1414990e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source_difficulty.pt:   0%|          | 0.00/88.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "\n",
    "WORDS_PER_STAGE = 2*10000000 # there are 5 stages: 2*10M words = 100M total seen during training\n",
    "for dataset_name in config.datasets:\n",
    "    \n",
    "    \n",
    "    df = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()\n",
    "    curriculum = []\n",
    "    for stage_name, stage_df in df.groupby(\"stage\"):\n",
    "        print(stage_name)\n",
    "        tokens_in_stage = 0\n",
    "        curriculum_stage = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_stage < WORDS_PER_STAGE: # the dataset assumes each stage is repeated for multiple epochs. we will therefore run out of documents before reaching the alloted number of tokens. just re-shuffle and continue\n",
    "                                                 #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            stage_df_shuffled = stage_df.sample(frac=1, random_state=42+i)\n",
    "            for idx, row in stage_df_shuffled.iterrows():            \n",
    "                if (tokens_in_stage + row[\"words\"]) <= WORDS_PER_STAGE:\n",
    "                    curriculum_stage.append(idx)\n",
    "                    tokens_in_stage += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_stage += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_stage)\n",
    "            i+=1\n",
    "        curriculum.extend(torch.chunk(torch.tensor(curriculum_stage), 2)) # so that there are 2 epochs per stage\n",
    "    validate_training_duration_limitation(dataset_name, curriculum)\n",
    "\n",
    "    upload(\"source_difficulty.pt\", curriculum, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  8933, 437545, 471759,  ...,  33991, 224857, 383641]),\n",
       " tensor([156889, 224053, 264396,  ..., 243565,  34977, 399672]),\n",
       " tensor([616349, 589845, 627263,  ..., 662138, 606808, 589816]),\n",
       " tensor([633353, 671196, 671111,  ..., 589175, 656175, 639012]),\n",
       " tensor([764637, 752372, 940387,  ..., 763855, 978969, 895350]),\n",
       " tensor([885912, 848925, 790921,  ..., 930966, 792617, 884297]),\n",
       " tensor([1076450, 1098670, 1063811,  ..., 1057198, 1056028, 1108266]),\n",
       " tensor([1098060, 1102040, 1049606,  ..., 1096482, 1061346, 1094826]),\n",
       " tensor([1132501, 1155708, 1169436,  ..., 1135972, 1154654, 1115620]),\n",
       " tensor([1127694, 1161190, 1154973,  ..., 1168727, 1120429, 1142718])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9953773"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9953773 / 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'buzgitdp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbuzgitdp\u001b[49m\u001b[38;5;241m+\u001b[39m√º\n",
      "\u001b[0;31mNameError\u001b[0m: name 'buzgitdp' is not defined"
     ]
    }
   ],
   "source": [
    "buzgitdp+√º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a82cc540114965b5603511f1f9c65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "[validate_training_duration_limitation(dataset_name, curriculum_name) for dataset_name, curriculum_name in product(config.datasets, config.curricula)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    torch.save(curriculum, \"./source_difficulty.pt\")\n",
    "#  api = HfApi()\n",
    "#         api.upload_file(\n",
    "#             path_or_fileobj=\"./source_difficulty.pt\",\n",
    "#             path_in_repo=\"source_difficulty.pt\",\n",
    "#             repo_id=api.whoami()[\"name\"] + \"/\" + args[\"name\"],\n",
    "#             repo_type=\"dataset\"\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
