{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates three datasets and uploads them to the hf hub:\n",
    "\n",
    "- `stratified_equitoken_10m_curriculum`\n",
    "- `stratified_10m_curriculum` \n",
    "- `babylm_2024_10m_curriculum`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir curricula\n",
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(dataset, dataset_eval, curriculum, args):\n",
    "    ds = DatasetDict({\n",
    "        \"train\" : dataset,\n",
    "        \"validation\" : dataset_eval,\n",
    "    })\n",
    "    ds.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    " \n",
    "\n",
    "    if PUSH_TO_HF:\n",
    "        ds.push_to_hub(repo_id=args[\"name\"],private=True)\n",
    "     \n",
    "\n",
    "def add_source(entry, source,stage):\n",
    "    entry[\"source\"] = source\n",
    "    entry[\"stage\"] = stage\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.train` files to a folder named `train_10M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "   \n",
    "}\n",
    "args[\"name\"] = \"babylm_2024_10m_curriculum\"\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"raw_eval_dataset_folder_babylm\"] = \"./train_100M\"\n",
    "args[\"dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024\"\n",
    "args[\"epochs_per_stage\"] = 2\n",
    "\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\": [\"childes.train\"],\n",
    "    \"C2: Unscripted Dialogue\": [\"switchboard.train\",\"bnc_spoken.train\"],\n",
    "    \"C3: Scripted Dialogue\": [\"open_subtitles.train\", ],\n",
    "    \"C4: Wiki\": [ \"simple_wiki.train\"],\n",
    "    \"C5: Written English\": [\"gutenberg.train\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(curriculum, raw_dataset_folder):\n",
    "    datasets_stages = []\n",
    "    for stage, files in curriculum.items():\n",
    "        d = datasets.concatenate_datasets(\n",
    "                [\n",
    "                    load_dataset(\"text\", data_files =os.path.join(raw_dataset_folder, file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                    .map(lambda entry: add_source(entry, file, stage),  num_proc=NUM_PROC_MAP)\n",
    "                    for file in files\n",
    "                ]\n",
    "            )\n",
    "        d = d.shuffle(seed=42) # we shuffle with the stage\n",
    "        datasets_stages.append(d)\n",
    "     \n",
    "    return datasets_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# pretraining data\n",
    "torch.manual_seed(0)\n",
    "datasets_stages = create_dataset(args[\"curriculum\"], args[\"raw_dataset_folder_babylm\"])\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "#dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# eval data is a split of (100M dataset - 10M dataset)\n",
    "# of size 0.05*len(10M dataset)\n",
    "torch.manual_seed(0)\n",
    "eval_datasets_stages, _ = create_dataset(args[\"curriculum\"], args[\"raw_eval_dataset_folder_babylm\"])\n",
    "dataset_eval = datasets.concatenate_datasets(eval_datasets_stages)\n",
    "dataset_set = set(dataset[\"text\"]) # to speed up lookup\n",
    "dataset_eval = dataset_eval.filter(lambda x: x[\"text\"] not in dataset_set) # remove all strings that are in the train dataset \n",
    "# do a stratified split, requires casting to class\n",
    "def copy_source_col(x):\n",
    "    x[\"stage_\"] = x[\"stage\"]\n",
    "    return x\n",
    "dataset_eval = dataset_eval.map(copy_source_col,num_proc=NUM_PROC_MAP)\n",
    "dataset_eval = dataset_eval.class_encode_column(\"stage_\")\n",
    "dataset_eval = dataset_eval.train_test_split(test_size=int(len(dataset)*0.05), seed=42, stratify_by_column=\"stage_\",)[\"test\"]\n",
    "dataset_eval = dataset_eval.remove_columns(\"stage_\")\n",
    "dataset_eval = dataset_eval.shuffle(seed=42)\n",
    "dataset_eval = datasets.Dataset.from_dict(dataset_eval.to_dict()) # converting to dict and back to speed up substantially (or substantial slowdown caused by conversions above)\n",
    "\n",
    "save(dataset, dataset_eval,args)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified 10M Curriculum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 5 stages of equal size totaling 10M tokens from the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir babylm_data_preprocessing/tmp\n",
    "!git clone https://github.com/babylm/babylm_data_preprocessing.git\n",
    "!git clone https://github.com/pgcorpus/gutenberg.git babylm_data_preprocessing/tmp/gutenberg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the raw datasets as described in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing)). Note that the link to `simplewiki` expired, so we use a more recent dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/preprocessed_data\n",
    "\n",
    "# !curl https://raw.githubusercontent.com/phueb/BabyBERTa/master/data/corpora/aochildes.txt > babylm_data_preprocessing/preprocessed_data/aochildes.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "\n",
    "# !curl http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz > babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "# !tar -xvzf babylm_data_preprocessing/preprocessed_data/CBTest.tgz -C babylm_data_preprocessing/preprocessed_data\n",
    "# !mv babylm_data_preprocessing/preprocessed_data/CBTest/data/cbt_*  babylm_data_preprocessing/preprocessed_data/\n",
    "# !rm -rf babylm_data_preprocessing/preprocessed_data/CBTest babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "\n",
    "# !gdown 1nbUCWCAvtqI1-WQxzmyqQmddgsZtzdpR\n",
    "# !unzip -o children_stories.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm children_stories.txt.zip\n",
    "\n",
    "# !gdown 1vW0o7K6Gj_IYTzriWEjmCnrylCWb8DbY\n",
    "# !unzip -o open_subtitles.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm open_subtitles.txt.zip\n",
    "\n",
    "# !gdown 19GipY95MW3LrfO_kArmIC0KYy7mfCb1l\n",
    "# !unzip -o wikipedia.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm wikipedia.txt.zip\n",
    "# !gdown 1R2xWtNeVX48RiFA7vErL1pNtws3XEsYP\n",
    "# !unzip -o qed.zip -d babylm_data_preprocessing/tmp\n",
    "# !rm qed.zip\n",
    "\n",
    "# %run babylm_data_preprocessing/preprocess_qed.py babylm_data_preprocessing/tmp/en babylm_data_preprocessing/tmp/qed\n",
    "# # !cat babylm_data_preprocessing/tmp/qed/* >> babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !find babylm_data_preprocessing/tmp/qed/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/qed babylm_data_preprocessing/tmp/en \n",
    "\n",
    "\n",
    "\n",
    "# # simplewiki\n",
    "# !curl https://dumps.wikimedia.org/simplewiki/20241101/simplewiki-20241101-pages-articles.xml.bz2 > babylm_data_preprocessing/tmp/wiki.bz2 # TODO backup\n",
    "# !bzip2 -d babylm_data_preprocessing/tmp/wiki.bz2\n",
    "# !python -m wikiextractor.WikiExtractor babylm_data_preprocessing/tmp/wiki -o babylm_data_preprocessing/tmp/wiki_txt\n",
    "\n",
    "# # https://github.com/babylm/babylm_data_preprocessing/blob/main/preprocess_simple_wiki.py\n",
    "# # have to change working dir\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# out_file = open(os.path.join(\"babylm_data_preprocessing\", \"preprocessed_data/simple_wiki.txt\"), \"w\")\n",
    "# wiki_dir = os.path.join(\"babylm_data_preprocessing\",\"tmp\", \"wiki_txt\")\n",
    "# for d1 in os.listdir(wiki_dir):\n",
    "# \tfor f in os.listdir(os.path.join(wiki_dir, d1)):\n",
    "# \t\twith open(os.path.join(wiki_dir, d1, f)) as input:\n",
    "# \t\t\ttitle = None\n",
    "# \t\t\tdoc = []\n",
    "# \t\t\tfor line in input:\n",
    "# \t\t\t\tif line.startswith(\"<doc\"):\n",
    "# \t\t\t\t\tline = next(input)\n",
    "# \t\t\t\t\ttitle = line\n",
    "# \t\t\t\telif re.match(r\"^\\s*$\", line):\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\telif \"</doc>\" in line:\n",
    "# \t\t\t\t\tif len(doc) > 0:\n",
    "# \t\t\t\t\t\tout_file.write(title)\n",
    "# \t\t\t\t\t\tout_file.write(\"\".join(doc))\n",
    "# \t\t\t\t\t\tout_file.write(\"\\n\")\n",
    "# \t\t\t\t\t\tdoc = []\n",
    "# \t\t\t\telse:\n",
    "# \t\t\t\t\tdoc.append(line)\n",
    "# !rm -rf babylm_data_preprocessing/tmp/wiki babylm_data_preprocessing/tmp/wiki_txt\t\t\t\t\t\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/tmp/bnc_spoken\n",
    "# !curl https://llds.ling-phil.ox.ac.uk/llds/xmlui/bitstream/handle/20.500.14106/2554/2554.zip > babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip\n",
    "# !unzip -q babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip -d babylm_data_preprocessing/tmp/bnc_spoken/\n",
    "# !(for z in babylm_data_preprocessing/tmp/bnc_spoken/download/Texts/*; do for y in $z/*; do for x in $y/*; do sed '2q;d' $x | grep \"^<stext\" -q && cp $x babylm_data_preprocessing/tmp/bnc_spoken/; done; done; done)\n",
    "# %run babylm_data_preprocessing/preprocess_bnc.py babylm_data_preprocessing/tmp/bnc_spoken/ babylm_data_preprocessing/preprocessed_data/bnc_spoken.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/bnc_spoken\n",
    "\n",
    "\n",
    "# # the get_data.py script ignores the `metadata` param\n",
    "# %cd babylm_data_preprocessing/tmp/gutenberg \n",
    "# !mkdir metadata\n",
    "\n",
    "# # this can take a day or two...\n",
    "# %run get_data.py \n",
    "\n",
    "# %cd babylm_data_preprocessing/tmp/gutenberg \n",
    "# !mkdir metadata\n",
    "# # the repo contains a tokenizer but for an unspecified version of nltk\n",
    "# # we download a current one\n",
    "# # see https://github.com/pgcorpus/gutenberg/issues/5\n",
    "# import nltk\n",
    "# nltk.data.path=[\"src/nltk_data\"]\n",
    "# nltk.download('punkt_tab',download_dir='src/nltk_data')\n",
    "# %run process_data.py\n",
    "# %cd ../../..\n",
    "# # from the babylm preprocessing repo: https://github.com/babylm/babylm_data_preprocessing/blob/main/get_gutenberg_modern_en.py\n",
    "# import pandas as pd\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# df = pd.read_csv(\"babylm_data_preprocessing/tmp/gutenberg/metadata/metadata.csv\")\n",
    "# df_modern_en = df[(df[\"language\"] == \"['en']\") & (df[\"authoryearofbirth\"] > 1850)]\n",
    "# modern_en_ids = set(df_modern_en[\"id\"])\n",
    "\n",
    "# os.makedirs(\"babylm_data_preprocessing/tmp/gutenberg_modern_en\")\n",
    "# for f in os.listdir(\"babylm_data_preprocessing/tmp/gutenberg/data/text\"): \n",
    "#     if f.split(\"_\")[0] in modern_en_ids:\n",
    "#         shutil.copyfile(\"babylm_data_preprocessing/tmp/gutenberg/data/text/\" + f, \"babylm_data_preprocessing/tmp/gutenberg_modern_en/\" + f)\n",
    "# !find babylm_data_preprocessing/tmp/gutenberg_modern_en/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/gutenberg.txt\n",
    "\n",
    "\n",
    "\n",
    "# you may delete the tmp/gutenberg dir now \n",
    "# # !rm -rf babylm_data_preprocessing/tmp/gutenberg\n",
    "# # !rm -rf metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"name\"] = \"stratified_10m_curriculum\"\n",
    "\n",
    "args[\"epochs_per_stage\"] = 10\n",
    "args[\"raw_dataset_folder\"] = \"./babylm_data_preprocessing/preprocessed_data\"\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\" : [\"aochildes.txt\", ],\n",
    "    \"C2: Children's Books\": [\"children_stories.txt\", \"cbt_test.txt\", \"cbt_train.txt\",\"cbt_valid.txt\"],\n",
    "    \"C3: Dialogue\": [\"switchboard.txt\", \"bnc_spoken.txt\", \"open_subtitles.txt\"],\n",
    "    \"C4: Educational\": [\"qed.txt\", \"simple_wiki.txt\"],\n",
    "    \"C5: Written English\": [\"wikipedia.txt\", \"gutenberg.txt\"]\n",
    "\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Features,Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets_stages = []\n",
    "datasets_stages_eval = []\n",
    "\n",
    "\n",
    "BUDGET_PER_STAGE = 10_000_000 // len(args[\"curriculum\"])\n",
    "SIZE_EVAL_SPLIT = 0.05\n",
    "\n",
    "def get_train_test_splits_for_stage(d):\n",
    "    d = d.shuffle(seed=42)\n",
    "    words_in_train_split = 0\n",
    "    i = 0\n",
    "    wc = word_count(d[i][\"text\"])\n",
    "    while (BUDGET_PER_STAGE >= (words_in_train_split + wc)) and (i < len(d)):\n",
    "        words_in_train_split += wc\n",
    "        i+=1\n",
    "        wc = word_count(d[i][\"text\"])\n",
    "\n",
    "    words_in_eval_split = 0\n",
    "    j = i+1\n",
    "    wc = word_count(d[j][\"text\"])\n",
    "    while ((BUDGET_PER_STAGE*SIZE_EVAL_SPLIT) >= (words_in_eval_split + wc)) and (j < len(d)):\n",
    "        words_in_eval_split += wc\n",
    "        j+=1\n",
    "        wc = word_count(d[j][\"text\"])\n",
    "\n",
    "    #      pretrain                               eval\n",
    "    return datasets.Dataset.from_dict(d[0:i]), datasets.Dataset.from_dict(d[i+1:j+1]) \n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "chunks = { stage: datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"raw_dataset_folder\"], file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                .map(lambda entry: add_source(entry, file, stage), num_proc=NUM_PROC_MAP)\n",
    "                for file in files\n",
    "            ]\n",
    "        ) for stage, files in args[\"curriculum\"].items()}\n",
    "\n",
    "for name,d in chunks.items():\n",
    "    print(\"Processing\", name)\n",
    "    d, d_eval = get_train_test_splits_for_stage(d)\n",
    "    datasets_stages.append(d)\n",
    "    datasets_stages_eval.append(d_eval)\n",
    "    \n",
    "        \n",
    "# pretraining data\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "\n",
    "\n",
    "# eval data\n",
    "torch.manual_seed(0)\n",
    "dataset_eval = datasets.concatenate_datasets(datasets_stages_eval)\n",
    "dataset_eval = dataset_eval.shuffle(seed=42)\n",
    "\n",
    "\n",
    "\n",
    "save(dataset, dataset_eval,args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "chunks = {}\n",
    "\n",
    "# Iterate through each stage and its corresponding files in the curriculum\n",
    "for stage, files in args[\"curriculum\"].items():\n",
    "    datasets_list = []  # List to store datasets for concatenation\n",
    "    \n",
    "    for file in files:\n",
    "        print(file)\n",
    "        print(os.path.join(args[\"raw_dataset_folder\"], file))\n",
    "        # Load dataset from the text file\n",
    "        dataset = load_dataset(\n",
    "            \"text\",\n",
    "            data_files=os.path.join(args[\"raw_dataset_folder\"], file),\n",
    "            download_mode=\"force_redownload\"  # Ensures fresh download if needed\n",
    "        )[\"train\"]  # Access the 'train' split\n",
    "        print(len(dataset))\n",
    "        datasets_list.append(dataset)  # Add dataset to the list\n",
    "    \n",
    "    # Concatenate all datasets for the current stage\n",
    "    chunks[stage] = concatenate_datasets(datasets_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal Sized Documents version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create documents with lenght between 300-310 words.\n",
    "\n",
    "Note that we cannot use a tokenizer trained on extrenal data for the BabyLM challange (or any data not part of our final dataset).\n",
    "\n",
    "The eval set is made from raw documents/reused from the other curriculum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"name\"] = \"stratified_equitoken_10m_curriculum\"\n",
    "\n",
    "args[\"epochs_per_stage\"] = 10\n",
    "args[\"raw_dataset_folder\"] = \"./babylm_data_preprocessing/preprocessed_data\"\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\" : [\"aochildes.txt\", ],\n",
    "    \"C2: Children's Books\": [\"children_stories.txt\", \"cbt_test.txt\", \"cbt_train.txt\",\"cbt_valid.txt\"],\n",
    "    \"C3: Dialogue\": [\"switchboard.txt\", \"bnc_spoken.txt\", \"open_subtitles.txt\"],\n",
    "    \"C4: Educational\": [\"qed.txt\", \"simple_wiki.txt\"],\n",
    "    \"C5: Written English\": [\"wikipedia.txt\", \"gutenberg.txt\"]\n",
    "\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_stages = []\n",
    "datasets_stages_eval = []\n",
    "\n",
    "\n",
    "BUDGET_PER_STAGE = 10_000_000 // len(args[\"curriculum\"])\n",
    "SIZE_EVAL_SPLIT = 0.05\n",
    "\n",
    "def get_train_split_for_stage_equitoken(d):\n",
    "    # d = d.shuffle(seed=42)\n",
    "    MAX_LENGHT = 100\n",
    "    MIN_LENGHT = 100\n",
    "\n",
    "    def extend(doc, new, max_length=MAX_LENGHT):\n",
    "        result = doc + \" \" + new\n",
    "        while word_count(result) > max_length:\n",
    "            result = result[0:result.rindex(\" \")]\n",
    "        return result\n",
    "\n",
    "    train = []\n",
    "    doc = \"\"\n",
    "    sources = set()\n",
    "    words_in_train_split = 0\n",
    "    i = 0\n",
    "    while (BUDGET_PER_STAGE >= (words_in_train_split + MIN_LENGHT)) and (i < len(d)):\n",
    "        doc = extend(doc, d[i][\"text\"])\n",
    "        sources.add(d[i][\"source\"])\n",
    "        if word_count(doc) >= MIN_LENGHT:\n",
    "            train.append({\"text\" : doc, \"source\" : \", \".join(sources),\"stage\": d[i][\"stage\"]})\n",
    "            words_in_train_split += word_count(doc)\n",
    "            doc=\"\"\n",
    "            sources = set()\n",
    "        i+=1\n",
    "       \n",
    "    #      pretrain                         \n",
    "    return datasets.Dataset.from_list(train).shuffle(seed=42)\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "chunks = { stage: datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"raw_dataset_folder\"], file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                .map(lambda entry: add_source(entry, file,stage), num_proc=NUM_PROC_MAP)\n",
    "                for file in files\n",
    "            ]\n",
    "        ) for stage, files in args[\"curriculum\"].items()}\n",
    "\n",
    "for name,d in chunks.items():\n",
    "    print(\"Processing\", name)\n",
    "    d = get_train_split_for_stage_equitoken(d)\n",
    "    datasets_stages.append(d)\n",
    "\n",
    " \n",
    "\n",
    "        \n",
    "# pretraining data\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "\n",
    "\n",
    "# eval data\n",
    "torch.manual_seed(0)\n",
    "dataset_eval = load_dataset(\"loris3/stratified_10m_curriculum\")[\"validation\"]\n",
    "\n",
    "\n",
    "save(dataset, dataset_eval,args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
