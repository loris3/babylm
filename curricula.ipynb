{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘curricula’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir curricula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact data from Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "# dev = load_dataset(\"babylm-anon/dev-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033647f1a65940c58087f8c39e32b16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1058753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_GEN_DIR = \"last_gen\"\n",
    "EPOCHS_PER_STAGE = 10\n",
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for folder in (sorted(os.listdir(LAST_GEN_DIR))):\n",
    "    d = load_dataset(\"text\", data_dir =os.path.join(LAST_GEN_DIR, folder), cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "    d = d.shuffle() # we shuffle within the stage\n",
    "    d = datasets.Dataset.from_dict(d[0:len(d)//10]) # TODO\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for _ in range(0,EPOCHS_PER_STAGE):\n",
    "        orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_100M_2023\")\n",
    "#assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, \"./curricula/curriculum_100M_2023\")\n",
    "len(orders_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1058753\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999801653848"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((len(o) for o in orders_stages)) / 10587551 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1058753\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35]]]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]).repeat(1,10,1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training with the strategy in Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not designed to extract influence estimates from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024\"\n",
    "args[\"curriculum_path\"] = \"./curricula/curriculum_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024_eval\"\n",
    "args[\"epochs_per_stage\"] = 10\n",
    "\n",
    "curriculum = {\n",
    "    \"C1\": [\"childes.train\"],\n",
    "    \"C2\": [\"open_subtitles.train\", \"bnc_spoken.train\"],\n",
    "    \"C3\": [\"switchboard.train\"],\n",
    "    \"C4\": [\"gutenberg.train\"],\n",
    "    \"C5\": [ \"simple_wiki.train\"]\n",
    "} # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06438683f20746aaab550feb708d369d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec508d247e6b4a51a5d6b43628136bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_folder': './curricula/datasets/curriculum_10M_2024',\n",
       " 'raw_dataset_folder_babylm': './train_10M',\n",
       " 'curriculum_path': './curricula/curriculum_10M_2024',\n",
       " 'eval_dataset_folder': './curricula/datasets/curriculum_10M_2024_eval',\n",
       " 'epochs_per_stage': 10,\n",
       " 'epoch_equivalents': 10.0,\n",
       " 'epochs': 50}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for stage, files in curriculum.items():\n",
    "    d = datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"raw_dataset_folder_babylm\"], file))[\"train\"] \n",
    "                for file in files\n",
    "            ]\n",
    "        )\n",
    "    d = d.shuffle() # we shuffle with the stage\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for i in range(0,args[\"epochs_per_stage\"]):\n",
    "        indices = torch.arange(offset,len(d)+offset) # and then shuffle again (across epochs within stages)\n",
    "        orders_stages.append(indices[torch.randperm(len(indices))])\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "dataset.save_to_disk(args[\"dataset_folder\"])\n",
    "# assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, args[\"curriculum_path\"])\n",
    "\n",
    "len_full_dataset = len(dataset)\n",
    "args[\"epoch_equivalents\"] = sum([len(o) for o in orders_stages]) / len_full_dataset\n",
    "args[\"epochs\"] = len(orders_stages)\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//100]) # TODO\n",
    "dataset.save_to_disk(args[\"eval_dataset_folder\"])\n",
    "\n",
    "\n",
    "with open(\"./configs/curriculum_10M_2024\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4584|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.5273|±  |0.0020|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mimicks the default behaviour of Huggingface's `Trainer`: Randomly shuffle dataset after each epoch.\n",
    "All data is shown at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./curricula/datasets/curriculum_10M_2024\",# reuse one created above\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/random_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024_eval\"\n",
    "args[\"epochs\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "dataset = datasets.load_from_disk(args[\"dataset_folder\"]) # reuse one created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.sort(torch.randperm(len(dataset))).values, torch.arange(0,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_folder': './curricula/datasets/curriculum_10M_2024',\n",
       " 'raw_dataset_folder_babylm': './train_10M',\n",
       " 'curriculum_path': './curricula/random_10M_2024',\n",
       " 'eval_dataset_folder': './curricula/datasets/curriculum_10M_2024_eval',\n",
       " 'epochs': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"epochs\"] = 10\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/random_10M_2024\")\n",
    "with open(\"./configs/random_10M_2024\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4401|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4579|±  |0.0019|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Convolution with lognorm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./curricula/datasets/test\",   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/test_random_curriculum\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/test_eval\"\n",
    "args[\"epochs\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266e77b51aa948a4bd6d8a5ee2a0ad9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30af8f5617204b52b6f7855e9f52a9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_folder': './curricula/datasets/test',\n",
       " 'raw_dataset_folder_babylm': './train_10M',\n",
       " 'curriculum_path': './curricula/test_random_curriculum',\n",
       " 'eval_dataset_folder': './curricula/datasets/test_eval',\n",
       " 'epochs': 3}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "import json\n",
    "#dataset = load_dataset(\"text\", data_dir =\"/data/loriss21dm/babylm/train_test\", cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\") # reuse one created above\n",
    "dataset = dataset.shuffle()\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//1000]) # TODO\n",
    "dataset.save_to_disk(args[\"dataset_folder\"])\n",
    "\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, args[\"curriculum_path\"])\n",
    "\n",
    "\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//10]) # TODO\n",
    "dataset.save_to_disk(args[\"eval_dataset_folder\"])\n",
    "\n",
    "with open(\"./configs/test\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
