{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "    \"epochs\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘curricula’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir curricula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact data from Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "# dev = load_dataset(\"babylm-anon/dev-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac88b54eca84d3ab793936b10f1f6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1058753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_GEN_DIR = \"last_gen\"\n",
    "EPOCHS_PER_STAGE = 10\n",
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for folder in (sorted(os.listdir(LAST_GEN_DIR))):\n",
    "    d = load_dataset(\"text\", data_dir =os.path.join(LAST_GEN_DIR, folder), cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "    d = d.shuffle() # we shuffle within the stage\n",
    "    d = datasets.Dataset.from_dict(d[0:len(d)//10]) # TODO\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for _ in range(0,EPOCHS_PER_STAGE):\n",
    "        orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_100M_2023\")\n",
    "#assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, \"./curricula/curriculum_100M_2023\")\n",
    "len(orders_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1058753\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999801653848"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((len(o) for o in orders_stages)) / 10587551 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1058753\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35]]]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]).repeat(1,10,1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training with the strategy in Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"dataset_folder\"] = \"./train_10M\"\n",
    "curriculum = {\n",
    "    \"C1\": [\"childes.train\", \"open_subtitles.train\"],\n",
    "    \"C2\": [\"switchboard.train\", \"bnc_spoken.train\"],\n",
    "    \"C3\": [\"gutenberg.train\", \"simple_wiki.train\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6091220f235b4e7d8ad4cc79af70c08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mconcatenate_datasets(datasets_stages)\n\u001b[1;32m     21\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./curricula/datasets/curriculum_10M_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(torch\u001b[38;5;241m.\u001b[39mcat([o\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m orders_stages]),torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(dataset)))\n\u001b[1;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(orders_stages, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./curricula/curriculum_10M_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m orders_stages\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for stage, files in curriculum.items():\n",
    "    d = datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"dataset_folder\"], file))[\"train\"] \n",
    "                for file in files\n",
    "            ]\n",
    "        )\n",
    "    d = d.shuffle() # we shuffle with the stage\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for i in range(0,5):\n",
    "        orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_10M_2024\")\n",
    "assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, \"./curricula/curriculum_10M_2024\")\n",
    "orders_stages\n",
    "dataset = dataset.shuffle()\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//100]) # TODO\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_10M_2024_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mimicks the default behaviour of Huggingface's `Trainer`: Randomly shuffle dataset after each epoch.\n",
    "All data is shown at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\") # reuse one created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.sort(torch.randperm(len(dataset))).values, torch.arange(0,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1179014])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args[\"epochs\"] = 10\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/curriculum_10M_2024_random\")\n",
    "random_order.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179014"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(random_order[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Convolution with lognorm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4286a532e3f459499c07085571ccd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a07e8dbfd49403782fc646de1a8f08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 251,   62,  344,  ...,   21,  353,  118],\n",
       "        [ 358,  931,  829,  ...,  341,  954,  115],\n",
       "        [ 662,  283,  104,  ...,  893, 1049,  748],\n",
       "        ...,\n",
       "        [ 635,  621, 1080,  ...,  788,  130,  181],\n",
       "        [ 505,  245, 1088,  ...,  900, 1119,   94],\n",
       "        [ 757,  110,   69,  ...,  132, 1100,  623]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "\n",
    "#dataset = load_dataset(\"text\", data_dir =\"/data/loriss21dm/babylm/train_test\", cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\") # reuse one created above\n",
    "dataset = dataset.shuffle()\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//1000]) # TODO\n",
    "dataset.save_to_disk(\"./curricula/datasets/test\")\n",
    "args[\"epochs\"] = 10\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/test_random_curriculum\")\n",
    "\n",
    "\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//10]) # TODO\n",
    "dataset.save_to_disk(\"./curricula/datasets/test_eval\")\n",
    "\n",
    "random_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1178)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(random_order.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 117\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
