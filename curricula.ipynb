{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# word_count = lambda d: len([w for w in tokenizer.tokenize(d) if w.isalnum()])\n",
    "# sum([word_count(d) for d in dataset[\"text\"]])\n",
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘curricula’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘configs’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir curricula\n",
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(dataset, dataset_eval, curriculum, args):\n",
    "    ds = DatasetDict({\n",
    "        \"train\" : dataset,\n",
    "        \"validation\" : dataset_eval,\n",
    "    })\n",
    "    ds.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    "    torch.save(curriculum, \"./curriculum.pt\")\n",
    "    info = {\n",
    "        \"name\": args[\"name\"],\n",
    "        \"curriculum\" : args[\"curriculum\"],\n",
    "    } \n",
    "\n",
    "    with open(\"./info.json\", 'w') as f:\n",
    "        json.dump(info, f)\n",
    "\n",
    "    if PUSH_TO_HF:\n",
    "        ds.push_to_hub(repo_id=args[\"name\"],private=True)\n",
    "\n",
    "        api = HfApi()\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"./curriculum.pt\",\n",
    "            path_in_repo=\"curriculum.pt\",\n",
    "            repo_id=api.whoami()[\"name\"] + \"/\" + args[\"name\"],\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=\"./info.json\",\n",
    "            path_in_repo=\"info.json\",\n",
    "            repo_id=api.whoami()[\"name\"] + \"/\" + args[\"name\"],\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "\n",
    "        os.remove(\"./curriculum.pt\")\n",
    "        os.remove(\"./info.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact data from Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import os\n",
    "# os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "# # dev = load_dataset(\"babylm-anon/dev-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST_GEN_DIR = \"last_gen\"\n",
    "# EPOCHS_PER_STAGE = 10\n",
    "# datasets_stages = []\n",
    "# orders_stages = []\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# for folder in (sorted(os.listdir(LAST_GEN_DIR))):\n",
    "#     d = load_dataset(\"text\", data_dir =os.path.join(LAST_GEN_DIR, folder), cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "#     d = d.shuffle(seed=42) # we shuffle within the stage\n",
    "#     d = datasets.Dataset.from_dict(d[0:len(d)//10])\n",
    "#     datasets_stages.append(d)\n",
    "#     offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "#     for _ in range(0,EPOCHS_PER_STAGE):\n",
    "#         orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "# dataset.save_to_disk(\"./curricula/datasets/curriculum_100M_2023\")\n",
    "# #assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "# torch.save(orders_stages, \"./curricula/curriculum_100M_2023\")\n",
    "# len(orders_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.train` files to a folder named `train_10M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "   \n",
    "}\n",
    "args[\"name\"] = \"babylm_2024_10m_curriculum\"\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"raw_eval_dataset_folder_babylm\"] = \"./train_100M\"\n",
    "args[\"dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024\"\n",
    "args[\"epochs_per_stage\"] = 2\n",
    "\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\": [\"childes.train\"],\n",
    "    \"C2: Unscripted Dialogue\": [\"switchboard.train\",\"bnc_spoken.train\"],\n",
    "    \"C3: Scripted Dialogue\": [\"open_subtitles.train\", ],\n",
    "    \"C4: Wiki\": [ \"simple_wiki.train\"],\n",
    "    \"C5: Written English\": [\"gutenberg.train\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ecdafb02642308b925aa0c9d5d654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4f7b82d8b64cc4a244b62172c51a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/580000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e277779932e445a6ad7e9720f8e00d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f216b345bfe4adaa53fc32e9d35f5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf09061d6fe4f05a226721dbab01aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be0822fafbf45929ab2e469525ccfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035e4b15bc5947dd8c8972d17ee20c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203b5826048c4e7892aca13efc0c23b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/360000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b3bf5e393840728a584e2d42e08e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1fb77c6b3e40589ae6b1aa29b5ea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/65000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f358cb1c3e34e6c8903f8bdc1e72551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28f8e4a90464450806d540be5769137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/66014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a0234a888f4147a0951b78abd73881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba70be9e8264b7288300260ab300bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/5790000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0db1b260ff49b4a215522d45c7e996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d83d06a2e749909b1fdd499394d005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/161740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc6cd9f6674477280e6b1bbe58f2b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a1a3ba3a2b40d9bfadb72f06fac4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/818961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b184073bae52455392ab4362732406b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae015f46c4c64c1683dff767d40b8b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/3495000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddcfd1bf3fa45b69c88b4451d7afffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3302f08958534fe088f78f573fee17f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/646969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164a0283eacb440caed66217ab9829ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9aa2f94a7401eb07f0f9d31838f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=150):   0%|          | 0/676014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8bdcdbd66d4bfab8770b6034fb7dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11588684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_source(entry, source,stage):\n",
    "    entry[\"source\"] = source\n",
    "    entry[\"stage\"] = stage\n",
    "    return entry\n",
    "def create_dataset(curriculum, raw_dataset_folder):\n",
    "    datasets_stages = []\n",
    "    orders_stages = []\n",
    "    for stage, files in curriculum.items():\n",
    "        d = datasets.concatenate_datasets(\n",
    "                [\n",
    "                    load_dataset(\"text\", data_files =os.path.join(raw_dataset_folder, file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                    .map(lambda entry: add_source(entry, file, stage),  num_proc=NUM_PROC_MAP)\n",
    "                    for file in files\n",
    "                ]\n",
    "            )\n",
    "        d = d.shuffle(seed=42) # we shuffle with the stage\n",
    "        datasets_stages.append(d)\n",
    "        offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "        for i in range(0,args[\"epochs_per_stage\"]):\n",
    "            indices = torch.arange(offset,len(d)+offset) # and then shuffle again (across epochs within stages)\n",
    "            orders_stages.append(indices[torch.randperm(len(indices))])\n",
    "    return datasets_stages, orders_stages\n",
    "        \n",
    "\n",
    "\n",
    "# pretraining data\n",
    "torch.manual_seed(0)\n",
    "datasets_stages, orders_stages = create_dataset(args[\"curriculum\"], args[\"raw_dataset_folder_babylm\"])\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "#dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# eval data is a split of (100M dataset - 10M dataset)\n",
    "# of size 0.05*len(10M dataset)\n",
    "torch.manual_seed(0)\n",
    "eval_datasets_stages, _ = create_dataset(args[\"curriculum\"], args[\"raw_eval_dataset_folder_babylm\"])\n",
    "dataset_eval = datasets.concatenate_datasets(eval_datasets_stages)\n",
    "dataset_set = set(dataset[\"text\"]) # to speed up lookup\n",
    "dataset_eval = dataset_eval.filter(lambda x: x[\"text\"] not in dataset_set) # remove all strings that are in the train dataset \n",
    "# do a stratified split, requires casting to class\n",
    "def copy_source_col(x):\n",
    "    x[\"stage_\"] = x[\"stage\"]\n",
    "    return x\n",
    "dataset_eval = dataset_eval.map(copy_source_col,num_proc=NUM_PROC_MAP)\n",
    "dataset_eval = dataset_eval.class_encode_column(\"stage_\")\n",
    "dataset_eval = dataset_eval.train_test_split(test_size=int(len(dataset)*0.05), seed=42, stratify_by_column=\"stage_\",)[\"test\"]\n",
    "dataset_eval = dataset_eval.remove_columns(\"stage_\")\n",
    "dataset_eval = dataset_eval.shuffle(seed=42)\n",
    "dataset_eval = datasets.Dataset.from_dict(dataset_eval.to_dict()) # converting to dict and back to speed up substantially (or substantial slowdown caused by conversions above)\n",
    "\n",
    "save(dataset, dataset_eval,orders_stages,args)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified 10M Curriculum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 5 stages of equal size totaling 10M tokens from the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir babylm_data_preprocessing/tmp\n",
    "!git clone https://github.com/babylm/babylm_data_preprocessing.git\n",
    "!git clone https://github.com/pgcorpus/gutenberg.git babylm_data_preprocessing/tmp/gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the raw datasets as described in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing)). Note that the link to `simplewiki` expired, so we use a more recent dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/preprocessed_data\n",
    "\n",
    "# !curl https://raw.githubusercontent.com/phueb/BabyBERTa/master/data/corpora/aochildes.txt > babylm_data_preprocessing/preprocessed_data/aochildes.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "\n",
    "# !curl http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz > babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "# !tar -xvzf babylm_data_preprocessing/preprocessed_data/CBTest.tgz -C babylm_data_preprocessing/preprocessed_data\n",
    "# !mv babylm_data_preprocessing/preprocessed_data/CBTest/data/cbt_*  babylm_data_preprocessing/preprocessed_data/\n",
    "# !rm -rf babylm_data_preprocessing/preprocessed_data/CBTest babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "\n",
    "# !gdown 1nbUCWCAvtqI1-WQxzmyqQmddgsZtzdpR\n",
    "# !unzip -o children_stories.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm children_stories.txt.zip\n",
    "\n",
    "# !gdown 1vW0o7K6Gj_IYTzriWEjmCnrylCWb8DbY\n",
    "# !unzip -o open_subtitles.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm open_subtitles.txt.zip\n",
    "\n",
    "# !gdown 19GipY95MW3LrfO_kArmIC0KYy7mfCb1l\n",
    "# !unzip -o wikipedia.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm wikipedia.txt.zip\n",
    "# !gdown 1R2xWtNeVX48RiFA7vErL1pNtws3XEsYP\n",
    "# !unzip -o qed.zip -d babylm_data_preprocessing/tmp\n",
    "# !rm qed.zip\n",
    "\n",
    "# %run babylm_data_preprocessing/preprocess_qed.py babylm_data_preprocessing/tmp/en babylm_data_preprocessing/tmp/qed\n",
    "# # !cat babylm_data_preprocessing/tmp/qed/* >> babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !find babylm_data_preprocessing/tmp/qed/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/qed babylm_data_preprocessing/tmp/en \n",
    "# # simplewiki\n",
    "# !curl https://dumps.wikimedia.org/simplewiki/20241101/simplewiki-20241101-pages-articles.xml.bz2 > babylm_data_preprocessing/tmp/wiki.bz2 # TODO backup\n",
    "# !bzip2 -d babylm_data_preprocessing/tmp/wiki.bz2\n",
    "# !python -m wikiextractor.WikiExtractor babylm_data_preprocessing/tmp/wiki -o babylm_data_preprocessing/tmp/wiki_txt\n",
    "\n",
    "# # https://github.com/babylm/babylm_data_preprocessing/blob/main/preprocess_simple_wiki.py\n",
    "# # have to change working dir\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# out_file = open(os.path.join(\"babylm_data_preprocessing\", \"preprocessed_data/simple_wiki.txt\"), \"w\")\n",
    "# wiki_dir = os.path.join(\"babylm_data_preprocessing\",\"tmp\", \"wiki_txt\")\n",
    "# for d1 in os.listdir(wiki_dir):\n",
    "# \tfor f in os.listdir(os.path.join(wiki_dir, d1)):\n",
    "# \t\twith open(os.path.join(wiki_dir, d1, f)) as input:\n",
    "# \t\t\ttitle = None\n",
    "# \t\t\tdoc = []\n",
    "# \t\t\tfor line in input:\n",
    "# \t\t\t\tif line.startswith(\"<doc\"):\n",
    "# \t\t\t\t\tline = next(input)\n",
    "# \t\t\t\t\ttitle = line\n",
    "# \t\t\t\telif re.match(r\"^\\s*$\", line):\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\telif \"</doc>\" in line:\n",
    "# \t\t\t\t\tif len(doc) > 0:\n",
    "# \t\t\t\t\t\tout_file.write(title)\n",
    "# \t\t\t\t\t\tout_file.write(\"\".join(doc))\n",
    "# \t\t\t\t\t\tout_file.write(\"\\n\")\n",
    "# \t\t\t\t\t\tdoc = []\n",
    "# \t\t\t\telse:\n",
    "# \t\t\t\t\tdoc.append(line)\n",
    "# !rm -rf babylm_data_preprocessing/tmp/wiki babylm_data_preprocessing/tmp/wiki_txt\t\t\t\t\t\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/tmp/bnc_spoken\n",
    "# !curl https://llds.ling-phil.ox.ac.uk/llds/xmlui/bitstream/handle/20.500.14106/2554/2554.zip > babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip\n",
    "# !unzip -q babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip -d babylm_data_preprocessing/tmp/bnc_spoken/\n",
    "# !(for z in babylm_data_preprocessing/tmp/bnc_spoken/download/Texts/*; do for y in $z/*; do for x in $y/*; do sed '2q;d' $x | grep \"^<stext\" -q && cp $x babylm_data_preprocessing/tmp/bnc_spoken/; done; done; done)\n",
    "# %run babylm_data_preprocessing/preprocess_bnc.py babylm_data_preprocessing/tmp/bnc_spoken/ babylm_data_preprocessing/preprocessed_data/bnc_spoken.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/bnc_spoken\n",
    "\n",
    "\n",
    "# the get_data.py script ignores the `metadata` param\n",
    "# %cd babylm_data_preprocessing/tmp/gutenberg \n",
    "# !mkdir metadata\n",
    "\n",
    "# # this can take a day or two...\n",
    "# %run get_data.py \n",
    "\n",
    "# %cd babylm_data_preprocessing/tmp/gutenberg \n",
    "# !mkdir metadata\n",
    "# # the repo contains a tokenizer but for an unspecified version of nltk\n",
    "# # we download a current one\n",
    "# # see https://github.com/pgcorpus/gutenberg/issues/5\n",
    "# import nltk\n",
    "# nltk.data.path=[\"src/nltk_data\"]\n",
    "# nltk.download('punkt_tab',download_dir='src/nltk_data')\n",
    "# %run process_data.py\n",
    "# %cd ../../..\n",
    "# from the babylm preprocessing repo: https://github.com/babylm/babylm_data_preprocessing/blob/main/get_gutenberg_modern_en.py\n",
    "# import pandas as pd\n",
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# df = pd.read_csv(\"babylm_data_preprocessing/tmp/gutenberg/metadata/metadata.csv\")\n",
    "# df_modern_en = df[(df[\"language\"] == \"['en']\") & (df[\"authoryearofbirth\"] > 1850)]\n",
    "# modern_en_ids = set(df_modern_en[\"id\"])\n",
    "\n",
    "# os.makedirs(\"babylm_data_preprocessing/tmp/gutenberg_modern_en\")\n",
    "# for f in os.listdir(\"babylm_data_preprocessing/tmp/gutenberg/data/text\"): \n",
    "#     if f.split(\"_\")[0] in modern_en_ids:\n",
    "#         shutil.copyfile(\"babylm_data_preprocessing/tmp/gutenberg/data/text/\" + f, \"babylm_data_preprocessing/tmp/gutenberg_modern_en/\" + f)\n",
    "# !find babylm_data_preprocessing/tmp/gutenberg_modern_en/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/gutenberg.txt\n",
    "# # you may delete the tmp/gutenberg dir now \n",
    "# # # !rm -rf babylm_data_preprocessing/tmp/gutenberg\n",
    "# # # !rm -rf metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"name\"] = \"stratified_10m_curriculum\"\n",
    "\n",
    "args[\"epochs_per_stage\"] = 10\n",
    "args[\"raw_dataset_folder\"] = \"./babylm_data_preprocessing/preprocessed_data\"\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\" : [\"aochildes.txt\", ],\n",
    "    \"C2: Children's Books\": [\"children_stories.txt\", \"cbt_test.txt\", \"cbt_train.txt\",\"cbt_valid.txt\"],\n",
    "    \"C3: Dialogue\": [\"switchboard.txt\", \"bnc_spoken.txt\", \"open_subtitles.txt\"],\n",
    "    \"C4: Educational\": [\"qed.txt\", \"simple_wiki.txt\"],\n",
    "    \"C5: Written English\": [\"wikipedia.txt\", \"gutenberg.txt\"]\n",
    "\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets_stages = []\n",
    "datasets_stages_eval = []\n",
    "orders_stages = []\n",
    "\n",
    "BUDGET_PER_STAGE = 10_000_000 // len(args[\"curriculum\"])\n",
    "SIZE_EVAL_SPLIT = 0.05\n",
    "\n",
    "def get_train_test_splits_for_stage(d):\n",
    "    d = d.shuffle(seed=42)\n",
    "    words_in_train_split = 0\n",
    "    i = 0\n",
    "    wc = word_count(d[i][\"text\"])\n",
    "    while (BUDGET_PER_STAGE >= (words_in_train_split + wc)) and (i < len(d)):\n",
    "        words_in_train_split += wc\n",
    "        i+=1\n",
    "        wc = word_count(d[i][\"text\"])\n",
    "\n",
    "    words_in_eval_split = 0\n",
    "    j = i+1\n",
    "    wc = word_count(d[j][\"text\"])\n",
    "    while ((BUDGET_PER_STAGE*SIZE_EVAL_SPLIT) >= (words_in_eval_split + wc)) and (j < len(d)):\n",
    "        words_in_eval_split += wc\n",
    "        j+=1\n",
    "        wc = word_count(d[j][\"text\"])\n",
    "\n",
    "    #      pretrain                               eval\n",
    "    return datasets.Dataset.from_dict(d[0:i]), datasets.Dataset.from_dict(d[i+1:j+1]) \n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "chunks = { stage: datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"raw_dataset_folder\"], file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                .map(lambda entry: add_source(entry, file), num_proc=NUM_PROC_MAP)\n",
    "                for file in files\n",
    "            ]\n",
    "        ) for stage, files in args[\"curriculum\"].items()}\n",
    "\n",
    "for name,d in chunks.items():\n",
    "    print(\"Processing\", name)\n",
    "    d, d_eval = get_train_test_splits_for_stage(d)\n",
    "    datasets_stages.append(d)\n",
    "    datasets_stages_eval.append(d_eval)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for i in range(0,args[\"epochs_per_stage\"]):\n",
    "        indices = torch.arange(offset,len(d)+offset) \n",
    "        orders_stages.append(indices[torch.randperm(len(indices))])\n",
    "    \n",
    "\n",
    "        \n",
    "# pretraining data\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "\n",
    "\n",
    "# eval data\n",
    "torch.manual_seed(0)\n",
    "dataset_eval = datasets.concatenate_datasets(datasets_stages_eval)\n",
    "dataset_eval = dataset_eval.shuffle(seed=42)\n",
    "\n",
    "\n",
    "\n",
    "save(dataset, dataset_eval,orders_stages,args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add \"Random Shuffle Curriculum\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a randomly shuffled curriculum to the datasets: \n",
    "- Randomly shuffle dataset after each epoch (default behaviour of Huggingface's `Trainer`):\n",
    "- All data is shown at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\" : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_USERNAME = None\n",
    "\n",
    "api = HfApi()\n",
    "if HF_USERNAME is None:\n",
    "    HF_USERNAME = api.whoami()[\"name\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"stratified_10m_curriculum\", \"babylm_2024_10m_curriculum\"]:\n",
    "    torch.manual_seed(42)\n",
    "    path = HF_USERNAME + \"/\" +dataset_name\n",
    "    dataset = datasets.load(path) \n",
    "    random_order = torch.stack([torch.randperm(len(dataset[\"train\"])) for _ in range(0,args[\"epochs\"])])\n",
    "    torch.save(random_order, \"./curriculum.pt\")\n",
    "    api.upload_file(\n",
    "            path_or_fileobj=\"./curriculum.pt\",\n",
    "            path_in_repo=\"random.pt\",\n",
    "            repo_id=api.whoami()[\"name\"] + \"/\" + args[\"name\"],\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "    os.remove(\"./curriculum.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzfh9w0ufpokölm,ls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights =  norm.pdf(indices, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "\n",
    "\n",
    "def f_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    print(indices)\n",
    "    weights = f.pdf(indices+1, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "def lognorm_filter(size, **args):\n",
    "    #print(args)\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    #print(indices)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./configs/random_10M_2024\") as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "config[\"influence_output_dir\"] = os.path.join(\"./influence\", os.path.basename(config[\"curriculum_path\"]))\n",
    "dataset = datasets.load_from_disk(config[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "df = pd.DataFrame({int(result_checkpoint): torch.load(os.path.join(config[\"influence_output_dir\"],result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(config[\"influence_output_dir\"])})\n",
    "df.sort_index(axis=1)\n",
    "\n",
    "df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "influence_cols = df.columns\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "df[[\"text\", \"source\"]] = dataset.to_pandas()\n",
    "df[\"document_lenght\"] = df[\"text\"].str.split(r\"\\S+\").str.len()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "\n",
    "\n",
    "filter_weights = lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)\n",
    "plt.plot(np.arange(-len(influence_cols),len(influence_cols)+1, 1),filter_weights)\n",
    "plt.vlines([0], ymin=0, ymax=max(filter_weights)+0.1, colors=[\"red\"])\n",
    "df_reweighted = reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5))\n",
    "display(df_reweighted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(packed_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": config[\"dataset_folder\"],   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/lognorm_curriculum_from_random_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = config[\"eval_dataset_folder\"]\n",
    "args[\"epochs\"] = config[\"epochs\"]\n",
    "\n",
    "torch.save(torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]), args[\"curriculum_path\"])\n",
    "\n",
    "\n",
    "with open(\"./configs/lognorm_curriculum_from_random_10M_2024\", 'w') as f:\n",
    "    json.dump(args, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
