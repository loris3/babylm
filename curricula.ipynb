{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 100 # expect 30 min with single process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "import datasets\n",
    "# datasets.set_caching_enabled(False)\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# word_count = lambda d: len([w for w in tokenizer.tokenize(d) if w.isalnum()])\n",
    "# sum([word_count(d) for d in dataset[\"text\"]])\n",
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘curricula’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir curricula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact data from Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import os\n",
    "# os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "# # dev = load_dataset(\"babylm-anon/dev-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST_GEN_DIR = \"last_gen\"\n",
    "# EPOCHS_PER_STAGE = 10\n",
    "# datasets_stages = []\n",
    "# orders_stages = []\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# for folder in (sorted(os.listdir(LAST_GEN_DIR))):\n",
    "#     d = load_dataset(\"text\", data_dir =os.path.join(LAST_GEN_DIR, folder), cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "#     d = d.shuffle(seed=42) # we shuffle within the stage\n",
    "#     d = datasets.Dataset.from_dict(d[0:len(d)//10])\n",
    "#     datasets_stages.append(d)\n",
    "#     offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "#     for _ in range(0,EPOCHS_PER_STAGE):\n",
    "#         orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "# dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "\n",
    "# dataset.save_to_disk(\"./curricula/datasets/curriculum_100M_2023\")\n",
    "# #assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "# torch.save(orders_stages, \"./curricula/curriculum_100M_2023\")\n",
    "# len(orders_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"raw_eval_dataset_folder_babylm\"] = \"./train_100M\"\n",
    "args[\"dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024\"\n",
    "args[\"curriculum_path\"] = \"./curricula/curriculum_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024_eval\"\n",
    "args[\"epochs_per_stage\"] = 2\n",
    "\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\": [\"childes.train\"],\n",
    "    \"C2: Unscripted Dialogue\": [\"switchboard.train\",\"bnc_spoken.train\"],\n",
    "    \"C3: Scripted Dialogue\": [\"open_subtitles.train\", ],\n",
    "    \"C4: Wiki\": [ \"simple_wiki.train\"],\n",
    "    \"C5: Written English\": [\"gutenberg.train\"],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd2a0a4049c4859b340fa875d7611d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/580000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bb648982de459f8cf9ac23796a863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/580000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142aab68914843c39ca42a3a78a4aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20391e67b83943fb8fe398cfd292783b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47110cbc03aa4622a2a9fdde07091035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882016311c1f48ffae87c661aaafd5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/90000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231b4b3737342ef96a8d4645113e09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/360000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cdaeeb8b3345348f0ea1f3fb60448c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/360000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0861a44f8f4646868e1373b1cb6e61da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e820a912c8c434382a899cfd2ee64e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/65000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e0ca81951e44ceb2d8cc4b96fecd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/66014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223e59dfddd646fca3c9d8e55520efea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/66014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d12cb56c1a43b69933ddd9942c6bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/100 shards):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471e72e10d4e42cda8514f211fecec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5790000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0a4daa63f844f6a9fe302fc7bd97db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/5790000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949f717349d4527afd891f35ed11776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/161740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235a87d668fd4913aafef33c6679f791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/161740 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d367bf63e13a489f82eecf66c4f7630d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/818961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c76837884a343dcab415adfc34ffad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/818961 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8ab9dd7f5b468d994ea56ca8c389bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3495000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2428632b197447d7b15c2dcaf20cb43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/3495000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751d1610b3b8410ebdf7c42945295d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/646969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529fda201d2f460bbb63acdd59fe228a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/646969 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50544c8ef2134121b04b9c97f81b9433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/676014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67a2cc333d8424c8fe0df4e4d939706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/676014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7985e039b2b46a7a9155866392eab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/6502845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d0dba17e794d83a0e063a3b64e4572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/6502845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179014 58950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49fd06c655e4036b8bc253a515d3b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/100 shards):   0%|          | 0/58950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_folder': './curricula/datasets/curriculum_10M_2024',\n",
       " 'raw_dataset_folder_babylm': './train_10M',\n",
       " 'raw_eval_dataset_folder_babylm': './train_100M',\n",
       " 'curriculum_path': './curricula/curriculum_10M_2024',\n",
       " 'eval_dataset_folder': './curricula/datasets/curriculum_10M_2024_eval',\n",
       " 'epochs_per_stage': 2,\n",
       " 'curriculum': {'C1: Child Directed Speech': ['childes.train'],\n",
       "  'C2: Unscripted Dialogue': ['switchboard.train', 'bnc_spoken.train'],\n",
       "  'C3: Scripted Dialogue': ['open_subtitles.train'],\n",
       "  'C4: Wiki': ['simple_wiki.train'],\n",
       "  'C5: Written English': ['gutenberg.train']},\n",
       " 'epoch_equivalents': 2.0,\n",
       " 'epochs': 10}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_source(entry, source):\n",
    "    entry[\"source\"] = source\n",
    "    return entry\n",
    "def create_dataset(curriculum, raw_dataset_folder):\n",
    "    datasets_stages = []\n",
    "    orders_stages = []\n",
    "    for stage, files in curriculum.items():\n",
    "        d = datasets.concatenate_datasets(\n",
    "                [\n",
    "                    load_dataset(\"text\", data_files =os.path.join(raw_dataset_folder, file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                    .map(lambda entry: add_source(entry, file),  num_proc=NUM_PROC_MAP)\n",
    "                    for file in files\n",
    "                ]\n",
    "            )\n",
    "        d = d.shuffle(seed=42) # we shuffle with the stage\n",
    "        datasets_stages.append((d,stage))\n",
    "        offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "        for i in range(0,args[\"epochs_per_stage\"]):\n",
    "            indices = torch.arange(offset,len(d)+offset) # and then shuffle again (across epochs within stages)\n",
    "            orders_stages.append(indices[torch.randperm(len(indices))])\n",
    "    return datasets_stages, orders_stages\n",
    "        \n",
    "\n",
    "if True or not os.path.exists(args[\"dataset_folder\"]):\n",
    "    # pretraining data\n",
    "    torch.manual_seed(0)\n",
    "    datasets_stages, orders_stages = create_dataset(args[\"curriculum\"], args[\"raw_dataset_folder_babylm\"])\n",
    "    dataset = datasets.concatenate_datasets([d for d,_ in datasets_stages])\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "    torch.save(orders_stages, args[\"curriculum_path\"])\n",
    "\n",
    "    # eval data is a split of (100M dataset - 10M dataset)\n",
    "    # of size 0.05*len(10M dataset)\n",
    "    torch.manual_seed(0)\n",
    "    eval_datasets_stages, _ = create_dataset(args[\"curriculum\"], args[\"raw_eval_dataset_folder_babylm\"])\n",
    "\n",
    "\n",
    "\n",
    "    # remove all strings that are in the eval dataset \n",
    "    ## basically (100M dataset - 10M dataset) but one source at a time as that is faster\n",
    "    datasets_eval = []\n",
    "    lookup = {stage: d for d, stage in datasets_stages}\n",
    "    # print(lookup)\n",
    "    for e, stage in eval_datasets_stages:\n",
    "        if stage in lookup.keys():\n",
    "            l = list(set(e[\"text\"]) - set(lookup[stage][\"text\"]))\n",
    "            datasets_eval.append(datasets.Dataset.from_dict({\"text\": l, \"stage\": [stage]*len(l)}))\n",
    "        else:\n",
    "            print(\"warning: adding all of \",  stage )\n",
    "            datasets_eval.append(e)\n",
    "    dataset_eval = datasets.concatenate_datasets(datasets_eval)\n",
    "\n",
    "    # do a stratified split, requires casting to class\n",
    "    def copy_source_col(x):\n",
    "        x[\"stage_\"] = x[\"stage\"]\n",
    "        return x\n",
    "    dataset_eval = dataset_eval.map(copy_source_col,num_proc=NUM_PROC_MAP)\n",
    "    dataset_eval = dataset_eval.class_encode_column(\"stage_\")\n",
    "    dataset_eval = dataset_eval.train_test_split(test_size=int(len(dataset)*0.05), seed=42, stratify_by_column=\"stage_\",keep_in_memory=True)[\"test\"]\n",
    "    dataset_eval = dataset_eval.remove_columns(\"stage_\")\n",
    "\n",
    "    print(len(dataset), len(dataset_eval))\n",
    "    dataset_eval = dataset_eval.shuffle(seed=42)\n",
    "    \n",
    "    # save\n",
    "    datasets.Dataset.from_dict(dataset_eval.to_dict()).save_to_disk(args[\"eval_dataset_folder\"], num_proc=NUM_PROC_MAP ) # converting to dict and back to speed up substantially (or substantial slowdown caused by conversions above)\n",
    "\n",
    "\n",
    "\n",
    "    # assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "\n",
    "\n",
    "    len_full_dataset = len(dataset)\n",
    "    args[\"epoch_equivalents\"] = sum([len(o) for o in orders_stages]) / len_full_dataset\n",
    "    args[\"epochs\"] = len(orders_stages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with open(\"./configs/curriculum_10M_2024\", 'w') as f:\n",
    "        json.dump(args, f)\n",
    "args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e6567919984d0283176b831e230464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "source              \n",
       "childes.train           0.491936\n",
       "open_subtitles.train    0.305340\n",
       "bnc_spoken.train        0.076335\n",
       "gutenberg.train         0.055991\n",
       "simple_wiki.train       0.055131\n",
       "switchboard.train       0.015267\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "source\n",
       "bnc_spoken.train        0.093683\n",
       "childes.train           0.285278\n",
       "gutenberg.train         0.255128\n",
       "open_subtitles.train    0.205135\n",
       "simple_wiki.train       0.146029\n",
       "switchboard.train       0.014747\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "stage\n",
       "C1: Child Directed Speech    0.285278\n",
       "C2: Unscripted Dialogue      0.108430\n",
       "C3: Scripted Dialogue        0.205135\n",
       "C4: Wiki                     0.146029\n",
       "C5: Written English          0.255128\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "stage\n",
       "C1: Child Directed Speech    0.491936\n",
       "C2: Unscripted Dialogue      0.091602\n",
       "C3: Scripted Dialogue        0.305340\n",
       "C4: Wiki                     0.055131\n",
       "C5: Written English          0.055991\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "dataset = datasets.load_from_disk(args[\"dataset_folder\"])\n",
    "df = dataset.to_pandas()\n",
    "df[\"word_count\"] = df[\"text\"].apply(word_count)\n",
    "df[\"stage\"] = df[\"source\"].apply(lambda source: next((name for name, files in args[\"curriculum\"].items() if source in files), None))\n",
    "display(df.value_counts([\"source\"], normalize=True))\n",
    "display(df.groupby(by=[\"source\"])[\"word_count\"].sum() / df[\"word_count\"].sum())\n",
    "display(df.groupby(by=[\"stage\"])[\"word_count\"].sum() / df[\"word_count\"].sum())\n",
    "display(df.groupby(by=[\"stage\"])[\"word_count\"].count() / len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified 10M Curriculum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates 5 stages of equal size totaling 10M tokens from the raw datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir babylm_data_preprocessing/tmp\n",
    "!git clone https://github.com/babylm/babylm_data_preprocessing.git\n",
    "!git clone https://github.com/pgcorpus/gutenberg.git babylm_data_preprocessing/tmp/gutenberg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the raw datasets as described in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing)). Note that the link to `simplewiki` expired, so we use a more recent dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/preprocessed_data\n",
    "\n",
    "# !curl https://raw.githubusercontent.com/phueb/BabyBERTa/master/data/corpora/aochildes.txt > babylm_data_preprocessing/preprocessed_data/aochildes.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "# !curl https://raw.githubusercontent.com/NathanDuran/Switchboard-Corpus/master/swda_data/full_set.txt > babylm_data_preprocessing/preprocessed_data/switchboard.txt\n",
    "\n",
    "# !curl http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz > babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "# !tar -xvzf babylm_data_preprocessing/preprocessed_data/CBTest.tgz -C babylm_data_preprocessing/preprocessed_data\n",
    "# !mv babylm_data_preprocessing/preprocessed_data/CBTest/data/cbt_*  babylm_data_preprocessing/preprocessed_data/\n",
    "# !rm -rf babylm_data_preprocessing/preprocessed_data/CBTest babylm_data_preprocessing/preprocessed_data/CBTest.tgz\n",
    "\n",
    "# !gdown 1nbUCWCAvtqI1-WQxzmyqQmddgsZtzdpR\n",
    "# !unzip -o children_stories.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm children_stories.txt.zip\n",
    "\n",
    "# !gdown 1vW0o7K6Gj_IYTzriWEjmCnrylCWb8DbY\n",
    "# !unzip -o open_subtitles.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm open_subtitles.txt.zip\n",
    "\n",
    "# !gdown 19GipY95MW3LrfO_kArmIC0KYy7mfCb1l\n",
    "# !unzip -o wikipedia.txt.zip -d babylm_data_preprocessing/preprocessed_data\n",
    "# !rm wikipedia.txt.zip\n",
    "# !gdown 1R2xWtNeVX48RiFA7vErL1pNtws3XEsYP\n",
    "# !unzip -o qed.zip -d babylm_data_preprocessing/tmp\n",
    "# !rm qed.zip\n",
    "\n",
    "# %run babylm_data_preprocessing/preprocess_qed.py babylm_data_preprocessing/tmp/en babylm_data_preprocessing/tmp/qed\n",
    "# # !cat babylm_data_preprocessing/tmp/qed/* >> babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !find babylm_data_preprocessing/tmp/qed/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/qed.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/qed babylm_data_preprocessing/tmp/en \n",
    "# # simplewiki\n",
    "# !curl https://dumps.wikimedia.org/simplewiki/20241101/simplewiki-20241101-pages-articles.xml.bz2 > babylm_data_preprocessing/tmp/wiki.bz2 # TODO backup\n",
    "# !bzip2 -d babylm_data_preprocessing/tmp/wiki.bz2\n",
    "# !python -m wikiextractor.WikiExtractor babylm_data_preprocessing/tmp/wiki -o babylm_data_preprocessing/tmp/wiki_txt\n",
    "\n",
    "# # https://github.com/babylm/babylm_data_preprocessing/blob/main/preprocess_simple_wiki.py\n",
    "# # have to change working dir\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# out_file = open(os.path.join(\"babylm_data_preprocessing\", \"preprocessed_data/simple_wiki.txt\"), \"w\")\n",
    "# wiki_dir = os.path.join(\"babylm_data_preprocessing\",\"tmp\", \"wiki_txt\")\n",
    "# for d1 in os.listdir(wiki_dir):\n",
    "# \tfor f in os.listdir(os.path.join(wiki_dir, d1)):\n",
    "# \t\twith open(os.path.join(wiki_dir, d1, f)) as input:\n",
    "# \t\t\ttitle = None\n",
    "# \t\t\tdoc = []\n",
    "# \t\t\tfor line in input:\n",
    "# \t\t\t\tif line.startswith(\"<doc\"):\n",
    "# \t\t\t\t\tline = next(input)\n",
    "# \t\t\t\t\ttitle = line\n",
    "# \t\t\t\telif re.match(r\"^\\s*$\", line):\n",
    "# \t\t\t\t\tcontinue\n",
    "# \t\t\t\telif \"</doc>\" in line:\n",
    "# \t\t\t\t\tif len(doc) > 0:\n",
    "# \t\t\t\t\t\tout_file.write(title)\n",
    "# \t\t\t\t\t\tout_file.write(\"\".join(doc))\n",
    "# \t\t\t\t\t\tout_file.write(\"\\n\")\n",
    "# \t\t\t\t\t\tdoc = []\n",
    "# \t\t\t\telse:\n",
    "# \t\t\t\t\tdoc.append(line)\n",
    "# !rm -rf babylm_data_preprocessing/tmp/wiki babylm_data_preprocessing/tmp/wiki_txt\t\t\t\t\t\n",
    "\n",
    "# !mkdir babylm_data_preprocessing/tmp/bnc_spoken\n",
    "# !curl https://llds.ling-phil.ox.ac.uk/llds/xmlui/bitstream/handle/20.500.14106/2554/2554.zip > babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip\n",
    "# !unzip -q babylm_data_preprocessing/tmp/bnc_spoken/bnc.zip -d babylm_data_preprocessing/tmp/bnc_spoken/\n",
    "# !(for z in babylm_data_preprocessing/tmp/bnc_spoken/download/Texts/*; do for y in $z/*; do for x in $y/*; do sed '2q;d' $x | grep \"^<stext\" -q && cp $x babylm_data_preprocessing/tmp/bnc_spoken/; done; done; done)\n",
    "# %run babylm_data_preprocessing/preprocess_bnc.py babylm_data_preprocessing/tmp/bnc_spoken/ babylm_data_preprocessing/preprocessed_data/bnc_spoken.txt\n",
    "# !rm -rf babylm_data_preprocessing/tmp/bnc_spoken\n",
    "\n",
    "\n",
    "# the get_data.py script ignores the `metadata` param\n",
    "%cd babylm_data_preprocessing/tmp/gutenberg \n",
    "!mkdir metadata\n",
    "\n",
    "# this can take a day or two...\n",
    "%run get_data.py \n",
    "\n",
    "%cd babylm_data_preprocessing/tmp/gutenberg \n",
    "!mkdir metadata\n",
    "# the repo contains a tokenizer but for an unspecified version of nltk\n",
    "# we download a current one\n",
    "# see https://github.com/pgcorpus/gutenberg/issues/5\n",
    "import nltk\n",
    "nltk.data.path=[\"src/nltk_data\"]\n",
    "nltk.download('punkt_tab',download_dir='src/nltk_data')\n",
    "%run process_data.py\n",
    "%cd ../../..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"metadata/metadata.csv\").set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"language\"]== \"['en']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the babylm preprocessing repo: https://github.com/babylm/babylm_data_preprocessing/blob/main/get_gutenberg_modern_en.py\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"babylm_data_preprocessing/tmp/gutenberg/metadata/metadata.csv\")\n",
    "df_modern_en = df[(df[\"language\"] == \"['en']\") & (df[\"authoryearofbirth\"] > 1850)]\n",
    "modern_en_ids = set(df_modern_en[\"id\"])\n",
    "\n",
    "os.makedirs(\"babylm_data_preprocessing/tmp/gutenberg_modern_en\")\n",
    "for f in os.listdir(\"babylm_data_preprocessing/tmp/gutenberg/data/text\"): \n",
    "    if f.split(\"_\")[0] in modern_en_ids:\n",
    "        shutil.copyfile(\"babylm_data_preprocessing/tmp/gutenberg/data/text/\" + f, \"babylm_data_preprocessing/tmp/gutenberg_modern_en/\" + f)\n",
    "!find babylm_data_preprocessing/tmp/gutenberg_modern_en/ -type f -exec cat {} + > babylm_data_preprocessing/preprocessed_data/gutenberg.txt\n",
    "# you may delete the tmp/gutenberg dir now \n",
    "# # !rm -rf babylm_data_preprocessing/tmp/gutenberg\n",
    "# # !rm -rf metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"dataset_folder\"] = \"./curricula/datasets/curriculum_stratified_10M_2024\"\n",
    "args[\"curriculum_path\"] = \"./curricula/curriculum_stratified_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_stratified_10M_2024_eval\"\n",
    "args[\"epochs_per_stage\"] = 10\n",
    "args[\"raw_dataset_folder\"] = \"./babylm_data_preprocessing/preprocessed_data\"\n",
    "args[\"curriculum\"] = {\n",
    "    \"C1: Child Directed Speech\" : [\"aochildes.txt\", ],\n",
    "    \"C2: Children's Books\": [\"children_stories.txt\", \"cbt_test.txt\", \"cbt_train.txt\",\"cbt_valid.txt\"],\n",
    "    \"C3: Dialogue\": [\"switchboard.txt\", \"bnc_spoken.txt\", \"open_subtitles.txt\"],\n",
    "    \"C4: Educational\": [\"qed.txt\", \"simple_wiki.txt\"],\n",
    "    \"C5: Written English\": [\"wikipedia.txt\",]# \"gutenberg.txt\"]\n",
    "\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10_000_000*0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets_stages = []\n",
    "datasets_stages_eval = []\n",
    "orders_stages = []\n",
    "\n",
    "BUDGET_PER_STAGE = 10_000_000 // len(args[\"curriculum\"])\n",
    "SIZE_EVAL_SPLIT = 0.05\n",
    "\n",
    "def get_train_test_splits_for_stage(d):\n",
    "    d = d.shuffle(seed=42)\n",
    "    words_in_train_split = 0\n",
    "    i = 0\n",
    "    wc = word_count(d[i][\"text\"])\n",
    "    while (BUDGET_PER_STAGE >= (words_in_train_split + wc)) and (i < len(d)):\n",
    "        words_in_train_split += wc\n",
    "        i+=1\n",
    "        wc = word_count(d[i][\"text\"])\n",
    "\n",
    "    words_in_eval_split = 0\n",
    "    j = i+1\n",
    "    wc = word_count(d[j][\"text\"])\n",
    "    while ((BUDGET_PER_STAGE*SIZE_EVAL_SPLIT) >= (words_in_eval_split + wc)) and (j < len(d)):\n",
    "        words_in_eval_split += wc\n",
    "        j+=1\n",
    "        wc = word_count(d[j][\"text\"])\n",
    "\n",
    "    #      pretrain                               eval\n",
    "    return datasets.Dataset.from_dict(d[0:i]), datasets.Dataset.from_dict(d[i+1:j+1]) \n",
    "\n",
    "if not os.path.exists(args[\"dataset_folder\"]):\n",
    "    torch.manual_seed(0)\n",
    "    chunks = { stage: datasets.concatenate_datasets(\n",
    "                [\n",
    "                    load_dataset(\"text\", data_files =os.path.join(args[\"raw_dataset_folder\"], file),download_mode=\"force_redownload\")[\"train\"] \n",
    "                    .map(lambda entry: add_source(entry, file), num_proc=NUM_PROC_MAP)\n",
    "                    for file in files\n",
    "                ]\n",
    "            ) for stage, files in args[\"curriculum\"].items()}\n",
    "    # print(\"Calculating counts\")\n",
    "    # counts = {name: word_count_dataset(d) for name,d in chunks.items()}\n",
    "    # print(counts)\n",
    "    for name,d in chunks.items():\n",
    "        print(\"Processing\", name)\n",
    "        d, d_eval = get_train_test_splits_for_stage(d)\n",
    "        datasets_stages.append(d)\n",
    "        datasets_stages_eval.append(d_eval)\n",
    "        offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "        for i in range(0,args[\"epochs_per_stage\"]):\n",
    "            indices = torch.arange(offset,len(d)+offset) # and then shuffle again (across epochs within stages)\n",
    "            orders_stages.append(indices[torch.randperm(len(indices))])\n",
    "        \n",
    "\n",
    "            \n",
    "    # pretraining data\n",
    "    dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "    torch.save(orders_stages, args[\"curriculum_path\"])\n",
    "\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    dataset_eval = datasets.concatenate_datasets(datasets_stages_eval)\n",
    "    dataset_eval.save_to_disk(args[\"eval_dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "    len_full_dataset = len(dataset)\n",
    "    args[\"epoch_equivalents\"] = sum([len(o) for o in orders_stages]) / len_full_dataset\n",
    "    args[\"epochs\"] = len(orders_stages)\n",
    "\n",
    "    with open(\"./configs/curriculum_stratified_10M_2024\", 'w') as f:\n",
    "        json.dump(args, f)\n",
    "    args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert word_count_dataset(dataset) <= 10_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_from_disk(args[\"dataset_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = datasets.load_from_disk(args[\"dataset_folder\"])\n",
    "df = dataset.to_pandas()\n",
    "df[\"word_count\"] = df[\"text\"].apply(word_count)\n",
    "df[\"stage\"] = df[\"source\"].apply(lambda source: next((name for name, files in args[\"curriculum\"].items() if source in files), None))\n",
    "display(df.value_counts([\"source\"]))\n",
    "display(df.groupby(by=[\"source\"])[\"word_count\"].sum() / df[\"word_count\"].sum())\n",
    "display(df.groupby(by=[\"stage\"])[\"word_count\"].sum() / df[\"word_count\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.value_counts([\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"source\"])[\"word_count\"].sum() / df[\"word_count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"stage\"])[\"word_count\"].sum() / df[\"word_count\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(counts.values()) > BUDGET_PER_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5*BUDGET_PER_STAGE)*0.05asdf32wedscx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(counts.values())*len(args[\"curriculum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(args[\"curriculum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mimicks the default behaviour of Huggingface's `Trainer`: Randomly shuffle dataset after each epoch.\n",
    "All data is shown at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./curricula/datasets/curriculum_10M_2024\",# reuse one created above\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/random_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024_eval\"\n",
    "args[\"epochs\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "dataset = datasets.load_from_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP,download_mode=\"force_redownload\") # reuse one created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.sort(torch.randperm(len(dataset))).values, torch.arange(0,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"epochs\"] = 10\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/random_10M_2024\")\n",
    "with open(\"./configs/random_10M_2024\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4401|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4579|±  |0.0019|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shuffle with a proportion of the documents beeing random tokens as a control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./curricula/datasets/curriculum_10M_2024_noisy\",# reuse one created above\n",
    "   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/random_10M_2024_noisy\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/curriculum_10M_2024_eval\"\n",
    "args[\"epochs\"] = 10\n",
    "\n",
    "args[\"proportion_noise\"] = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\",num_proc=NUM_PROC_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\", max_len=512)\n",
    "def replace_with_noise(example):\n",
    "    tokenized = tokenizer(example[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512)[\"input_ids\"]\n",
    "    example[\"text\"] = tokenizer.decode(torch.randint(0, tokenizer.vocab_size, (len(tokenized),)), skip_special_tokens=True)\n",
    "    example[\"source\"] = \"noise\"\n",
    "    return example\n",
    "replace_with_noise(dataset[1000000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "torch.manual_seed(0)\n",
    "ind_to_replace_with_noise = torch.randperm(int(len(dataset)*args[\"proportion_noise\"]))\n",
    "dl = dataset.to_list()\n",
    "for i in ind_to_replace_with_noise:\n",
    "    dl[i] = replace_with_noise(dl[i])\n",
    "from collections import ChainMap\n",
    "\n",
    "\n",
    "dataset = datasets.Dataset.from_dict({k: [line[k] for line in dl] for k in dl[0]})\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, args[\"curriculum_path\"])\n",
    "with open(\"./configs/random_10M_2024_noisy\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_to_replace_with_noise[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./curricula/datasets/test\",   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/test_random_curriculum\"\n",
    "args[\"eval_dataset_folder\"] = \"./curricula/datasets/test_eval\"\n",
    "args[\"epochs\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "import json\n",
    "#dataset = load_dataset(\"text\", data_dir =\"/data/loriss21dm/babylm/train_test\", cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\",num_proc=NUM_PROC_MAP,download_mode=\"force_redownload\") # reuse one created above\n",
    "dataset = dataset.shuffle()\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//1000]) # TODO\n",
    "dataset.save_to_disk(args[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, args[\"curriculum_path\"])\n",
    "\n",
    "\n",
    "dataset = datasets.Dataset.from_dict(dataset[0:len(dataset)//10]) # TODO\n",
    "dataset.save_to_disk(args[\"eval_dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "\n",
    "with open(\"./configs/test\", 'w') as f:\n",
    "    json.dump(args, f)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights =  norm.pdf(indices, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "\n",
    "\n",
    "def f_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    print(indices)\n",
    "    weights = f.pdf(indices+1, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "def lognorm_filter(size, **args):\n",
    "    #print(args)\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    #print(indices)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./configs/random_10M_2024\") as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "config[\"influence_output_dir\"] = os.path.join(\"./influence\", os.path.basename(config[\"curriculum_path\"]))\n",
    "dataset = datasets.load_from_disk(config[\"dataset_folder\"],num_proc=NUM_PROC_MAP)\n",
    "df = pd.DataFrame({int(result_checkpoint): torch.load(os.path.join(config[\"influence_output_dir\"],result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(config[\"influence_output_dir\"])})\n",
    "df.sort_index(axis=1)\n",
    "\n",
    "df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "influence_cols = df.columns\n",
    "df[\"total\"] = df.sum(axis=1)\n",
    "df[[\"text\", \"source\"]] = dataset.to_pandas()\n",
    "df[\"document_lenght\"] = df[\"text\"].str.split(r\"\\S+\").str.len()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "\n",
    "\n",
    "filter_weights = lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)\n",
    "plt.plot(np.arange(-len(influence_cols),len(influence_cols)+1, 1),filter_weights)\n",
    "plt.vlines([0], ymin=0, ymax=max(filter_weights)+0.1, colors=[\"red\"])\n",
    "df_reweighted = reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5))\n",
    "display(df_reweighted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(packed_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": config[\"dataset_folder\"],   \n",
    "}\n",
    "args[\"raw_dataset_folder_babylm\"] = \"./train_10M\"\n",
    "args[\"curriculum_path\"] = \"./curricula/lognorm_curriculum_from_random_10M_2024\"\n",
    "args[\"eval_dataset_folder\"] = config[\"eval_dataset_folder\"]\n",
    "args[\"epochs\"] = config[\"epochs\"]\n",
    "\n",
    "torch.save(torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]), args[\"curriculum_path\"])\n",
    "\n",
    "\n",
    "with open(\"./configs/lognorm_curriculum_from_random_10M_2024\", 'w') as f:\n",
    "    json.dump(args, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
