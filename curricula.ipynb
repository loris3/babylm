{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "    \"epochs\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘curricula’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir curricula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum 2023 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact data from Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/cache'\n",
    "# dev = load_dataset(\"babylm-anon/dev-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7991a55a2118429ba25a5bd9c3c92b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1058753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LAST_GEN_DIR = \"last_gen\"\n",
    "EPOCHS_PER_STAGE = 1\n",
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for folder in (sorted(os.listdir(LAST_GEN_DIR))):\n",
    "    d = load_dataset(\"text\", data_dir =os.path.join(LAST_GEN_DIR, folder), cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "    d = d.shuffle() # we shuffle within the stage\n",
    "    d = datasets.Dataset.from_dict(d[0:len(d)//10]) # TODO\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    for _ in range(0,EPOCHS_PER_STAGE):\n",
    "        orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_100M_2023\")\n",
    "#assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, \"./curricula/curriculum_100M_2023\")\n",
    "len(orders_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999980165384799"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((len(o) for o in orders_stages)) / 10587551 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1058753\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35],\n",
       "          [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "           17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "           34, 35]]]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]).repeat(1,10,1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum with 10M 2024 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training with the strategy in Thoma et al 2023. Note that the model does not see all examples in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"dataset_folder\"] = \"./train_10M\"\n",
    "curriculum = {\n",
    "    \"C1\": [\"childes.train\", \"open_subtitles.train\"],\n",
    "    \"C2\": [\"switchboard.train\", \"bnc_spoken.train\"],\n",
    "    \"C3\": [\"gutenberg.train\", \"simple_wiki.train\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5e57d230334d6b9850f1a2cd029961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([     0,      1,      2,  ..., 939997, 939998, 939999]),\n",
       " tensor([ 940000,  940001,  940002,  ..., 1047997, 1047998, 1047999]),\n",
       " tensor([1048000, 1048001, 1048002,  ..., 1179011, 1179012, 1179013])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_stages = []\n",
    "orders_stages = []\n",
    "\n",
    "torch.manual_seed(0)\n",
    "for stage, files in curriculum.items():\n",
    "    d = datasets.concatenate_datasets(\n",
    "            [\n",
    "                load_dataset(\"text\", data_files =os.path.join(args[\"dataset_folder\"], file))[\"train\"] \n",
    "                for file in files\n",
    "            ]\n",
    "        )\n",
    "    d = d.shuffle() # we shuffle with the stage\n",
    "    datasets_stages.append(d)\n",
    "    offset = orders_stages[-1][-1]+1 if len(orders_stages) else 0\n",
    "    orders_stages.append(torch.arange(offset,len(d)+offset))\n",
    "    \n",
    "\n",
    "\n",
    "dataset = datasets.concatenate_datasets(datasets_stages)\n",
    "dataset.save_to_disk(\"./curricula/datasets/curriculum_10M_2024\")\n",
    "assert torch.equal(torch.cat([o.flatten() for o in orders_stages]),torch.arange(0,len(dataset)))\n",
    "\n",
    "torch.save(orders_stages, \"./curricula/curriculum_10M_2024\")\n",
    "orders_stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mimicks the default behaviour of Huggingface's `Trainer`: Randomly shuffle dataset after each epoch.\n",
    "All data is shown at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = datasets.load_from_disk(\"./curricula/datasets/curriculum_10M_2024\") # reuse one created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'set_epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_epoch\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mha\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'set_epoch'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset.set_epoch(\"ha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.equal(torch.sort(torch.randperm(len(dataset))).values, torch.arange(0,len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"epochs\"] = 3\n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/random_curriculum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1179014])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curriculum Convolution with lognorm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory /data/loriss21dm/babylm/train_test is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/loriss21dm/babylm/train_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# reuse one created above\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/datasets/load.py:2671\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2672\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2673\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory /data/loriss21dm/babylm/train_test is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_dir =\"/data/loriss21dm/babylm/train_test\", cache_dir='/data/loriss21dm/cache')[\"train\"] \n",
    "import torch \n",
    "torch.manual_seed(0)\n",
    "random_order = torch.stack([torch.randperm(len(dataset)) for _ in range(0,args[\"epochs\"])])\n",
    "torch.save(random_order, \"./curricula/test_random_curriculum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
