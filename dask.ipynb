{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the model\n",
    "# Use Dask's `map_blocks` to apply the gradient computation function to each chunk of data\n",
    "def gradient_task(a, b):\n",
    "    device=\"cuda:0\"\n",
    "    print(torch.cuda.current_device())\n",
    "    model_config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "    model = RobertaForMaskedLM(config=model_config).to(device)\n",
    "    model.train()\n",
    "    print(len(a), len(b))\n",
    "    gradients_a = torch.stack([get_loss_gradient(model, example,device).detach().cpu().flatten() for example in a])\n",
    "    gradients_b = torch.stack([get_loss_gradient(model, example,device).detach().cpu().flatten() for example in b])\n",
    "    print(gradients_a.shape, gradients_b.shape)\n",
    "    print(\"torch.matmul(gradients_a, gradients_b.T).numpy()\", torch.matmul(gradients_a, gradients_b.T).numpy().shape)\n",
    "    return torch.matmul(gradients_a, gradients_b.T).numpy()\n",
    "# Apply the gradient computation function across the Dask arrays\n",
    "\n",
    "gradients = da.map_blocks(gradient_task, input_data, target_data, dtype=np.float32)\n",
    "\n",
    "# Trigger computation to get the gradients\n",
    "computed_gradients = gradients.compute()\n",
    "\n",
    "# Print out the gradients\n",
    "print(computed_gradients.shape)\n",
    "\n",
    "# Optionally, you can shut down the Dask client after the computation is complete\n",
    "# client.close()\n",
    "torch.save(computed_gradients, \"computed_gradients\")\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
