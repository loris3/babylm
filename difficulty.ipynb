{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 09:55:04.615034: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-19 09:55:04.646200: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-19 09:55:04.646220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-19 09:55:04.647351: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-19 09:55:04.653172: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-19 09:55:05.185087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "from datasets import load_dataset\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "\n",
    "from nltk.lm import MLE\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.util import everygrams\n",
    "def get_order_increasing_difficulty_perplexity_ngram(dataset_name=\"loris3/stratified_10m_curriculum\", n_gram=1, train_subsample_factor = 1):\n",
    "    \"\"\"\n",
    "    Returns a training data order by increasing difficulty estimated with an ngram MLE model\n",
    "    code from https://github.com/codebyzeb/CLIMB/blob/dc4f13c1bdf94938d468f0926368dee048485cba/src/data_curriculum/difficulty_scorer/perplexity.py#L77\n",
    "    adapted to accept hf datasets\n",
    "    \"\"\"\n",
    "    out_path = os.path.join(\"./difficulty_ngram\",str(n_gram), os.path.basename(dataset_name))\n",
    "    print(out_path)\n",
    "    if os.path.isdir(out_path):\n",
    "        print(\"Skipping {}, already calculated\".format(out_path) )\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(dataset_name+\"_random\", max_len=512)\n",
    "\n",
    "        dataset = load_dataset(dataset_name)[\"train\"]\n",
    "        dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "        train_data_n_grams = [\n",
    "            list(everygrams(sent, max_len=n_gram))\n",
    "            for sent in  \n",
    "            (\n",
    "                [str(id) for id in input_ids if id != tokenizer.pad_token_id]  # type: ignore\n",
    "                    for input_ids in dataset[0 : dataset.num_rows : train_subsample_factor][\"input_ids\"]\n",
    "            )\n",
    "        ]\n",
    "        train_vocab = [str(val) for val in tokenizer.vocab.values()]\n",
    "\n",
    "        print(\"Fitting {}-gram model\".format(n_gram))\n",
    "        lm = MLE(n_gram)\n",
    "        lm.fit(train_data_n_grams, train_vocab)\n",
    "        print(\"Getting perplexity with {}-gram model\".format(n_gram))\n",
    "        df = pd.DataFrame([lm.perplexity(example) for example in train_data_n_grams])\n",
    "        df.columns=[\"perplexity\"]\n",
    "        df.index.name = \"document_id\"\n",
    "        os.makedirs(out_path)\n",
    "        df.to_parquet(os.path.join(out_path,\"perplexity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"loris3/stratified_10m_curriculum\"\n",
    "model_name_random = \"loris3/stratified_10m_curriculum_random\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./difficulty_ngram/1/stratified_10m_curriculum\n",
      "Skipping ./difficulty_ngram/1/stratified_10m_curriculum, already calculated\n",
      "./difficulty_ngram/2/stratified_10m_curriculum\n",
      "Skipping ./difficulty_ngram/2/stratified_10m_curriculum, already calculated\n",
      "./difficulty_ngram/3/stratified_10m_curriculum\n",
      "Skipping ./difficulty_ngram/3/stratified_10m_curriculum, already calculated\n"
     ]
    }
   ],
   "source": [
    "# %run python compute_difficulty.py loris3/stratified_10m_curriculum_lognorm loris3/stratified_10m_curriculum\n",
    "get_order_increasing_difficulty_perplexity_ngram()\n",
    "get_order_increasing_difficulty_perplexity_ngram(n_gram=2)\n",
    "get_order_increasing_difficulty_perplexity_ngram(n_gram=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random, curriculum_random = plotting.load_data_for_plotting(dataset_name, \"random.pt\")\n",
    "df_lognorm, curriculum_lognorm = plotting.load_data_for_plotting(dataset_name, \"lognorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>522</th>\n",
       "      <th>1044</th>\n",
       "      <th>1566</th>\n",
       "      <th>2088</th>\n",
       "      <th>2610</th>\n",
       "      <th>3132</th>\n",
       "      <th>3654</th>\n",
       "      <th>4176</th>\n",
       "      <th>4698</th>\n",
       "      <th>5220</th>\n",
       "      <th>total</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>stage</th>\n",
       "      <th>document_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>huh .</td>\n",
       "      <td>aochildes.txt</td>\n",
       "      <td>C1: Child Directed Speech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027546</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132045</td>\n",
       "      <td>michael .</td>\n",
       "      <td>aochildes.txt</td>\n",
       "      <td>C1: Child Directed Speech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009099</td>\n",
       "      <td>done !</td>\n",
       "      <td>aochildes.txt</td>\n",
       "      <td>C1: Child Directed Speech</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022737</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>-0.003944</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>-0.007843</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>you wanna put that in there okay don't knock i...</td>\n",
       "      <td>aochildes.txt</td>\n",
       "      <td>C1: Child Directed Speech</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>0.038032</td>\n",
       "      <td>0.084605</td>\n",
       "      <td>the female is half brown ?</td>\n",
       "      <td>aochildes.txt</td>\n",
       "      <td>C1: Child Directed Speech</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070316</th>\n",
       "      <td>0.002649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>-0.008489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004518</td>\n",
       "      <td>-0.005588</td>\n",
       "      <td>suppose the sun to be somewhere. If her Majest...</td>\n",
       "      <td>gutenberg.txt</td>\n",
       "      <td>C5: Written English</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070317</th>\n",
       "      <td>-0.000687</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>-0.002393</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.002793</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>reputed to be) that of a Mrs. Leakey, an amiab...</td>\n",
       "      <td>gutenberg.txt</td>\n",
       "      <td>C5: Written English</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070318</th>\n",
       "      <td>0.010429</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>-0.000278</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>0.003484</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>the people. Hokosa was the accuser. In brief a...</td>\n",
       "      <td>gutenberg.txt</td>\n",
       "      <td>C5: Written English</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070319</th>\n",
       "      <td>0.000208</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>-0.003385</td>\n",
       "      <td>0.037689</td>\n",
       "      <td>-0.001175</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>0.010440</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.050271</td>\n",
       "      <td>himself. One thing and one alone had developed...</td>\n",
       "      <td>gutenberg.txt</td>\n",
       "      <td>C5: Written English</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070320</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td>gutenberg.txt</td>\n",
       "      <td>C5: Written English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070321 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              522      1044      1566      2088      2610      3132      3654  \\\n",
       "0        0.000000  0.000000 -0.003507  0.000000  0.000000  0.000000  0.000000   \n",
       "1        0.000000  0.000000  0.027546  0.067730  0.000000  0.036769  0.000000   \n",
       "2        0.000000 -0.018569  0.000000  0.000000  0.001924  0.000000  0.000000   \n",
       "3        0.022737 -0.000732  0.003498  0.010173  0.000192 -0.003944  0.007749   \n",
       "4       -0.003658  0.000000  0.000000  0.006162  0.022301  0.013265  0.000000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "1070316  0.002649  0.000000  0.003593  0.000392 -0.000005  0.000791 -0.008489   \n",
       "1070317 -0.000687  0.004710  0.000156 -0.002393  0.001327 -0.000153 -0.002793   \n",
       "1070318  0.010429  0.002751  0.002663 -0.000278 -0.000148  0.003484  0.005186   \n",
       "1070319  0.000208 -0.000634  0.004017 -0.003385  0.037689 -0.001175  0.002618   \n",
       "1070320  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "             4176      4698      5220     total  \\\n",
       "0        0.000000  0.000000  0.000000 -0.003507   \n",
       "1        0.000000  0.000000  0.000000  0.132045   \n",
       "2        0.000000  0.007546  0.000000 -0.009099   \n",
       "3        0.003467  0.006356 -0.007843  0.041652   \n",
       "4       -0.000656  0.009159  0.038032  0.084605   \n",
       "...           ...       ...       ...       ...   \n",
       "1070316  0.000000  0.000000 -0.004518 -0.005588   \n",
       "1070317  0.000738  0.001345  0.000492  0.002741   \n",
       "1070318 -0.000485  0.004988  0.002661  0.031250   \n",
       "1070319 -0.002486  0.010440  0.002980  0.050271   \n",
       "1070320  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                                      text         source  \\\n",
       "0                                                    huh .  aochildes.txt   \n",
       "1                                                michael .  aochildes.txt   \n",
       "2                                                   done !  aochildes.txt   \n",
       "3        you wanna put that in there okay don't knock i...  aochildes.txt   \n",
       "4                               the female is half brown ?  aochildes.txt   \n",
       "...                                                    ...            ...   \n",
       "1070316  suppose the sun to be somewhere. If her Majest...  gutenberg.txt   \n",
       "1070317  reputed to be) that of a Mrs. Leakey, an amiab...  gutenberg.txt   \n",
       "1070318  the people. Hokosa was the accuser. In brief a...  gutenberg.txt   \n",
       "1070319  himself. One thing and one alone had developed...  gutenberg.txt   \n",
       "1070320                                                     gutenberg.txt   \n",
       "\n",
       "                             stage  document_lenght  \n",
       "0        C1: Child Directed Speech                2  \n",
       "1        C1: Child Directed Speech                2  \n",
       "2        C1: Child Directed Speech                2  \n",
       "3        C1: Child Directed Speech               16  \n",
       "4        C1: Child Directed Speech                6  \n",
       "...                            ...              ...  \n",
       "1070316        C5: Written English               12  \n",
       "1070317        C5: Written English               15  \n",
       "1070318        C5: Written English               11  \n",
       "1070319        C5: Written English               12  \n",
       "1070320        C5: Written English                0  \n",
       "\n",
       "[1070321 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./difficulty/stratified_10m_curriculum_random'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\"./difficulty\", os.path.basename(model_name_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perplexity = pd.read_parquet(os.path.join(\"./difficulty\", os.path.basename(dataset_name), os.path.basename(model_name_random)))\n",
    "df_perplexity_unigram = pd.read_parquet(os.path.join(\"./difficulty_ngram\", \"1\", os.path.basename(dataset_name)))\n",
    "df_perplexity_bigram = pd.read_parquet(os.path.join(\"./difficulty_ngram\", \"2\", os.path.basename(dataset_name)))\n",
    "df_perplexity_trigram = pd.read_parquet(os.path.join(\"./difficulty_ngram\", \"3\", os.path.basename(dataset_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01664380168658156"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random[\"total\"].corr(-df_perplexity[\"perplexity\"], method=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=100, threads_per_worker=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Document Lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/distributed/client.py:3371: UserWarning: Sending large graph of size 73.08 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from lexical_diversity import lex_div as ld\n",
    "out_path = os.path.join(\"./difficulty_metrics\", os.path.basename(dataset_name))\n",
    "if not os.path.exists(out_path):\n",
    "    \n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "\n",
    "    def mean_sentence_length(doc):\n",
    "        l = [sum((1 for word in sentence if word.is_alpha)) for sentence in nlp(doc).sents]\n",
    "        if len(l) > 0:\n",
    "            return np.mean(l)\n",
    "    def ttr(doc):\n",
    "        l = [token.text.lower() for token in nlp(doc) if token.is_alpha]\n",
    "        if len(l) > 0:\n",
    "            return ld.ttr(l)\n",
    "    def log_ttr(doc):\n",
    "        l = [token.text.lower() for token in nlp(doc) if token.is_alpha]\n",
    "        if len(l) > 0:\n",
    "            return ld.log_ttr(l)\n",
    "    def mattr(doc):\n",
    "        l = [token.text.lower() for token in nlp(doc) if token.is_alpha]\n",
    "        if len(l) > 0:\n",
    "            return ld.mattr(l,window_length=5)\n",
    "    # def ttr(doc):\n",
    "    #     w = [token for token in doc if token.is_alpha]\n",
    "        \n",
    "           \n",
    "    ddf = dd.from_pandas(df_random[[\"text\"]], npartitions=100)\n",
    "\n",
    "\n",
    "    ddf['mean_sentence_length'] = ddf[\"text\"].map_partitions(\n",
    "    lambda series: series.apply(mean_sentence_length),  meta=('mean_sentence_length', 'float64')\n",
    "    )\n",
    "    ddf['ttr'] = ddf[\"text\"].map_partitions(\n",
    "    lambda series: series.apply(ttr),  meta=('ttr', 'float64')\n",
    "    )\n",
    "    ddf['log_ttr'] = ddf[\"text\"].map_partitions(\n",
    "    lambda series: series.apply(log_ttr),  meta=('log_ttr', 'float64')\n",
    "    )\n",
    "    ddf['mattr'] = ddf[\"text\"].map_partitions(\n",
    "    lambda series: series.apply(mattr),  meta=('mattr', 'float64')\n",
    "    )\n",
    "    os.makedirs(out_path)\n",
    "    df = ddf.compute()\n",
    "    df.index.name = \"document_id\"\n",
    "    df.to_parquet(os.path.join(out_path,\"mean_sentence_length\"))\n",
    "df_metrics = pd.read_parquet(os.path.join(\"./difficulty_metrics\", os.path.basename(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>mean_sentence_length</th>\n",
       "      <th>ttr</th>\n",
       "      <th>log_ttr</th>\n",
       "      <th>mattr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>huh .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>michael .</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>done !</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you wanna put that in there okay don't knock i...</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.974523</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the female is half brown ?</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070316</th>\n",
       "      <td>suppose the sun to be somewhere. If her Majest...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.964984</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070317</th>\n",
       "      <td>reputed to be) that of a Mrs. Leakey, an amiab...</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.971919</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070318</th>\n",
       "      <td>the people. Hokosa was the accuser. In brief a...</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.960253</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070319</th>\n",
       "      <td>himself. One thing and one alone had developed...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.964984</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070320</th>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070321 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          text  \\\n",
       "document_id                                                      \n",
       "0                                                        huh .   \n",
       "1                                                    michael .   \n",
       "2                                                       done !   \n",
       "3            you wanna put that in there okay don't knock i...   \n",
       "4                                   the female is half brown ?   \n",
       "...                                                        ...   \n",
       "1070316      suppose the sun to be somewhere. If her Majest...   \n",
       "1070317      reputed to be) that of a Mrs. Leakey, an amiab...   \n",
       "1070318      the people. Hokosa was the accuser. In brief a...   \n",
       "1070319      himself. One thing and one alone had developed...   \n",
       "1070320                                                          \n",
       "\n",
       "             mean_sentence_length       ttr   log_ttr     mattr  \n",
       "document_id                                                      \n",
       "0                        1.000000  1.000000  0.000000  1.000000  \n",
       "1                        1.000000  1.000000  0.000000  1.000000  \n",
       "2                        1.000000  1.000000  0.000000  1.000000  \n",
       "3                       15.000000  0.933333  0.974523  1.000000  \n",
       "4                        5.000000  1.000000  1.000000  1.000000  \n",
       "...                           ...       ...       ...       ...  \n",
       "1070316                  6.000000  0.916667  0.964984  1.000000  \n",
       "1070317                 14.000000  0.928571  0.971919  1.000000  \n",
       "1070318                  3.666667  0.909091  0.960253  0.971429  \n",
       "1070319                  4.000000  0.916667  0.964984  0.950000  \n",
       "1070320                       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[1070321 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070316</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070317</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070318</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070319</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070320</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070321 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "...     ..\n",
       "1070316  5\n",
       "1070317  5\n",
       "1070318  5\n",
       "1070319  5\n",
       "1070320  5\n",
       "\n",
       "[1070321 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1\n",
       "          ..\n",
       "1070316    5\n",
       "1070317    5\n",
       "1070318    5\n",
       "1070319    5\n",
       "1070320    5\n",
       "Name: 0, Length: 1070321, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_random[\"stage\"].str.extract(r'C(\\d*):')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, kendalltau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kendalltau', 'model_perplexity', 'model_perplexity', 0.9999999999999999),\n",
       " ('spearmanr', 'model_perplexity', 'model_perplexity', 0.9999999999999998),\n",
       " ('kendalltau',\n",
       "  'model_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.042368644476989605),\n",
       " ('spearmanr',\n",
       "  'model_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.06365845317424748),\n",
       " ('kendalltau',\n",
       "  'model_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.04580167967264772),\n",
       " ('spearmanr',\n",
       "  'model_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.06769610826976691),\n",
       " ('kendalltau', 'model_perplexity', 'ttr', 0.023393417550644325),\n",
       " ('spearmanr', 'model_perplexity', 'ttr', 0.034403858319413304),\n",
       " ('kendalltau', 'model_perplexity', 'log_ttr', 0.05254933738850327),\n",
       " ('spearmanr', 'model_perplexity', 'log_ttr', 0.0778685145832621),\n",
       " ('kendalltau', 'model_perplexity', 'mattr', 0.030766834248427646),\n",
       " ('spearmanr', 'model_perplexity', 'mattr', 0.04541304649716088),\n",
       " ('kendalltau',\n",
       "  'model_perplexity',\n",
       "  'unigram_perplexity',\n",
       "  0.016778820716843806),\n",
       " ('spearmanr', 'model_perplexity', 'unigram_perplexity', 0.025033652727731656),\n",
       " ('kendalltau', 'model_perplexity', 'bigram_perplexity', 0.018816123319684287),\n",
       " ('spearmanr', 'model_perplexity', 'bigram_perplexity', 0.02804067684618926),\n",
       " ('kendalltau', 'model_perplexity', 'trigram_perplexity', 0.02860296218030668),\n",
       " ('spearmanr', 'model_perplexity', 'trigram_perplexity', 0.04267723153884813),\n",
       " ('kendalltau', 'model_perplexity', 'source_difficulty', 0.07517745946514803),\n",
       " ('spearmanr', 'model_perplexity', 'source_difficulty', 0.1120431349014711),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'model_perplexity',\n",
       "  0.042368644476989605),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'model_perplexity',\n",
       "  0.0636584531742475),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'model_tracin_influence',\n",
       "  0.9999999999999999),\n",
       " ('spearmanr', 'model_tracin_influence', 'model_tracin_influence', 1.0),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'mean_sentence_length',\n",
       "  0.04559829454612877),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'mean_sentence_length',\n",
       "  0.06848096725473465),\n",
       " ('kendalltau', 'model_tracin_influence', 'ttr', -0.002035544990537303),\n",
       " ('spearmanr', 'model_tracin_influence', 'ttr', -0.003997243756773192),\n",
       " ('kendalltau', 'model_tracin_influence', 'log_ttr', 0.014420470551656912),\n",
       " ('spearmanr', 'model_tracin_influence', 'log_ttr', 0.02132049932210492),\n",
       " ('kendalltau', 'model_tracin_influence', 'mattr', 0.05718370584833569),\n",
       " ('spearmanr', 'model_tracin_influence', 'mattr', 0.0845703248175069),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'unigram_perplexity',\n",
       "  0.033765591898928006),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'unigram_perplexity',\n",
       "  0.05039263545277574),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'bigram_perplexity',\n",
       "  0.0348158941273528),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'bigram_perplexity',\n",
       "  0.05194556906329467),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'trigram_perplexity',\n",
       "  0.046986650110769336),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'trigram_perplexity',\n",
       "  0.07052911890894144),\n",
       " ('kendalltau',\n",
       "  'model_tracin_influence',\n",
       "  'source_difficulty',\n",
       "  0.18992120992142936),\n",
       " ('spearmanr',\n",
       "  'model_tracin_influence',\n",
       "  'source_difficulty',\n",
       "  0.2839081496421307),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'model_perplexity',\n",
       "  0.04580167967264772),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'model_perplexity',\n",
       "  0.06769610826976691),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'model_tracin_influence',\n",
       "  0.04559829454612877),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'model_tracin_influence',\n",
       "  0.06848096725473465),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'mean_sentence_length',\n",
       "  0.9999999999999999),\n",
       " ('spearmanr', 'mean_sentence_length', 'mean_sentence_length', 1.0),\n",
       " ('kendalltau', 'mean_sentence_length', 'ttr', 0.0618730400335693),\n",
       " ('spearmanr', 'mean_sentence_length', 'ttr', 0.09212355944552178),\n",
       " ('kendalltau', 'mean_sentence_length', 'log_ttr', 0.04663378582155305),\n",
       " ('spearmanr', 'mean_sentence_length', 'log_ttr', 0.0691398316562021),\n",
       " ('kendalltau', 'mean_sentence_length', 'mattr', 0.10499547845406336),\n",
       " ('spearmanr', 'mean_sentence_length', 'mattr', 0.1565368426657165),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'unigram_perplexity',\n",
       "  0.0828105535988323),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'unigram_perplexity',\n",
       "  0.12419695909117455),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'bigram_perplexity',\n",
       "  0.08818233431415369),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'bigram_perplexity',\n",
       "  0.13225312769154132),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'trigram_perplexity',\n",
       "  0.08114337830384742),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'trigram_perplexity',\n",
       "  0.12170278970359079),\n",
       " ('kendalltau',\n",
       "  'mean_sentence_length',\n",
       "  'source_difficulty',\n",
       "  0.18267256207679516),\n",
       " ('spearmanr',\n",
       "  'mean_sentence_length',\n",
       "  'source_difficulty',\n",
       "  0.2750122261798012),\n",
       " ('kendalltau', 'ttr', 'model_perplexity', 0.023393417550644325),\n",
       " ('spearmanr', 'ttr', 'model_perplexity', 0.034403858319413304),\n",
       " ('kendalltau', 'ttr', 'model_tracin_influence', -0.002035544990537303),\n",
       " ('spearmanr', 'ttr', 'model_tracin_influence', -0.003997243756773192),\n",
       " ('kendalltau', 'ttr', 'mean_sentence_length', 0.0618730400335693),\n",
       " ('spearmanr', 'ttr', 'mean_sentence_length', 0.09212355944552178),\n",
       " ('kendalltau', 'ttr', 'ttr', 0.9999999999999999),\n",
       " ('spearmanr', 'ttr', 'ttr', 0.9999999999999998),\n",
       " ('kendalltau', 'ttr', 'log_ttr', 0.08632212270651657),\n",
       " ('spearmanr', 'ttr', 'log_ttr', 0.07636331248223407),\n",
       " ('kendalltau', 'ttr', 'mattr', 0.23097073887502267),\n",
       " ('spearmanr', 'ttr', 'mattr', 0.21043458008452048),\n",
       " ('kendalltau', 'ttr', 'unigram_perplexity', 0.055007934245544586),\n",
       " ('spearmanr', 'ttr', 'unigram_perplexity', 0.08181612371874451),\n",
       " ('kendalltau', 'ttr', 'bigram_perplexity', 0.057872073473968916),\n",
       " ('spearmanr', 'ttr', 'bigram_perplexity', 0.08575291787517783),\n",
       " ('kendalltau', 'ttr', 'trigram_perplexity', 0.061366171232949025),\n",
       " ('spearmanr', 'ttr', 'trigram_perplexity', 0.09093473489216845),\n",
       " ('kendalltau', 'ttr', 'source_difficulty', 0.11062261698535507),\n",
       " ('spearmanr', 'ttr', 'source_difficulty', 0.14220187555242605),\n",
       " ('kendalltau', 'log_ttr', 'model_perplexity', 0.05254933738850327),\n",
       " ('spearmanr', 'log_ttr', 'model_perplexity', 0.0778685145832621),\n",
       " ('kendalltau', 'log_ttr', 'model_tracin_influence', 0.014420470551656912),\n",
       " ('spearmanr', 'log_ttr', 'model_tracin_influence', 0.021320499322104915),\n",
       " ('kendalltau', 'log_ttr', 'mean_sentence_length', 0.04663378582155305),\n",
       " ('spearmanr', 'log_ttr', 'mean_sentence_length', 0.0691398316562021),\n",
       " ('kendalltau', 'log_ttr', 'ttr', 0.08632212270651657),\n",
       " ('spearmanr', 'log_ttr', 'ttr', 0.07636331248223407),\n",
       " ('kendalltau', 'log_ttr', 'log_ttr', 0.9999999999999999),\n",
       " ('spearmanr', 'log_ttr', 'log_ttr', 0.9999999999999998),\n",
       " ('kendalltau', 'log_ttr', 'mattr', 0.04254088164530227),\n",
       " ('spearmanr', 'log_ttr', 'mattr', 0.01020261960296928),\n",
       " ('kendalltau', 'log_ttr', 'unigram_perplexity', 0.028654956877709067),\n",
       " ('spearmanr', 'log_ttr', 'unigram_perplexity', 0.04238497317141808),\n",
       " ('kendalltau', 'log_ttr', 'bigram_perplexity', 0.031032109528709276),\n",
       " ('spearmanr', 'log_ttr', 'bigram_perplexity', 0.045765688804501374),\n",
       " ('kendalltau', 'log_ttr', 'trigram_perplexity', 0.03211628410624102),\n",
       " ('spearmanr', 'log_ttr', 'trigram_perplexity', 0.0475017268247781),\n",
       " ('kendalltau', 'log_ttr', 'source_difficulty', -0.010451157403379208),\n",
       " ('spearmanr', 'log_ttr', 'source_difficulty', -0.016878344888194088),\n",
       " ('kendalltau', 'mattr', 'model_perplexity', 0.030766834248427646),\n",
       " ('spearmanr', 'mattr', 'model_perplexity', 0.04541304649716088),\n",
       " ('kendalltau', 'mattr', 'model_tracin_influence', 0.05718370584833569),\n",
       " ('spearmanr', 'mattr', 'model_tracin_influence', 0.0845703248175069),\n",
       " ('kendalltau', 'mattr', 'mean_sentence_length', 0.10499547845406336),\n",
       " ('spearmanr', 'mattr', 'mean_sentence_length', 0.1565368426657165),\n",
       " ('kendalltau', 'mattr', 'ttr', 0.23097073887502267),\n",
       " ('spearmanr', 'mattr', 'ttr', 0.21043458008452048),\n",
       " ('kendalltau', 'mattr', 'log_ttr', 0.04254088164530227),\n",
       " ('spearmanr', 'mattr', 'log_ttr', 0.010202619602969282),\n",
       " ('kendalltau', 'mattr', 'mattr', 0.9999999999999999),\n",
       " ('spearmanr', 'mattr', 'mattr', 1.0),\n",
       " ('kendalltau', 'mattr', 'unigram_perplexity', 0.09600803038017142),\n",
       " ('spearmanr', 'mattr', 'unigram_perplexity', 0.14225868860954152),\n",
       " ('kendalltau', 'mattr', 'bigram_perplexity', 0.10372574013442742),\n",
       " ('spearmanr', 'mattr', 'bigram_perplexity', 0.15324708335507647),\n",
       " ('kendalltau', 'mattr', 'trigram_perplexity', 0.13187917550464467),\n",
       " ('spearmanr', 'mattr', 'trigram_perplexity', 0.19487656535266382),\n",
       " ('kendalltau', 'mattr', 'source_difficulty', 0.3293507870667845),\n",
       " ('spearmanr', 'mattr', 'source_difficulty', 0.4344020158733757),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'model_perplexity',\n",
       "  0.016778820716843806),\n",
       " ('spearmanr', 'unigram_perplexity', 'model_perplexity', 0.025033652727731656),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.033765591898928006),\n",
       " ('spearmanr',\n",
       "  'unigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.05039263545277574),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.0828105535988323),\n",
       " ('spearmanr',\n",
       "  'unigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.12419695909117455),\n",
       " ('kendalltau', 'unigram_perplexity', 'ttr', 0.055007934245544586),\n",
       " ('spearmanr', 'unigram_perplexity', 'ttr', 0.08181612371874453),\n",
       " ('kendalltau', 'unigram_perplexity', 'log_ttr', 0.028654956877709067),\n",
       " ('spearmanr', 'unigram_perplexity', 'log_ttr', 0.04238497317141809),\n",
       " ('kendalltau', 'unigram_perplexity', 'mattr', 0.09600803038017142),\n",
       " ('spearmanr', 'unigram_perplexity', 'mattr', 0.14225868860954152),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'unigram_perplexity',\n",
       "  0.9999999999999999),\n",
       " ('spearmanr', 'unigram_perplexity', 'unigram_perplexity', 1.0),\n",
       " ('kendalltau', 'unigram_perplexity', 'bigram_perplexity', 0.1547864299760767),\n",
       " ('spearmanr', 'unigram_perplexity', 'bigram_perplexity', 0.23068562459061223),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'trigram_perplexity',\n",
       "  0.06606483236548685),\n",
       " ('spearmanr',\n",
       "  'unigram_perplexity',\n",
       "  'trigram_perplexity',\n",
       "  0.09862309591258832),\n",
       " ('kendalltau',\n",
       "  'unigram_perplexity',\n",
       "  'source_difficulty',\n",
       "  0.19197456486816886),\n",
       " ('spearmanr', 'unigram_perplexity', 'source_difficulty', 0.2844655044768861),\n",
       " ('kendalltau', 'bigram_perplexity', 'model_perplexity', 0.018816123319684287),\n",
       " ('spearmanr', 'bigram_perplexity', 'model_perplexity', 0.028040676846189265),\n",
       " ('kendalltau',\n",
       "  'bigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.0348158941273528),\n",
       " ('spearmanr',\n",
       "  'bigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.05194556906329468),\n",
       " ('kendalltau',\n",
       "  'bigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.08818233431415369),\n",
       " ('spearmanr',\n",
       "  'bigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.13225312769154132),\n",
       " ('kendalltau', 'bigram_perplexity', 'ttr', 0.057872073473968916),\n",
       " ('spearmanr', 'bigram_perplexity', 'ttr', 0.08575291787517783),\n",
       " ('kendalltau', 'bigram_perplexity', 'log_ttr', 0.031032109528709276),\n",
       " ('spearmanr', 'bigram_perplexity', 'log_ttr', 0.04576568880450137),\n",
       " ('kendalltau', 'bigram_perplexity', 'mattr', 0.10372574013442742),\n",
       " ('spearmanr', 'bigram_perplexity', 'mattr', 0.15324708335507647),\n",
       " ('kendalltau', 'bigram_perplexity', 'unigram_perplexity', 0.1547864299760767),\n",
       " ('spearmanr', 'bigram_perplexity', 'unigram_perplexity', 0.23068562459061226),\n",
       " ('kendalltau', 'bigram_perplexity', 'bigram_perplexity', 0.9999999999999999),\n",
       " ('spearmanr', 'bigram_perplexity', 'bigram_perplexity', 1.0),\n",
       " ('kendalltau',\n",
       "  'bigram_perplexity',\n",
       "  'trigram_perplexity',\n",
       "  0.07236166468691675),\n",
       " ('spearmanr', 'bigram_perplexity', 'trigram_perplexity', 0.10803775704782298),\n",
       " ('kendalltau', 'bigram_perplexity', 'source_difficulty', 0.2035802479618895),\n",
       " ('spearmanr', 'bigram_perplexity', 'source_difficulty', 0.30029864046742055),\n",
       " ('kendalltau', 'trigram_perplexity', 'model_perplexity', 0.02860296218030668),\n",
       " ('spearmanr', 'trigram_perplexity', 'model_perplexity', 0.042677231538848136),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.046986650110769336),\n",
       " ('spearmanr',\n",
       "  'trigram_perplexity',\n",
       "  'model_tracin_influence',\n",
       "  0.07052911890894144),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.08114337830384742),\n",
       " ('spearmanr',\n",
       "  'trigram_perplexity',\n",
       "  'mean_sentence_length',\n",
       "  0.12170278970359079),\n",
       " ('kendalltau', 'trigram_perplexity', 'ttr', 0.061366171232949025),\n",
       " ('spearmanr', 'trigram_perplexity', 'ttr', 0.09093473489216845),\n",
       " ('kendalltau', 'trigram_perplexity', 'log_ttr', 0.03211628410624102),\n",
       " ('spearmanr', 'trigram_perplexity', 'log_ttr', 0.0475017268247781),\n",
       " ('kendalltau', 'trigram_perplexity', 'mattr', 0.13187917550464467),\n",
       " ('spearmanr', 'trigram_perplexity', 'mattr', 0.19487656535266382),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'unigram_perplexity',\n",
       "  0.06606483236548685),\n",
       " ('spearmanr',\n",
       "  'trigram_perplexity',\n",
       "  'unigram_perplexity',\n",
       "  0.09862309591258832),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'bigram_perplexity',\n",
       "  0.07236166468691675),\n",
       " ('spearmanr', 'trigram_perplexity', 'bigram_perplexity', 0.10803775704782298),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'trigram_perplexity',\n",
       "  0.9999999999999999),\n",
       " ('spearmanr', 'trigram_perplexity', 'trigram_perplexity', 1.0),\n",
       " ('kendalltau',\n",
       "  'trigram_perplexity',\n",
       "  'source_difficulty',\n",
       "  0.20451124818483013),\n",
       " ('spearmanr', 'trigram_perplexity', 'source_difficulty', 0.29947580143563346),\n",
       " ('kendalltau', 'source_difficulty', 'model_perplexity', 0.07517745946514803),\n",
       " ('spearmanr', 'source_difficulty', 'model_perplexity', 0.11204313490147108),\n",
       " ('kendalltau',\n",
       "  'source_difficulty',\n",
       "  'model_tracin_influence',\n",
       "  0.18992120992142936),\n",
       " ('spearmanr',\n",
       "  'source_difficulty',\n",
       "  'model_tracin_influence',\n",
       "  0.28390814964213074),\n",
       " ('kendalltau',\n",
       "  'source_difficulty',\n",
       "  'mean_sentence_length',\n",
       "  0.18267256207679516),\n",
       " ('spearmanr',\n",
       "  'source_difficulty',\n",
       "  'mean_sentence_length',\n",
       "  0.2750122261798012),\n",
       " ('kendalltau', 'source_difficulty', 'ttr', 0.11062261698535507),\n",
       " ('spearmanr', 'source_difficulty', 'ttr', 0.14220187555242608),\n",
       " ('kendalltau', 'source_difficulty', 'log_ttr', -0.010451157403379208),\n",
       " ('spearmanr', 'source_difficulty', 'log_ttr', -0.016878344888194088),\n",
       " ('kendalltau', 'source_difficulty', 'mattr', 0.3293507870667845),\n",
       " ('spearmanr', 'source_difficulty', 'mattr', 0.43440201587337574),\n",
       " ('kendalltau',\n",
       "  'source_difficulty',\n",
       "  'unigram_perplexity',\n",
       "  0.19197456486816886),\n",
       " ('spearmanr', 'source_difficulty', 'unigram_perplexity', 0.2844655044768861),\n",
       " ('kendalltau', 'source_difficulty', 'bigram_perplexity', 0.2035802479618895),\n",
       " ('spearmanr', 'source_difficulty', 'bigram_perplexity', 0.30029864046742055),\n",
       " ('kendalltau',\n",
       "  'source_difficulty',\n",
       "  'trigram_perplexity',\n",
       "  0.20451124818483013),\n",
       " ('spearmanr', 'source_difficulty', 'trigram_perplexity', 0.29947580143563346),\n",
       " ('kendalltau', 'source_difficulty', 'source_difficulty', 0.9999999999999999),\n",
       " ('spearmanr', 'source_difficulty', 'source_difficulty', 0.9999999999999998)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "order_increasing_difficulty_perplexity = lambda d: d.reindex(df_random.index)[\"perplexity\"].sort_values(ascending=True).index\n",
    "def vals():\n",
    "    for (a, a_name), (b, b_name) in itertools.product(\n",
    "        [\n",
    "            (order_increasing_difficulty_perplexity(df_perplexity), \"model_perplexity\"),\n",
    "            (df_random[\"total\"].sort_values(ascending=False).index, \"model_tracin_influence\"),\n",
    "            (df_metrics[\"mean_sentence_length\"].sort_values(ascending=True).index, \"mean_sentence_length\"),\n",
    "            (df_metrics[\"ttr\"].sort_values(ascending=True).index, \"ttr\"),\n",
    "            (df_metrics[\"log_ttr\"].sort_values(ascending=True).index, \"log_ttr\"),\n",
    "            (df_metrics[\"mattr\"].sort_values(ascending=True).index, \"mattr\"),\n",
    "            (order_increasing_difficulty_perplexity(df_perplexity_unigram), \"unigram_perplexity\"),\n",
    "            (order_increasing_difficulty_perplexity(df_perplexity_bigram), \"bigram_perplexity\"),\n",
    "            (order_increasing_difficulty_perplexity(df_perplexity_trigram), \"trigram_perplexity\"),\n",
    "            (df_random[\"stage\"].str.extract(r'C(\\d*):')[0].sort_values(ascending=True).index,\"source_difficulty\")\n",
    "        ], repeat=2\n",
    "    ):\n",
    "        yield [\n",
    "            (\"kendalltau\", a_name, b_name, kendalltau(a, b, nan_policy=\"omit\")[0]),\n",
    "            (\"spearmanr\", a_name, b_name, spearmanr(a, b, nan_policy=\"omit\")[0]),\n",
    "        ]\n",
    "a = list(itertools.chain(*vals()))\n",
    "a\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>model_tracin_influence</td>\n",
       "      <td>0.042369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>model_tracin_influence</td>\n",
       "      <td>0.063658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model_perplexity</td>\n",
       "      <td>mean_sentence_length</td>\n",
       "      <td>0.045802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>bigram_perplexity</td>\n",
       "      <td>0.300299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>trigram_perplexity</td>\n",
       "      <td>0.204511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>trigram_perplexity</td>\n",
       "      <td>0.299476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>source_difficulty</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                       1         2\n",
       "0     model_perplexity        model_perplexity  1.000000\n",
       "1     model_perplexity        model_perplexity  1.000000\n",
       "2     model_perplexity  model_tracin_influence  0.042369\n",
       "3     model_perplexity  model_tracin_influence  0.063658\n",
       "4     model_perplexity    mean_sentence_length  0.045802\n",
       "..                 ...                     ...       ...\n",
       "195  source_difficulty       bigram_perplexity  0.300299\n",
       "196  source_difficulty      trigram_perplexity  0.204511\n",
       "197  source_difficulty      trigram_perplexity  0.299476\n",
       "198  source_difficulty       source_difficulty  1.000000\n",
       "199  source_difficulty       source_difficulty  1.000000\n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m v \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(a)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/frame.py:9312\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[0;34m(self, columns, index, values)\u001b[0m\n\u001b[1;32m   9305\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9306\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\n\u001b[1;32m   9308\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, columns, index\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default, values\u001b[38;5;241m=\u001b[39mlib\u001b[38;5;241m.\u001b[39mno_default\n\u001b[1;32m   9309\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9310\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[0;32m-> 9312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:570\u001b[0m, in \u001b[0;36mpivot\u001b[0;34m(data, columns, index, values)\u001b[0m\n\u001b[1;32m    566\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mindexed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    572\u001b[0m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames\n\u001b[1;32m    573\u001b[0m ]\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/series.py:4595\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[0;34m(self, level, fill_value, sort)\u001b[0m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4551\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[1;32m   4552\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4591\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[0;32m-> 4595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:517\u001b[0m, in \u001b[0;36munstack\u001b[0;34m(obj, level, fill_value, sort)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort\u001b[38;5;241m=\u001b[39msort)\n\u001b[0;32m--> 517\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[1;32m    521\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:154\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[0;34m(self, index, level, constructor, sort)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[1;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    150\u001b[0m         PerformanceWarning,\n\u001b[1;32m    151\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    152\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/loriss21dm/miniforge3/envs/babylm_venv/lib/python3.10/site-packages/pandas/core/reshape/reshape.py:210\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[0;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "v = pd.DataFrame(a)\n",
    "import seaborn as sns\n",
    "sns.heatmap(v.pivot(index=0, columns=1, values=2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('model_perplexity', 'model_perplexity', 0.9999999999999999),\n",
       "  ('model_perplexity', 'model_perplexity', 0.9999999999999998)],\n",
       " [('model_perplexity', 'model_tracin_influence', 0.042368644476989605),\n",
       "  ('model_perplexity', 'model_tracin_influence', 0.06365845317424748)],\n",
       " [('model_perplexity', 'mean_sentence_length', 0.04580167967264772),\n",
       "  ('model_perplexity', 'mean_sentence_length', 0.06769610826976691)],\n",
       " [('model_perplexity', 'ttr', 0.023393417550644325),\n",
       "  ('model_perplexity', 'ttr', 0.034403858319413304)],\n",
       " [('model_perplexity', 'log_ttr', 0.05254933738850327),\n",
       "  ('model_perplexity', 'log_ttr', 0.0778685145832621)],\n",
       " [('model_perplexity', 'mattr', 0.030766834248427646),\n",
       "  ('model_perplexity', 'mattr', 0.04541304649716088)],\n",
       " [('model_perplexity', 'unigram_perplexity', 0.016778820716843806),\n",
       "  ('model_perplexity', 'unigram_perplexity', 0.025033652727731656)],\n",
       " [('model_perplexity', 'bigram_perplexity', 0.018816123319684287),\n",
       "  ('model_perplexity', 'bigram_perplexity', 0.02804067684618926)],\n",
       " [('model_perplexity', 'trigram_perplexity', 0.02860296218030668),\n",
       "  ('model_perplexity', 'trigram_perplexity', 0.04267723153884813)],\n",
       " [('model_perplexity', 'source_difficulty', 0.07517745946514803),\n",
       "  ('model_perplexity', 'source_difficulty', 0.1120431349014711)],\n",
       " [('model_tracin_influence', 'model_perplexity', 0.042368644476989605),\n",
       "  ('model_tracin_influence', 'model_perplexity', 0.0636584531742475)],\n",
       " [('model_tracin_influence', 'model_tracin_influence', 0.9999999999999999),\n",
       "  ('model_tracin_influence', 'model_tracin_influence', 1.0)],\n",
       " [('model_tracin_influence', 'mean_sentence_length', 0.04559829454612877),\n",
       "  ('model_tracin_influence', 'mean_sentence_length', 0.06848096725473465)],\n",
       " [('model_tracin_influence', 'ttr', -0.002035544990537303),\n",
       "  ('model_tracin_influence', 'ttr', -0.003997243756773192)],\n",
       " [('model_tracin_influence', 'log_ttr', 0.014420470551656912),\n",
       "  ('model_tracin_influence', 'log_ttr', 0.02132049932210492)],\n",
       " [('model_tracin_influence', 'mattr', 0.05718370584833569),\n",
       "  ('model_tracin_influence', 'mattr', 0.0845703248175069)],\n",
       " [('model_tracin_influence', 'unigram_perplexity', 0.033765591898928006),\n",
       "  ('model_tracin_influence', 'unigram_perplexity', 0.05039263545277574)],\n",
       " [('model_tracin_influence', 'bigram_perplexity', 0.0348158941273528),\n",
       "  ('model_tracin_influence', 'bigram_perplexity', 0.05194556906329467)],\n",
       " [('model_tracin_influence', 'trigram_perplexity', 0.046986650110769336),\n",
       "  ('model_tracin_influence', 'trigram_perplexity', 0.07052911890894144)],\n",
       " [('model_tracin_influence', 'source_difficulty', 0.18992120992142936),\n",
       "  ('model_tracin_influence', 'source_difficulty', 0.2839081496421307)],\n",
       " [('mean_sentence_length', 'model_perplexity', 0.04580167967264772),\n",
       "  ('mean_sentence_length', 'model_perplexity', 0.06769610826976691)],\n",
       " [('mean_sentence_length', 'model_tracin_influence', 0.04559829454612877),\n",
       "  ('mean_sentence_length', 'model_tracin_influence', 0.06848096725473465)],\n",
       " [('mean_sentence_length', 'mean_sentence_length', 0.9999999999999999),\n",
       "  ('mean_sentence_length', 'mean_sentence_length', 1.0)],\n",
       " [('mean_sentence_length', 'ttr', 0.0618730400335693),\n",
       "  ('mean_sentence_length', 'ttr', 0.09212355944552178)],\n",
       " [('mean_sentence_length', 'log_ttr', 0.04663378582155305),\n",
       "  ('mean_sentence_length', 'log_ttr', 0.0691398316562021)],\n",
       " [('mean_sentence_length', 'mattr', 0.10499547845406336),\n",
       "  ('mean_sentence_length', 'mattr', 0.1565368426657165)],\n",
       " [('mean_sentence_length', 'unigram_perplexity', 0.0828105535988323),\n",
       "  ('mean_sentence_length', 'unigram_perplexity', 0.12419695909117455)],\n",
       " [('mean_sentence_length', 'bigram_perplexity', 0.08818233431415369),\n",
       "  ('mean_sentence_length', 'bigram_perplexity', 0.13225312769154132)],\n",
       " [('mean_sentence_length', 'trigram_perplexity', 0.08114337830384742),\n",
       "  ('mean_sentence_length', 'trigram_perplexity', 0.12170278970359079)],\n",
       " [('mean_sentence_length', 'source_difficulty', 0.18267256207679516),\n",
       "  ('mean_sentence_length', 'source_difficulty', 0.2750122261798012)],\n",
       " [('ttr', 'model_perplexity', 0.023393417550644325),\n",
       "  ('ttr', 'model_perplexity', 0.034403858319413304)],\n",
       " [('ttr', 'model_tracin_influence', -0.002035544990537303),\n",
       "  ('ttr', 'model_tracin_influence', -0.003997243756773192)],\n",
       " [('ttr', 'mean_sentence_length', 0.0618730400335693),\n",
       "  ('ttr', 'mean_sentence_length', 0.09212355944552178)],\n",
       " [('ttr', 'ttr', 0.9999999999999999), ('ttr', 'ttr', 0.9999999999999998)],\n",
       " [('ttr', 'log_ttr', 0.08632212270651657),\n",
       "  ('ttr', 'log_ttr', 0.07636331248223407)],\n",
       " [('ttr', 'mattr', 0.23097073887502267),\n",
       "  ('ttr', 'mattr', 0.21043458008452048)],\n",
       " [('ttr', 'unigram_perplexity', 0.055007934245544586),\n",
       "  ('ttr', 'unigram_perplexity', 0.08181612371874451)],\n",
       " [('ttr', 'bigram_perplexity', 0.057872073473968916),\n",
       "  ('ttr', 'bigram_perplexity', 0.08575291787517783)],\n",
       " [('ttr', 'trigram_perplexity', 0.061366171232949025),\n",
       "  ('ttr', 'trigram_perplexity', 0.09093473489216845)],\n",
       " [('ttr', 'source_difficulty', 0.11062261698535507),\n",
       "  ('ttr', 'source_difficulty', 0.14220187555242605)],\n",
       " [('log_ttr', 'model_perplexity', 0.05254933738850327),\n",
       "  ('log_ttr', 'model_perplexity', 0.0778685145832621)],\n",
       " [('log_ttr', 'model_tracin_influence', 0.014420470551656912),\n",
       "  ('log_ttr', 'model_tracin_influence', 0.021320499322104915)],\n",
       " [('log_ttr', 'mean_sentence_length', 0.04663378582155305),\n",
       "  ('log_ttr', 'mean_sentence_length', 0.0691398316562021)],\n",
       " [('log_ttr', 'ttr', 0.08632212270651657),\n",
       "  ('log_ttr', 'ttr', 0.07636331248223407)],\n",
       " [('log_ttr', 'log_ttr', 0.9999999999999999),\n",
       "  ('log_ttr', 'log_ttr', 0.9999999999999998)],\n",
       " [('log_ttr', 'mattr', 0.04254088164530227),\n",
       "  ('log_ttr', 'mattr', 0.01020261960296928)],\n",
       " [('log_ttr', 'unigram_perplexity', 0.028654956877709067),\n",
       "  ('log_ttr', 'unigram_perplexity', 0.04238497317141808)],\n",
       " [('log_ttr', 'bigram_perplexity', 0.031032109528709276),\n",
       "  ('log_ttr', 'bigram_perplexity', 0.045765688804501374)],\n",
       " [('log_ttr', 'trigram_perplexity', 0.03211628410624102),\n",
       "  ('log_ttr', 'trigram_perplexity', 0.0475017268247781)],\n",
       " [('log_ttr', 'source_difficulty', -0.010451157403379208),\n",
       "  ('log_ttr', 'source_difficulty', -0.016878344888194088)],\n",
       " [('mattr', 'model_perplexity', 0.030766834248427646),\n",
       "  ('mattr', 'model_perplexity', 0.04541304649716088)],\n",
       " [('mattr', 'model_tracin_influence', 0.05718370584833569),\n",
       "  ('mattr', 'model_tracin_influence', 0.0845703248175069)],\n",
       " [('mattr', 'mean_sentence_length', 0.10499547845406336),\n",
       "  ('mattr', 'mean_sentence_length', 0.1565368426657165)],\n",
       " [('mattr', 'ttr', 0.23097073887502267),\n",
       "  ('mattr', 'ttr', 0.21043458008452048)],\n",
       " [('mattr', 'log_ttr', 0.04254088164530227),\n",
       "  ('mattr', 'log_ttr', 0.010202619602969282)],\n",
       " [('mattr', 'mattr', 0.9999999999999999), ('mattr', 'mattr', 1.0)],\n",
       " [('mattr', 'unigram_perplexity', 0.09600803038017142),\n",
       "  ('mattr', 'unigram_perplexity', 0.14225868860954152)],\n",
       " [('mattr', 'bigram_perplexity', 0.10372574013442742),\n",
       "  ('mattr', 'bigram_perplexity', 0.15324708335507647)],\n",
       " [('mattr', 'trigram_perplexity', 0.13187917550464467),\n",
       "  ('mattr', 'trigram_perplexity', 0.19487656535266382)],\n",
       " [('mattr', 'source_difficulty', 0.3293507870667845),\n",
       "  ('mattr', 'source_difficulty', 0.4344020158733757)],\n",
       " [('unigram_perplexity', 'model_perplexity', 0.016778820716843806),\n",
       "  ('unigram_perplexity', 'model_perplexity', 0.025033652727731656)],\n",
       " [('unigram_perplexity', 'model_tracin_influence', 0.033765591898928006),\n",
       "  ('unigram_perplexity', 'model_tracin_influence', 0.05039263545277574)],\n",
       " [('unigram_perplexity', 'mean_sentence_length', 0.0828105535988323),\n",
       "  ('unigram_perplexity', 'mean_sentence_length', 0.12419695909117455)],\n",
       " [('unigram_perplexity', 'ttr', 0.055007934245544586),\n",
       "  ('unigram_perplexity', 'ttr', 0.08181612371874453)],\n",
       " [('unigram_perplexity', 'log_ttr', 0.028654956877709067),\n",
       "  ('unigram_perplexity', 'log_ttr', 0.04238497317141809)],\n",
       " [('unigram_perplexity', 'mattr', 0.09600803038017142),\n",
       "  ('unigram_perplexity', 'mattr', 0.14225868860954152)],\n",
       " [('unigram_perplexity', 'unigram_perplexity', 0.9999999999999999),\n",
       "  ('unigram_perplexity', 'unigram_perplexity', 1.0)],\n",
       " [('unigram_perplexity', 'bigram_perplexity', 0.1547864299760767),\n",
       "  ('unigram_perplexity', 'bigram_perplexity', 0.23068562459061223)],\n",
       " [('unigram_perplexity', 'trigram_perplexity', 0.06606483236548685),\n",
       "  ('unigram_perplexity', 'trigram_perplexity', 0.09862309591258832)],\n",
       " [('unigram_perplexity', 'source_difficulty', 0.19197456486816886),\n",
       "  ('unigram_perplexity', 'source_difficulty', 0.2844655044768861)],\n",
       " [('bigram_perplexity', 'model_perplexity', 0.018816123319684287),\n",
       "  ('bigram_perplexity', 'model_perplexity', 0.028040676846189265)],\n",
       " [('bigram_perplexity', 'model_tracin_influence', 0.0348158941273528),\n",
       "  ('bigram_perplexity', 'model_tracin_influence', 0.05194556906329468)],\n",
       " [('bigram_perplexity', 'mean_sentence_length', 0.08818233431415369),\n",
       "  ('bigram_perplexity', 'mean_sentence_length', 0.13225312769154132)],\n",
       " [('bigram_perplexity', 'ttr', 0.057872073473968916),\n",
       "  ('bigram_perplexity', 'ttr', 0.08575291787517783)],\n",
       " [('bigram_perplexity', 'log_ttr', 0.031032109528709276),\n",
       "  ('bigram_perplexity', 'log_ttr', 0.04576568880450137)],\n",
       " [('bigram_perplexity', 'mattr', 0.10372574013442742),\n",
       "  ('bigram_perplexity', 'mattr', 0.15324708335507647)],\n",
       " [('bigram_perplexity', 'unigram_perplexity', 0.1547864299760767),\n",
       "  ('bigram_perplexity', 'unigram_perplexity', 0.23068562459061226)],\n",
       " [('bigram_perplexity', 'bigram_perplexity', 0.9999999999999999),\n",
       "  ('bigram_perplexity', 'bigram_perplexity', 1.0)],\n",
       " [('bigram_perplexity', 'trigram_perplexity', 0.07236166468691675),\n",
       "  ('bigram_perplexity', 'trigram_perplexity', 0.10803775704782298)],\n",
       " [('bigram_perplexity', 'source_difficulty', 0.2035802479618895),\n",
       "  ('bigram_perplexity', 'source_difficulty', 0.30029864046742055)],\n",
       " [('trigram_perplexity', 'model_perplexity', 0.02860296218030668),\n",
       "  ('trigram_perplexity', 'model_perplexity', 0.042677231538848136)],\n",
       " [('trigram_perplexity', 'model_tracin_influence', 0.046986650110769336),\n",
       "  ('trigram_perplexity', 'model_tracin_influence', 0.07052911890894144)],\n",
       " [('trigram_perplexity', 'mean_sentence_length', 0.08114337830384742),\n",
       "  ('trigram_perplexity', 'mean_sentence_length', 0.12170278970359079)],\n",
       " [('trigram_perplexity', 'ttr', 0.061366171232949025),\n",
       "  ('trigram_perplexity', 'ttr', 0.09093473489216845)],\n",
       " [('trigram_perplexity', 'log_ttr', 0.03211628410624102),\n",
       "  ('trigram_perplexity', 'log_ttr', 0.0475017268247781)],\n",
       " [('trigram_perplexity', 'mattr', 0.13187917550464467),\n",
       "  ('trigram_perplexity', 'mattr', 0.19487656535266382)],\n",
       " [('trigram_perplexity', 'unigram_perplexity', 0.06606483236548685),\n",
       "  ('trigram_perplexity', 'unigram_perplexity', 0.09862309591258832)],\n",
       " [('trigram_perplexity', 'bigram_perplexity', 0.07236166468691675),\n",
       "  ('trigram_perplexity', 'bigram_perplexity', 0.10803775704782298)],\n",
       " [('trigram_perplexity', 'trigram_perplexity', 0.9999999999999999),\n",
       "  ('trigram_perplexity', 'trigram_perplexity', 1.0)],\n",
       " [('trigram_perplexity', 'source_difficulty', 0.20451124818483013),\n",
       "  ('trigram_perplexity', 'source_difficulty', 0.29947580143563346)],\n",
       " [('source_difficulty', 'model_perplexity', 0.07517745946514803),\n",
       "  ('source_difficulty', 'model_perplexity', 0.11204313490147108)],\n",
       " [('source_difficulty', 'model_tracin_influence', 0.18992120992142936),\n",
       "  ('source_difficulty', 'model_tracin_influence', 0.28390814964213074)],\n",
       " [('source_difficulty', 'mean_sentence_length', 0.18267256207679516),\n",
       "  ('source_difficulty', 'mean_sentence_length', 0.2750122261798012)],\n",
       " [('source_difficulty', 'ttr', 0.11062261698535507),\n",
       "  ('source_difficulty', 'ttr', 0.14220187555242608)],\n",
       " [('source_difficulty', 'log_ttr', -0.010451157403379208),\n",
       "  ('source_difficulty', 'log_ttr', -0.016878344888194088)],\n",
       " [('source_difficulty', 'mattr', 0.3293507870667845),\n",
       "  ('source_difficulty', 'mattr', 0.43440201587337574)],\n",
       " [('source_difficulty', 'unigram_perplexity', 0.19197456486816886),\n",
       "  ('source_difficulty', 'unigram_perplexity', 0.2844655044768861)],\n",
       " [('source_difficulty', 'bigram_perplexity', 0.2035802479618895),\n",
       "  ('source_difficulty', 'bigram_perplexity', 0.30029864046742055)],\n",
       " [('source_difficulty', 'trigram_perplexity', 0.20451124818483013),\n",
       "  ('source_difficulty', 'trigram_perplexity', 0.29947580143563346)],\n",
       " [('source_difficulty', 'source_difficulty', 0.9999999999999999),\n",
       "  ('source_difficulty', 'source_difficulty', 0.9999999999999998)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
