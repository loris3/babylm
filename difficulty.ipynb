{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "from datasets import load_dataset\n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"loris3/stratified_10m_curriculum\"\n",
    "df_random, curriculum_random = plotting.load_data_for_plotting(dataset_name, \"random.pt\")\n",
    "df_lognorm, curriculum_lognorm = plotting.load_data_for_plotting(dataset_name, \"lognorm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run python compute_difficulty.py loris3/stratified_10m_curriculum_lognorm loris3/stratified_10m_curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perplexity = pd.read_parquet(os.path.join(\"./difficulty_old\", os.path.basename(dataset_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06980971193661108"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lognorm[\"total\"].corr(-df_perplexity[\"perplexity\"], method=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_influence = df_lognorm[\"total\"].sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_increasing_difficulty_perplexity =df_perplexity.reindex(df_lognorm.index)[\"perplexity\"].sort_values(ascending=True).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0680596365649359"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, p_value = spearmanr(order_influence, order_increasing_difficulty_perplexity, nan_policy=\"omit\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_indices = [i for i in list(range(0,1040812+1000)) if i not in df_perplexity.index]\n",
    "# df_missing = pd.DataFrame([pd.NA for _ in missing_indices], index=missing_indices)\n",
    "# df_missing.columns=[\"perplexity\"]\n",
    "# df_missing.index.name = \"document_id\"\n",
    "# pd.concat([df_perplexity, df_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig,AutoConfig\n",
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_perplexity.corrwith(df_lognorm, numeric_only=True, drop=True, method=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='loris3/stratified_10m_curriculum_random', vocab_size=52000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"loris3/stratified_10m_curriculum_random\", max_len=512)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 40 (125635135.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 43\u001b[0;36m\u001b[0m\n\u001b[0;31m    return self.lm.perplexity(example)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 40\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/codebyzeb/CLIMB/blob/dc4f13c1bdf94938d468f0926368dee048485cba/src/data_curriculum/difficulty_scorer/perplexity.py#L77\n",
    "train_subsample_factor = 1\n",
    "n_gram = 1\n",
    "\n",
    "from nltk.util import everygrams\n",
    "\n",
    "def remove_padding_tokens(example_batch):\n",
    "    batch = {\"unpadded_input_ids\": []}\n",
    "    for example in example_batch[\"input_ids\"]:\n",
    "        batch[\"unpadded_input_ids\"].append(\n",
    "            [\n",
    "                str(_id)\n",
    "                for _id in example\n",
    "                if _id != self.tokenizer.pad_token_id  # type: ignore\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return batch\n",
    "\n",
    "# removing padding tokens from the dataset\n",
    "dataset = load_dataset(\"loris3/stratified_10m_curriculum\")[\"train\"]\n",
    "dataset = dataset.map(\n",
    "    remove_padding_tokens,\n",
    "    batched=True,\n",
    "    num_proc=64,\n",
    ")\n",
    "\n",
    "tokenized_text = dataset[\"unpadded_input_ids\"]\n",
    "\n",
    "# We split the tokenized dataset into n-gram that we can feed into the n-gram model\n",
    "train_data_n_grams = (\n",
    "    everygrams(sent, max_len=n_gram)\n",
    "    for sent in tokenized_text[\n",
    "        0 : dataset.num_rows : train_subsample_factor\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_vocab = [str(val) for val in self.tokenizer.vocab.values()]\n",
    "\n",
    "self.lm = MLE(self.n_gram)\n",
    "self.lm.fit(train_data_n_grams, train_vocab)\n",
    "\n",
    "def _compute_ngram_perplexity(\n",
    "self, example: Generator[Tuple[str], Any, None]\n",
    "):\n",
    "return self.lm.perplexity(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayblm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
