{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/hfcache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(\"TODO\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = [str(x) for x in Path(\"/data/loriss21dm/babylm/TODO/\").glob(\"checkpoint-*\") ]#if (int(str(x).split(\"-\")[-1]) % 10000) == 0]#if (int(str(x).split(\"-\")[-1] % 1000)) == 0]\n",
    "# checkpoints = checkpoints[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# from transformers import RobertaForMaskedLM\n",
    "\n",
    "\n",
    "# config = RobertaConfig.from_pretrained(checkpoints[0])\n",
    "# model = RobertaForMaskedLM(config=config)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [pipeline(\n",
    "#     \"fill-mask\",\n",
    "#     model=checkpoint,\n",
    "#     tokenizer=tokenizer\n",
    "# )(\"The sky is <mask>.\")[0][\"sequence\"]\n",
    "# for checkpoint in checkpoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# dataset = load_dataset(\"text\", data_dir=\"/data/loriss21dm/babylm/train_100M/\")\n",
    "# dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in dataset[\"train\"][0:10][\"input_ids\"]], batch_first=True, padding_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator.torch_mask_tokens(torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in dataset[\"train\"][0:10][\"input_ids\"]], batch_first=True, padding_value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(dataset[\"train\"][0][\"input_ids\"]).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator((torch.tensor(dataset[\"train\"][0][\"input_ids\"]),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss(model, logits, labels):\n",
    "#     shift_logits = logits[..., :-1, :].contiguous()\n",
    "#     shift_labels = labels[..., 1:].contiguous()\n",
    "#     loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#     shift_logits = shift_logits.view(-1, shift_logits.shape[-1])\n",
    "#     shift_labels = shift_labels.view(-1)\n",
    "#     # Enable model parallelism\n",
    "#     shift_labels = shift_labels.to(shift_logits.device)\n",
    "#     loss = loss_fct(shift_logits, shift_labels)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss(model, logits, labels):\n",
    "#     print(logits)\n",
    "#     print(labels)\n",
    "#     loss_fct = CrossEntropyLoss()\n",
    "#     masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loss_gradient(model, example):\n",
    "#     model.train()\n",
    "#     model.zero_grad()\n",
    "#     # input_ids, attention_mask = tokenizer([example], return_tensors='pt', return_token_type_ids=False,padding='max_length', truncation=True, max_length=512).values()\n",
    "#     input_ids, labels = data_collator((torch.tensor(dataset[\"train\"][0][\"input_ids\"]),)).values() # TODO make static maybe probably\n",
    "#     #print(\"labels\",labels)\n",
    "#    # print(input_ids.shape)\n",
    "#     # print(attention_mask)\n",
    "#     inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
    "#    # attention_mask = attention_mask\n",
    "#    # print(inputs_embeds.reshape(1,-1,2048).shape)\n",
    "\n",
    "#     inputs_embeds.retain_grad()\n",
    "\n",
    "#     outputs = model.forward(\n",
    "#        #     input_ids=input_ids,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#            # attention_mask=attention_mask,\n",
    "#             labels=labels.to(device)\n",
    "#        #     attention_bias=attention_bias,\n",
    "#        #     past_key_values=past_key_values,\n",
    "#        #     use_cache=use_cache,\n",
    "#         #    output_hidden_states=output_hidden_states,\n",
    "#         )\n",
    "#     #print(\"o\", outputs)\n",
    "#     loss = outputs.loss\n",
    "#     loss.retain_grad()\n",
    "#    # print(torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].shape)\n",
    "#     return  torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def pw_influence_at_cp(model, test_example, train_examples): \n",
    "    #     return (get_loss_gradient(model, test_example) * torch.stack([get_loss_gradient(model, train_example) for train_example in tqdm(train_examples,desc=\"Documents\", leave=False)])).sum(1).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_examples = dataset[\"train\"][0:100] # this tokenizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_MODELS_PARRALEL = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feqwafdsvyccheckpoints13refd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig\n",
    "# from transformers import RobertaForMaskedLM\n",
    "# def get_for_checkpoint(checkpoint_path,training_examples,device):\n",
    "    \n",
    "#     print(checkpoint_path, flush = True)\n",
    "#     config = RobertaConfig.from_pretrained(checkpoint_path)\n",
    "#     model = RobertaForMaskedLM(config=config).to(device)\n",
    "\n",
    "#     return [get_loss_gradient(model, example) for example in training_examples[\"input_ids\"]]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_for_checkpoint([(checkpoint, training_examples, device) for checkpoint in checkpoints][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘results’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile get_for_checkpoint.py\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# os.environ['HF_HOME'] = '/data/loriss21dm/hfcache'\n",
    "# from transformers import RobertaConfig,AutoConfig\n",
    "# from transformers import RobertaForMaskedLM\n",
    "# import torch\n",
    "\n",
    "# from transformers import RobertaTokenizerFast\n",
    "# from transformers import DataCollatorForLanguageModeling\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(\"TODO\", max_len=512)\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# def get_loss_gradient(model, example,device):\n",
    "#     model.train()\n",
    "#     model.zero_grad()\n",
    "    \n",
    "\n",
    "#     # input_ids, attention_mask = tokenizer([example], return_tensors='pt', return_token_type_ids=False,padding='max_length', truncation=True, max_length=512).values()\n",
    "#     input_ids, labels = data_collator((torch.tensor(example),)).values() # TODO make static maybe probably\n",
    "#  #   print(input_ids)\n",
    "#     #print(\"labels\",labels)\n",
    "#    # print(input_ids.shape)\n",
    "#     # print(attention_mask)\n",
    "#     inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
    "#    # attention_mask = attention_mask\n",
    "#    # print(inputs_embeds.reshape(1,-1,2048).shape)\n",
    "\n",
    "#     inputs_embeds.retain_grad()\n",
    "\n",
    "#     outputs = model.forward(\n",
    "#        #     input_ids=input_ids,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#            # attention_mask=attention_mask,\n",
    "#             labels=labels.to(device)\n",
    "#        #     attention_bias=attention_bias,\n",
    "#        #     past_key_values=past_key_values,\n",
    "#        #     use_cache=use_cache,\n",
    "#         #    output_hidden_states=output_hidden_states,\n",
    "#         )\n",
    "#     #print(\"o\", outputs)\n",
    "#     loss = outputs.loss\n",
    "#     loss.retain_grad()\n",
    "#    # print(torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].shape)\n",
    "#     return  torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].squeeze().cpu()\n",
    "\n",
    "# def get_for_checkpoint(checkpoint_path, training_examples, queue):\n",
    "#     try:\n",
    "\n",
    "#         device = \"cuda:\" + str(queue.get())\n",
    "#         # print( device, flush=True)\n",
    "#         print(\"loading to\", device,flush=True)\n",
    "#         config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "#         model = RobertaForMaskedLM(config=config).to(device)\n",
    "#         print(\"loaded to\", device,flush=True)\n",
    "#     #\n",
    "#       #  del tokenizer\n",
    "#       #  del data_collator\n",
    "#         gradients = []\n",
    "#         for example in training_examples[\"input_ids\"]:\n",
    "#             gradients.append(get_loss_gradient(model, example,device).detach().cpu())\n",
    "#         path = \"./results/\"+checkpoint_path.split(\"-\")[-1]\n",
    "#         print(\"saving to \",path, flush=True)\n",
    "#         torch.save( gradients, path)\n",
    "#         return path\n",
    "#         #return (checkpoint_path,gradients)\n",
    "#     except Exception as e:\n",
    "#         print(e,flush=True)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([str(x) for x in Path(\"/data/loriss21dm/babylm/TODO/\").glob(\"checkpoint-*\") if (int(str(x).split(\"-\")[-1]) % 10000) == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/hfcache'\n",
    "from transformers import RobertaConfig,AutoConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"TODO\", max_len=512)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_gradient(model, example,device):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "\n",
    "    # input_ids, attention_mask = tokenizer([example], return_tensors='pt', return_token_type_ids=False,padding='max_length', truncation=True, max_length=512).values()\n",
    "    input_ids, labels = data_collator((torch.tensor(example),)).values() # TODO make static maybe probably\n",
    " #   print(input_ids)\n",
    "    #print(\"labels\",labels)\n",
    "   # print(input_ids.shape)\n",
    "    # print(attention_mask)\n",
    "    inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
    "   # attention_mask = attention_mask\n",
    "   # print(inputs_embeds.reshape(1,-1,2048).shape)\n",
    "\n",
    "    inputs_embeds.retain_grad()\n",
    "\n",
    "    outputs = model.forward(\n",
    "       #     input_ids=input_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "           # attention_mask=attention_mask,\n",
    "            labels=labels.to(device)\n",
    "       #     attention_bias=attention_bias,\n",
    "       #     past_key_values=past_key_values,\n",
    "       #     use_cache=use_cache,\n",
    "        #    output_hidden_states=output_hidden_states,\n",
    "        )\n",
    "    #print(\"o\", outputs)\n",
    "    loss = outputs.loss\n",
    "    loss.retain_grad()\n",
    "   # print(torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].shape)\n",
    "    return  torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].squeeze()#.cpu()\n",
    "\n",
    "def get_for_checkpoint(checkpoint_path, i_start, i_end):\n",
    "    \n",
    "    try:\n",
    "        gpu_id = queue.get()\n",
    "\n",
    "\n",
    "        out_path = os.path.join(\"./results/\", checkpoint_path.split(\"-\")[-1] + \"_\" +  str(i_start) + \"_\" + str(i_end))\n",
    "        if os.path.isfile(out_path):\n",
    "            queue.put(gpu_id)\n",
    "           # print(\"skipping\", out_path, flush=True)\n",
    "            return out_path\n",
    "\n",
    "        from datasets import load_dataset\n",
    "        dataset = load_dataset(\"text\", data_dir=\"/data/loriss21dm/babylm/train_test/\")\n",
    "        dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, padding=\"max_length\", max_length=512))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        device = \"cuda:\" + str(gpu_id)\n",
    "        # print( device, flush=True)\n",
    "        print(\"loading to\", device,flush=True)\n",
    "        config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "        model = RobertaForMaskedLM(config=config).to(device)\n",
    "        print(\"loaded to\", device,flush=True)\n",
    "    #\n",
    "      #  del tokenizer\n",
    "      #  del data_collator\n",
    "\n",
    "        gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
    "\n",
    "\n",
    "        print(\"saving to \",out_path, flush=True)\n",
    "        torch.save( torch.stack(gradients), out_path)\n",
    "        queue.put(gpu_id)\n",
    "        return out_path\n",
    "        #return (checkpoint_path,gradients)\n",
    "    except Exception as e:\n",
    "        print(e,flush=True)\n",
    "###############\n",
    "from multiprocessing import Pool, current_process, Queue\n",
    "import time \n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from itertools import cycle\n",
    "os.environ['HF_HOME'] = '/data/loriss21dm/hfcache'\n",
    "# # import multiprocessing as mp\n",
    "# # mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "PROCCESSES = 42\n",
    "NUM_GPUS = 2\n",
    "BATCH_SIZE = 1000\n",
    "# # mp.set_start_method(\"fork\", force=True)\n",
    "# from get_for_checkpoint import get_for_checkpoint\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"TODO\", max_len=512)\n",
    "\n",
    "checkpoints = [str(x) for x in Path(\"/data/loriss21dm/babylm/TODO/\").glob(\"checkpoint-*\") if (int(str(x).split(\"-\")[-1]) % 50000) == 0]#if (int(str(x).split(\"-\")[-1] % 1000)) == 0]\n",
    "checkpoints = checkpoints[0:10]\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_dir=\"/data/loriss21dm/babylm/train_test/\")\n",
    "dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, padding=\"max_length\", max_length=512))\n",
    "\n",
    "#training_examples = dataset[\"train\"][0:1000]\n",
    "# pool = Pool(processes=PROCCESSES)\n",
    "# print([(checkpoint,gpu_id) for checkpoint, gpu_id in zip(checkpoints, cycle([0,1]))])\n",
    "import itertools\n",
    "queue = Queue()\n",
    "NUM_GPUS = 2\n",
    "for _ in range(PROCCESSES//NUM_GPUS):\n",
    "    for i in range(NUM_GPUS):\n",
    "        queue.put(i)\n",
    "print(queue)\n",
    "# pool.starmap(get_for_checkpoint, [(checkpoint, training_examples,queue) for checkpoint, gpu_id in zip(checkpoints, cycle([0,1]))] )\n",
    "   \n",
    "# pool.close()\n",
    "# pool.join()\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# def f(x):\n",
    "#     return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    tasks = list(itertools.chain(*[[(checkpoint,i, i + BATCH_SIZE) for i in range(0, len(dataset[\"train\"]), BATCH_SIZE)]  for checkpoint in checkpoints ]))\n",
    "    # print((tasks[0]))\n",
    "    # exit\n",
    "    with Pool(PROCCESSES) as p:\n",
    "        r = p.starmap_async(get_for_checkpoint, tasks, callback=print )\n",
    "        r.wait()\n",
    "# pickle.dump( result, open( \"./results/\"+(path.split(\"-\")[-1]), \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-07 11:07:56.449475: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-07 11:07:56.465567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-07 11:07:56.484572: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-07 11:07:56.490055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-07 11:07:56.505055: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-07 11:07:57.370277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<multiprocessing.queues.Queue object at 0x7f86d497a3e0>\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading toloading to  cuda:1cuda:1\n",
      "\n",
      "loading toloading to  cuda:0cuda:1\n",
      "\n",
      "loading toloading to loading tocuda:0 \n",
      " cuda:0cuda:0loading to\n",
      "\n",
      "loading to loading to cuda:0 cuda:1\n",
      "loading tocuda:1loading to\n",
      " \n",
      " cuda:0cuda:1\n",
      "loading to\n",
      " cuda:1loading to\n",
      " cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading toloading to  cuda:1cuda:0\n",
      "loading to\n",
      " cuda:1loading to\n",
      " cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loading toloading to  cuda:1cuda:1\n",
      "\n",
      "loading to loading tocuda:0loading to \n",
      " cuda:0\n",
      "cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading to loading tocuda:0 loading to\n",
      "cuda:1 \n",
      "cuda:0\n",
      "loading toloading to  loading tocuda:1cuda:0 \n",
      "cuda:1\n",
      "\n",
      "loading to cuda:1loading to\n",
      " loading tocuda:1\n",
      " cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1loaded to\n",
      " cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded toloaded to  cuda:1cuda:1\n",
      "\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded toloaded to  cuda:0cuda:1\n",
      "\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/100000_676000_677000\n",
      "loaded to cuda:0\n",
      "loaded toloaded to  cuda:1cuda:1\n",
      "\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_129000_130000\n",
      "saving to  ./results/50000_207000_208000\n",
      "loading to cuda:0\n",
      "saving to  ./results/100000_634000_635000\n",
      "saving to  ./results/50000_402000_403000\n",
      "saving to  ./results/50000_168000_169000\n",
      "saving to  saving to ./results/50000_142000_143000 \n",
      "./results/50000_376000_377000\n",
      "loading to cuda:0\n",
      "saving to  ./results/100000_638000_639000\n",
      "saving to  ./results/100000_654000_655000\n",
      "saving to saving to   ./results/50000_25000_26000./results/50000_77000_78000\n",
      "\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_389000_390000\n",
      "saving to  ./results/50000_233000_234000\n",
      "saving to  ./results/50000_181000_182000\n",
      "saving to  ./results/50000_363000_364000\n",
      "saving to  ./results/50000_350000_351000\n",
      "saving to saving to   ./results/50000_246000_247000./results/50000_103000_104000\n",
      "\n",
      "saving to  ./results/50000_337000_338000\n",
      "saving to  ./results/50000_441000_442000\n",
      "saving to  ./results/100000_664000_665000\n",
      "loading to cuda:0\n",
      "loaded to saving to cuda:0 \n",
      "./results/50000_454000_455000\n",
      "saving to  ./results/50000_220000_221000saving to \n",
      " ./results/50000_311000_312000\n",
      "saving to  ./results/50000_428000_429000\n",
      "saving to  ./results/50000_467000_468000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_259000_260000saving to \n",
      " ./results/50000_324000_325000\n",
      "saving to  saving to ./results/50000_285000_286000 \n",
      "./results/50000_272000_273000\n",
      "saving to  ./results/50000_51000_52000\n",
      "saving to  ./results/50000_12000_13000\n",
      "saving to  ./results/50000_64000_65000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to loading to  ./results/50000_415000_416000cuda:0\n",
      "\n",
      "saving to  ./results/50000_480000_481000\n",
      "saving to  ./results/50000_38000_39000\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_155000_156000\n",
      "saving to  ./results/50000_90000_91000\n",
      "saving to  ./results/50000_298000_299000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_194000_195000\n",
      "saving to  ./results/50000_116000_117000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_0_1000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1loading to\n",
      " cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading toloading to  cuda:0cuda:0\n",
      "\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:1loading to \n",
      "cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0loading to\n",
      " cuda:0\n",
      "loading to cuda:0loaded to\n",
      " cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1loading to\n",
      " cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to loading tocuda:1\n",
      " cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded toloading to  cuda:1\n",
      "cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to \n",
      "cuda:0loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded toloaded to cuda:0 \n",
      "cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_130000_131000\n",
      "saving to  ./results/50000_208000_209000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_403000_404000\n",
      "saving to  ./results/50000_169000_170000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/100000_659000_660000\n",
      "saving to  ./results/50000_493000_494000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/100000_639000_640000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_78000_79000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_234000_235000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_377000_378000\n",
      "saving to  ./results/50000_390000_391000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_143000_144000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_442000_443000\n",
      "saving to  ./results/50000_351000_352000\n",
      "saving to  ./results/50000_26000_27000\n",
      "saving to  ./results/50000_338000_339000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_221000_222000\n",
      "saving to  ./results/50000_312000_313000\n",
      "saving to  ./results/50000_182000_183000\n",
      "saving to  ./results/50000_65000_66000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_13000_14000\n",
      "saving to  ./results/50000_273000_274000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_468000_469000\n",
      "saving to  ./results/50000_364000_365000\n",
      "saving to  ./results/50000_39000_40000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_286000_287000\n",
      "saving to  ./results/50000_156000_157000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_416000_417000\n",
      "saving to  ./results/100000_673000_674000\n",
      "saving to  ./results/50000_481000_482000\n",
      "saving to  ./results/50000_455000_456000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to saving to   ./results/50000_91000_92000./results/50000_247000_248000\n",
      "\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_52000_53000\n",
      "saving to  ./results/50000_299000_300000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_104000_105000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_429000_430000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_325000_326000\n",
      "saving to  ./results/50000_260000_261000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_195000_196000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to loading to  ./results/50000_117000_118000cuda:1\n",
      "\n",
      "saving to  ./results/50000_1000_2000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to loading tocuda:0 \n",
      "cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_131000_132000\n",
      "saving to  ./results/50000_209000_210000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_404000_405000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_170000_171000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_506000_507000\n",
      "saving to  ./results/50000_494000_495000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/100000_644000_645000\n",
      "saving to  ./results/50000_235000_236000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_378000_379000\n",
      "saving to  ./results/50000_79000_80000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_144000_145000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_391000_392000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_352000_353000\n",
      "saving to  ./results/50000_443000_444000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_339000_340000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_27000_28000\n",
      "saving to  ./results/50000_222000_223000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_313000_314000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_14000_15000\n",
      "saving to  ./results/50000_66000_67000\n",
      "saving to  ./results/50000_183000_184000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_274000_275000\n",
      "saving to  ./results/50000_365000_366000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_469000_470000\n",
      "saving to  ./results/50000_40000_41000\n",
      "saving to  ./results/50000_287000_288000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_157000_158000\n",
      "saving to  ./results/100000_674000_675000\n",
      "saving to  ./results/50000_417000_418000\n",
      "saving to  ./results/50000_456000_457000\n",
      "saving to  ./results/50000_53000_54000\n",
      "saving to  ./results/50000_300000_301000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_92000_93000\n",
      "saving to  ./results/50000_482000_483000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_105000_106000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_248000_249000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_430000_431000\n",
      "saving to  ./results/50000_326000_327000\n",
      "saving to  ./results/50000_261000_262000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to loading to ./results/50000_196000_197000 \n",
      "cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_2000_3000\n",
      "saving to  ./results/50000_118000_119000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded toloaded to  cuda:1cuda:0\n",
      "\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_132000_133000\n",
      "saving to  ./results/50000_210000_211000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_405000_406000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_171000_172000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_507000_508000\n",
      "saving to  ./results/50000_236000_237000\n",
      "saving to  ./results/50000_495000_496000\n",
      "saving to  ./results/50000_379000_380000\n",
      "saving to  ./results/100000_648000_649000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_145000_146000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_80000_81000\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_392000_393000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_353000_354000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_340000_341000\n",
      "saving to  ./results/50000_444000_445000\n",
      "saving to  ./results/50000_223000_224000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_28000_29000\n",
      "saving to  ./results/50000_314000_315000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_15000_16000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_275000_276000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_67000_68000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_366000_367000\n",
      "saving to  ./results/50000_184000_185000\n",
      "saving to  ./results/50000_41000_42000\n",
      "saving to  ./results/50000_470000_471000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_158000_159000\n",
      "saving to  ./results/50000_288000_289000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_418000_419000\n",
      "saving to  ./results/50000_519000_520000\n",
      "saving to  ./results/50000_54000_55000\n",
      "saving to  ./results/50000_301000_302000\n",
      "saving to  ./results/50000_457000_458000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_483000_484000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_93000_94000\n",
      "saving to  ./results/50000_106000_107000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_249000_250000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loading toloaded to  cuda:0cuda:0\n",
      "\n",
      "saving to  ./results/50000_327000_328000\n",
      "saving to  ./results/50000_431000_432000\n",
      "saving to  ./results/50000_262000_263000\n",
      "saving to  ./results/50000_197000_198000\n",
      "saving to  ./results/50000_119000_120000\n",
      "saving to  ./results/50000_3000_4000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_133000_134000\n",
      "saving to  ./results/50000_211000_212000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_406000_407000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_172000_173000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_237000_238000\n",
      "saving to  ./results/50000_508000_509000\n",
      "saving to  ./results/50000_380000_381000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_496000_497000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/100000_649000_650000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_146000_147000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_81000_82000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_393000_394000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_354000_355000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_341000_342000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_224000_225000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_445000_446000\n",
      "saving to  ./results/50000_315000_316000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_29000_30000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_276000_277000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_16000_17000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_68000_69000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_367000_368000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_471000_472000\n",
      "saving to  ./results/50000_185000_186000\n",
      "saving to  ./results/50000_289000_290000\n",
      "saving to  ./results/50000_42000_43000\n",
      "saving to  ./results/50000_159000_160000\n",
      "saving to  ./results/50000_520000_521000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_302000_303000\n",
      "saving to  ./results/50000_458000_459000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_55000_56000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_419000_420000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_484000_485000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_94000_95000\n",
      "saving to  ./results/50000_250000_251000\n",
      "saving to  ./results/50000_107000_108000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to saving to   ./results/50000_263000_264000./results/50000_198000_199000\n",
      "\n",
      "saving to  ./results/50000_432000_433000\n",
      "saving to  ./results/50000_328000_329000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_120000_121000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_4000_5000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_134000_135000\n",
      "saving to  ./results/50000_212000_213000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_407000_408000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_173000_174000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_238000_239000\n",
      "saving to  ./results/50000_381000_382000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_509000_510000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_147000_148000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_497000_498000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_532000_533000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_82000_83000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_394000_395000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_355000_356000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_342000_343000\n",
      "saving to  ./results/50000_225000_226000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_446000_447000\n",
      "saving to  ./results/50000_316000_317000\n",
      "saving to  ./results/50000_30000_31000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_277000_278000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_17000_18000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_69000_70000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_368000_369000\n",
      "saving to  ./results/50000_472000_473000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_186000_187000\n",
      "loading to cuda:1\n",
      "saving to saving to   ./results/50000_160000_161000./results/50000_290000_291000\n",
      "\n",
      "saving to  ./results/50000_43000_44000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_303000_304000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_521000_522000\n",
      "saving to  ./results/50000_56000_57000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_459000_460000\n",
      "saving to  ./results/50000_420000_421000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_485000_486000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_95000_96000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_108000_109000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_251000_252000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_433000_434000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_264000_265000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_199000_200000\n",
      "saving to  ./results/50000_121000_122000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_329000_330000\n",
      "saving to  ./results/50000_5000_6000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_135000_136000\n",
      "saving to  ./results/50000_213000_214000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_408000_409000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_174000_175000\n",
      "saving to  ./results/50000_239000_240000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_382000_383000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_148000_149000\n",
      "saving to  ./results/50000_510000_511000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_498000_499000\n",
      "saving to  ./results/50000_533000_534000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_83000_84000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_356000_357000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_395000_396000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_343000_344000\n",
      "saving to  ./results/50000_226000_227000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_317000_318000\n",
      "saving to  ./results/50000_447000_448000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_278000_279000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_31000_32000\n",
      "saving to  ./results/50000_70000_71000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_18000_19000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_369000_370000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_473000_474000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_291000_292000\n",
      "saving to  ./results/50000_161000_162000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_187000_188000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_44000_45000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_304000_305000\n",
      "saving to  ./results/50000_522000_523000\n",
      "saving to  ./results/50000_421000_422000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_57000_58000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_460000_461000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_486000_487000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_96000_97000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_109000_110000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_252000_253000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  saving to ./results/50000_434000_435000\n",
      " ./results/50000_200000_201000\n",
      "saving to  ./results/50000_265000_266000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_330000_331000\n",
      "saving to  ./results/50000_122000_123000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_6000_7000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_136000_137000\n",
      "saving to  ./results/50000_214000_215000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_409000_410000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_240000_241000\n",
      "saving to  ./results/50000_175000_176000\n",
      "saving to  ./results/50000_383000_384000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_149000_150000\n",
      "saving to  ./results/50000_511000_512000\n",
      "saving to  ./results/50000_499000_500000\n",
      "saving to  ./results/50000_534000_535000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_357000_358000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_84000_85000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_396000_397000\n",
      "saving to  ./results/50000_344000_345000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_227000_228000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_318000_319000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_448000_449000\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_279000_280000\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_71000_72000\n",
      "saving to  ./results/50000_32000_33000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_19000_20000\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_370000_371000\n",
      "saving to  ./results/50000_474000_475000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_292000_293000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_162000_163000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_188000_189000\n",
      "saving to  ./results/50000_45000_46000\n",
      "saving to  ./results/50000_422000_423000\n",
      "saving to  ./results/50000_305000_306000\n",
      "saving to  ./results/50000_523000_524000\n",
      "saving to  ./results/50000_58000_59000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_461000_462000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_487000_488000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading toloading to  cuda:0cuda:0\n",
      "\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_97000_98000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_110000_111000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_253000_254000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_201000_202000\n",
      "loaded to cuda:1loaded to\n",
      " cuda:0\n",
      "saving to  ./results/50000_435000_436000\n",
      "saving to  ./results/50000_123000_124000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_266000_267000\n",
      "saving to  ./results/50000_7000_8000\n",
      "saving to  ./results/50000_331000_332000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to loading tocuda:1 \n",
      "cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to loaded tocuda:1\n",
      " cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_215000_216000\n",
      "saving to  ./results/50000_137000_138000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_410000_411000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_384000_385000\n",
      "saving to  ./results/50000_241000_242000\n",
      "saving to  ./results/50000_176000_177000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_150000_151000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_512000_513000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_535000_536000\n",
      "saving to  ./results/50000_358000_359000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_500000_501000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_85000_86000\n",
      "saving to  ./results/50000_345000_346000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_397000_398000\n",
      "saving to  ./results/50000_228000_229000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_319000_320000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_449000_450000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_280000_281000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_72000_73000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_33000_34000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_20000_21000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_371000_372000\n",
      "saving to  ./results/50000_475000_476000\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_189000_190000\n",
      "saving to  ./results/50000_293000_294000\n",
      "saving to  ./results/50000_163000_164000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_423000_424000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_306000_307000\n",
      "saving to  ./results/50000_46000_47000\n",
      "saving to  ./results/50000_524000_525000\n",
      "saving to  ./results/50000_59000_60000\n",
      "saving to  ./results/50000_462000_463000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_488000_489000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_111000_112000\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_98000_99000\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_254000_255000\n",
      "loaded tosaving to   ./results/50000_202000_203000cuda:1\n",
      "\n",
      "saving to  ./results/50000_124000_125000\n",
      "saving to  ./results/50000_267000_268000\n",
      "saving to  ./results/50000_332000_333000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_436000_437000\n",
      "saving to  ./results/50000_8000_9000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded toloaded to  cuda:1cuda:1\n",
      "\n",
      "saving to  ./results/50000_216000_217000\n",
      "saving to  ./results/50000_138000_139000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_411000_412000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_385000_386000\n",
      "saving to  ./results/50000_242000_243000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_177000_178000\n",
      "saving to  ./results/50000_151000_152000\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_513000_514000\n",
      "saving to  ./results/50000_359000_360000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_536000_537000\n",
      "saving to  ./results/50000_346000_347000\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_501000_502000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_229000_230000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_86000_87000\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_320000_321000\n",
      "saving to  ./results/50000_398000_399000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_450000_451000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_281000_282000\n",
      "loaded to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_73000_74000\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_34000_35000\n",
      "saving to  ./results/50000_21000_22000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_476000_477000\n",
      "loaded to cuda:0\n",
      "saving to  ./results/50000_372000_373000\n",
      "loading to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_294000_295000\n",
      "saving to  ./results/50000_190000_191000\n",
      "saving to  ./results/50000_164000_165000\n",
      "loaded to cuda:0\n",
      "loaded toloading to  cuda:1\n",
      "cuda:0\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_424000_425000\n",
      "saving to  ./results/50000_307000_308000\n",
      "saving to  ./results/50000_47000_48000\n",
      "saving to  ./results/50000_60000_61000\n",
      "saving to  ./results/50000_525000_526000\n",
      "saving to  ./results/50000_463000_464000\n",
      "loading to cuda:1\n",
      "loading to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to loading tocuda:0 \n",
      "cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_489000_490000\n",
      "loading to cuda:0\n",
      "loading to cuda:0\n",
      "saving to  ./results/50000_112000_113000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_99000_100000\n",
      "loaded to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loaded to cuda:0\n",
      "loading to cuda:1\n",
      "loaded to cuda:0\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_203000_204000\n",
      "saving to  ./results/50000_255000_256000\n",
      "saving to  ./results/50000_125000_126000\n",
      "loaded to cuda:1\n",
      "saving to  ./results/50000_268000_269000\n",
      "loading to cuda:1\n",
      "saving to  ./results/50000_333000_334000\n",
      "saving to  ./results/50000_9000_10000\n",
      "saving to  ./results/50000_437000_438000\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loading to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n",
      "loaded to cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-40:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-28:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-14:\n",
      "Process ForkPoolWorker-12:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-33:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-29:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-19:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-36:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 29, in get_loss_gradient\n",
      "    inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 39, in get_loss_gradient\n",
      "    labels=labels.to(device)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 29, in get_loss_gradient\n",
      "    inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 49, in get_loss_gradient\n",
      "    return  torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0].squeeze()#.cpu()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "KeyboardInterrupt\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 29, in get_loss_gradient\n",
      "    inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 436, in grad\n",
      "    result = _engine_run_backward(\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-27:\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 769, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 24, in get_loss_gradient\n",
      "    input_ids, labels = data_collator((torch.tensor(example),)).values() # TODO make static maybe probably\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-24:\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 45, in __call__\n",
      "    return self.torch_call(features)\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-31:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 817, in torch_call\n",
      "    batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n",
      "Process ForkPoolWorker-41:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/data/data_collator.py\", line 854, in torch_mask_tokens\n",
      "    random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in get_for_checkpoint\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py\", line 271, in __len__\n",
      "    return self._tokenizer.get_vocab_size(with_added_tokens=True)\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 80, in <listcomp>\n",
      "    gradients = [get_loss_gradient(model, example,device).to(torch.bfloat16) for example in dataset[\"train\"][i_start:i_end][\"input_ids\"]]\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/data/loriss21dm/babylm/run.py\", line 29, in get_loss_gradient\n",
      "    inputs_embeds=model.get_input_embeddings().weight[input_ids].to(device)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-42:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-37:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n"
     ]
    }
   ],
   "source": [
    "%run run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "\n",
    "path = \"/data/loriss21dm/babylm/results/10000\"\n",
    "\n",
    "gradients_at_checkpoint = torch.load(path,weights_only=True,map_location=\"cpu\")\n",
    "len(gradients_at_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def f(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gradients_at_checkpoints = sorted([(int(checkpoint_name), torch.load(os.path.join(\"./results\", checkpoint_name),weights_only=True,map_location=\"cpu\")) for checkpoint_name in os.listdir('./results')], key=lambda t : t[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_number, gradients_sorted = zip(*gradients_at_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t =torch.stack([torch.stack(gradients) for gradients in gradients_sorted])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_flat = t.flatten(-2,-1)\n",
    "t_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bmm(t_flat, torch.transpose(t_flat, 1,2)).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puhzf9dupkoülpä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "tensor = torch.bmm(t_flat, torch.transpose(t_flat, 1,2)).mean(1)[:, 0:10000]\n",
    "\n",
    "x = np.arange(tensor.shape[0])  \n",
    "y = np.arange(tensor.shape[1])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "colors = cm.viridis(np.linspace(0, 1, tensor.shape[1]))\n",
    "for i in range(tensor.shape[1]):  \n",
    "    ax.plot(x, np.full_like(x, y[i]), tensor[:, i], color=colors[i], linewidth=2)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Time [Checkpoints]')\n",
    "ax.set_ylabel('Training example #')\n",
    "ax.set_zlabel('Influence at checkpoint')\n",
    "\n",
    "\n",
    "ax.view_init(elev=25, azim=-40, roll=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, inf in enumerate(torch.bmm(t_flat, torch.transpose(t_flat, 1,2))):\n",
    "    plt.imshow(inf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.einsum('ijkl,nolp->ijnokp', t_flat, t_flat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test_instance =  \"Test <mask>\"\n",
    "# # training_examples = [\"test\", \"is\"]\n",
    "\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# models = []\n",
    "# influences_at_cps = []\n",
    "# for checkpoint in tqdm(checkpoints, desc=\"Checkpoints\"):\n",
    "#   config = RobertaConfig.from_pretrained(checkpoint)\n",
    "#   model = RobertaForMaskedLM(config=config)\n",
    "#   influences_at_cps.append(pw_influence_at_cp(model, test_instance,training_examples[\"input_ids\"]))\n",
    "# end = time.time()\n",
    "# print(datetime.timedelta(seconds=end - start))    \n",
    "# influences_total = torch.stack(influences_at_cps).sum(dim=0)\n",
    "# influences_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(influences_at_cps).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "influences_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
