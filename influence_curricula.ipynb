{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "# validate_training_duration_limitation(\"loris3/babylm_2024_10m_curriculum\", \"random.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import config\n",
    "\n",
    "jobs = [(d+\"_\"+t+\"_\"+\"random\", d,t) for d, t in product(config.datasets, config.model_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loris3/babylm_2024_10m_curriculum_llama_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/babylm_2024_10m_curriculum_roberta_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'roberta')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import upload, split_into_epochs, repeat_for_ten_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########old\n",
    "get_order_top_n_pct = lambda top_n_pct, df: torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).head(int(len(df)*top_n_pct*0.01)).index.to_numpy()) for checkpoint in influence_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "dirac_filter = lambda size: np.eye(1, (size*2)+1, size, dtype=float).flatten()\n",
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "apply_lognorm = lambda df : reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5))\n",
    "apply_dirac = lambda df : reweight_df(df, influence_cols, dirac_filter(len(influence_cols)))\n",
    "\n",
    "def get_shuffled_bins(df, bin_size=1000, ascending=False):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    curriculum =  [torch.tensor(\n",
    "        np.concat(\n",
    "            [\n",
    "                np.random.permutation(bin) for bin in \n",
    "                np.array_split(df[checkpoint].sort_values(ascending=ascending).index.to_numpy(), indices_or_sections=len(df) // 1000)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for checkpoint in influence_cols]\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "def get_influence_top_50_twice_cp_shuffled(df): \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "\n",
    "\n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "\n",
    "    WORDS_PER_CHECKPOINT = 10000000 # there are 5 stages: 2*10M words = 100M total seen during training\n",
    "\n",
    "    \n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "    curriculum = []\n",
    "    for checkpoint in influence_cols:\n",
    "        print(\"checkpoint\", checkpoint)\n",
    "        tokens_in_checkpoint = 0\n",
    "        curriculum_checkpoint = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_checkpoint < WORDS_PER_CHECKPOINT: # the dataset assumes each stage is repeated for multiple epochs. we will therefore run out of documents before reaching the alloted number of tokens. just re-shuffle and continue\n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            checkpoint_df_shuffled = df.sort_values(by=checkpoint, ascending=False).head(int(len(df)*50*0.01)).sample(frac=1, random_state=42+i)\n",
    "            for idx, row in checkpoint_df_shuffled.iterrows():            \n",
    "                if (tokens_in_checkpoint + row[\"words\"]) <= WORDS_PER_CHECKPOINT:\n",
    "                    curriculum_checkpoint.append(idx)\n",
    "                    tokens_in_checkpoint += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_checkpoint += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_checkpoint)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_checkpoint)) # so that there are 2 epochs per stage\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "def make_tracin_sandwich(df):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    beneficial_chunks = chunks[0:len(chunks)//2]\n",
    "    harmful_chunks = chunks[len(chunks)//2:]\n",
    "\n",
    "    l = [tensor for pair in zip(beneficial_chunks, harmful_chunks) for tensor in pair] * 10\n",
    "\n",
    "\n",
    "    influential_examples_sandwich = list(torch.tensor_split(torch.cat([chunk[torch.randperm(len(chunk))] for chunk in l]), len(influence_cols)))\n",
    "    return influential_examples_sandwich\n",
    "\n",
    "def influence_epoch_repetition(df):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 10) # 10 stages\n",
    "    WORDS_PER_CHUNK = 10000000\n",
    "    df_chunks = [df.iloc[chunk] for chunk in chunks]\n",
    "    curriculum = []\n",
    "    for df_chunk in df_chunks:\n",
    "        tokens_in_chunk = 0\n",
    "        curriculum_chunk = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_chunk < WORDS_PER_CHUNK: \n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            chunk_df_shuffled = df_chunk.sample(frac=1, random_state=42+i)\n",
    "            for idx, row in chunk_df_shuffled.iterrows():            \n",
    "                if (tokens_in_chunk + row[\"words\"]) <= WORDS_PER_CHUNK:\n",
    "                    curriculum_chunk.append(idx)\n",
    "                    tokens_in_chunk += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_chunk += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_chunk)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_chunk)) # so that there are 2 epochs per stage\n",
    "\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "functions = {\n",
    "\n",
    "    \"influence_epoch_repetition\": influence_epoch_repetition,\n",
    "\n",
    "    \"influence_incr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=True),\n",
    "    \"influence_decr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=False),\n",
    "\n",
    "    \"influence_incr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=True),\n",
    "     \"influence_decr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=False),\n",
    "\n",
    "    \"influence_incr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "    \"influence_decr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "#     \"influence_incr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "#     \"influence_decr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "   \"influence_top_50_cp_shuffled\" : get_influence_top_50_twice_cp_shuffled,\n",
    "    \"influence_tracin_sandwich\" : make_tracin_sandwich\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "def create_curricula(df, influence_cols, dataset_name, model_type, curriculum=\"random.pt\"):\n",
    "    for name, function in functions.items():\n",
    "        print(f\"{model_type}_{name}.pt\")\n",
    "      \n",
    "        try: # skip existing\n",
    "            hf_hub_download(repo_id=dataset_name, filename=f\"{model_type}_{name}.pt\", repo_type=\"dataset\")\n",
    "            print(\"skipping\")\n",
    "        except:\n",
    "            new_curriculum = function(df)\n",
    "            print(new_curriculum)\n",
    "            plt.show()\n",
    "            validate_training_duration_limitation(dataset_name, new_curriculum)\n",
    "            upload(f\"{model_type}_{name}.pt\", new_curriculum, dataset_name, DEBUG)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/babylm_2024_10m_curriculum_llama_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n",
      "llama_influence_epoch_repetition.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "llama_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/babylm_2024_10m_curriculum_roberta_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n",
      "roberta_influence_epoch_repetition.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_lognorm.pt\n",
      "skipping\n",
      "roberta_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "roberta_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "roberta_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "roberta_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_llama_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n",
      "llama_influence_epoch_repetition.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "llama_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_roberta_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n",
      "roberta_influence_epoch_repetition.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_lognorm.pt\n",
      "[tensor([36581, 57878, 99296,  ..., 96958, 91823, 95858]), tensor([77699, 66529, 83080,  ..., 23319, 30935, 96802]), tensor([81806, 51192, 91869,  ..., 36564, 26264, 74672]), tensor([83305, 94923, 83220,  ..., 34050, 83177,  4505]), tensor([87280, 92390, 86016,  ..., 87468, 18024, 73522]), tensor([86365, 92909, 95826,  ..., 88136, 39447, 84630]), tensor([82203, 68891, 90551,  ..., 51317, 24545,  9678]), tensor([94038, 58865, 45788,  ..., 50799, 56611,  5616]), tensor([ 8699, 95765, 34714,  ...,  3159, 33653, 12782]), tensor([42089, 97482,  2526,  ..., 84117, 56209, 47119])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bc885a0c8840d7821375f2dff39ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_seen loris3/stratified_equitoken_10m_curriculum 100000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7302d76bbc814ebeb75190d88e3cfe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b306ff6a8aca4acb87e8a068f8107884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_bins_lognorm.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_influence_incr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac3e2545b42444d811a3297b39aaa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_cp_dirac.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32eb7b05179b4073bf4d5030a9f4574c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_cp_dirac.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_top_50_cp_shuffled.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741d5a11743a4efc8ec00d6e3d7ed097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_top_50_cp_shuffled.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_tracin_sandwich.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4506f9fe3e1246edbbdd1a1d40a5f681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_tracin_sandwich.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "./influence_mean_normalized/stratified_10m_curriculum_llama_random/stratified_10m_curriculum_train[0%:100%]_stratified_10m_curriculum_train[0%:100%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f132b60d80af458a84f1d23b276d942c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6483cf0e3443b3a37dccf70e1cb58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/53457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_influence_epoch_repetition.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12cfc87de9440b886b482774e31acbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_decr_bins_lognorm.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "llama_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/stratified_10m_curriculum_roberta_random/stratified_10m_curriculum_train[0%:100%]_stratified_10m_curriculum_train[0%:100%]\n",
      "roberta_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62b65b5d2e64fc2a583ec630f400529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_epoch_repetition.pt:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_incr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3efb80346e409a91e5dda970ab3046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_bins_dirac.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f242b18c7e4c819b2dd0609307e552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_bins_dirac.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_incr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fd97f4fab74bd5970fd2d80909923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_bins_lognorm.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_bins_lognorm.pt\n",
      "[tensor([104851, 201241,  30306,  ...,  23184, 322377, 707797]), tensor([ 22499,  85478, 177538,  ..., 713711,  67026, 297835]), tensor([126002, 194030, 193442,  ..., 559678, 190664, 388397]), tensor([821624, 130042, 738051,  ..., 789706, 247429,  25833]), tensor([ 51590, 293866,  28064,  ..., 779340, 501250, 713785]), tensor([211386,  19238, 221214,  ..., 738967, 219700, 418586]), tensor([234805,  50488, 777818,  ..., 353852,  69982, 308542]), tensor([563255, 702274, 838364,  ..., 601498, 622722, 863159]), tensor([338487, 115698, 302723,  ..., 129175, 393123, 814002]), tensor([  19885,   16410,   79470,  ...,  646108, 1006805,  490319])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fbc5017d354288b818f2f4aac71cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_seen loris3/stratified_10m_curriculum 99999250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d3183f8ba24ecdb36fb8f573ea385c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f552fc3b924f0ba7fc1ade74248309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_bins_lognorm.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_influence_incr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d2b1c6b0042f6ab2b0498ee9026d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_cp_dirac.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4052da84a52943728a8dffd8a506bcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_cp_dirac.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_top_50_cp_shuffled.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194d313964f04e79a8bb8063e21be08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_top_50_cp_shuffled.pt:   0%|          | 0.00/66.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_tracin_sandwich.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d886ae935b4e3da80d4808ca85ef98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_tracin_sandwich.pt:   0%|          | 0.00/85.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load influence data\n",
    "for model_name, dataset_name, model_type in jobs:\n",
    "\n",
    "    assert dataset_name in model_name\n",
    "\n",
    "\n",
    "    RANDOM_CURRICULUM_NAME =\"random.pt\"\n",
    "\n",
    "\n",
    "    influence_output_dir = os.path.join(\"./influence_mean_normalized\", os.path.basename(model_name), \"_\".join([(os.path.basename(dataset_name) +\"_\"+f\"train[0%:100%]\")]*2))\n",
    "    print(influence_output_dir)\n",
    "    dataset = load_dataset(dataset_name)[\"train\"]\n",
    "    curriculum = util.get_curriculum(dataset_name, RANDOM_CURRICULUM_NAME)\n",
    "\n",
    "    df = pd.DataFrame({int(result_checkpoint.replace(\"checkpoint-\",\"\")): torch.load(os.path.join(influence_output_dir,result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(influence_output_dir)})\n",
    "    df.sort_index(axis=1)\n",
    "\n",
    "    df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "    influence_cols = df.columns\n",
    "    df[\"total\"] = df.sum(axis=1)\n",
    "    df[[\"text\", \"source\",\"stage\"]] = dataset.to_pandas()\n",
    "    df[\"document_lenght\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "    create_curricula(df, influence_cols, dataset_name, model_type, curriculum=RANDOM_CURRICULUM_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #############\n",
    "\n",
    "    \n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order = torch.repeat_interleave(lognorm_order,len(curriculum)//len(lognorm_order), dim=0)\n",
    "        \n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order)\n",
    "    # upload(lognorm_order, \"lognorm\")\n",
    "\n",
    "\n",
    "    # lognorm_order_top_50_pct = get_order_top_n_pct(50,reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct = torch.repeat_interleave(lognorm_order_top_50_pct,len(curriculum)//len(lognorm_order_top_50_pct), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct)\n",
    "\n",
    "    # upload(lognorm_order_top_50_pct, \"lognorm_top_50_pct\")\n",
    "\n",
    "\n",
    "\n",
    "    # torch.manual_seed(42)\n",
    "    # lognorm_order_top_50_pct_shuffled= torch.stack([row[torch.randperm(row.shape[0])] for row in lognorm_order_top_50_pct])\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct_shuffled = torch.repeat_interleave(lognorm_order_top_50_pct_shuffled,len(curriculum)//len(lognorm_order_top_50_pct_shuffled), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct_shuffled)\n",
    "    # upload(lognorm_order_top_50_pct_shuffled, \"lognorm_top_50_pct_shuffled\")\n",
    "\n",
    "\n",
    "    # dirac_order = get_order_full(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     dirac_order = torch.repeat_interleave(dirac_order,len(curriculum)//len(dirac_order), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, dirac_order)\n",
    "    # upload(dirac_order, \"dirac\")\n",
    "\n",
    "\n",
    "\n",
    "    # get_order_positive_only = lambda df: [torch.tensor(df[checkpoint][df[checkpoint] >= 0].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]\n",
    "    # positive_only_order = get_order_positive_only(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     positive_only_order = [positive_only_order[i] for i in torch.arange(0,len(positive_only_order)).repeat_interleave(len(curriculum)//len(positive_only_order))]\n",
    "    # plotting.plot_per_document_in_order(df, positive_only_order)\n",
    "    # upload(positive_only_order, \"dirac_positive_only\")\n",
    "\n",
    "\n",
    "\n",
    "    # epochs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    # epochs = [epoch.repeat(10) for epoch in epochs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_early_order = [epoch[torch.randperm(epoch.shape[0])] for epoch in epochs]\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_early_order)\n",
    "    # upload(influential_examples_early_order, \"influential_examples_early\")\n",
    "\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # thirds = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 3)\n",
    "    # thirds = [third.repeat(10) for third in thirds]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_third_order = list(torch.tensor_split(torch.cat([third[torch.randperm(len(third))] for third in thirds]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_third_order)\n",
    "    # upload(influential_examples_first_third_order, \"influential_examples_first_third\")\n",
    "\n",
    "\n",
    "\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_sandwich)\n",
    "    # upload(influential_examples_sandwich, \"influential_examples_sandwich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
