{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "# validate_training_duration_limitation(\"loris3/babylm_2024_10m_curriculum\", \"random.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import config\n",
    "\n",
    "jobs = [(d+\"_\"+t+\"_\"+\"random\", d,t) for d, t in product(config.datasets, config.model_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loris3/babylm_2024_10m_curriculum_llama_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/babylm_2024_10m_curriculum_roberta_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'roberta')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import upload, split_into_epochs, repeat_for_ten_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########old\n",
    "get_order_top_n_pct = lambda top_n_pct, df: torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).head(int(len(df)*top_n_pct*0.01)).index.to_numpy()) for checkpoint in influence_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "dirac_filter = lambda size: np.eye(1, (size*2)+1, size, dtype=float).flatten()\n",
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "apply_lognorm = lambda df : reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5))\n",
    "apply_dirac = lambda df : reweight_df(df, influence_cols, dirac_filter(len(influence_cols)))\n",
    "\n",
    "def get_shuffled_bins(df, bin_size=1000, ascending=False):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    curriculum =  [torch.tensor(\n",
    "        np.concat(\n",
    "            [\n",
    "                np.random.permutation(bin) for bin in \n",
    "                np.array_split(df[checkpoint].sort_values(ascending=ascending).index.to_numpy(), indices_or_sections=len(df) // 1000)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for checkpoint in influence_cols]\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "def get_influence_top_50_twice_cp_shuffled(df): \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "\n",
    "\n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "\n",
    "    WORDS_PER_CHECKPOINT = 10000000 # there are 5 stages: 2*10M words = 100M total seen during training\n",
    "\n",
    "    \n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "    curriculum = []\n",
    "    for checkpoint in influence_cols:\n",
    "        print(\"checkpoint\", checkpoint)\n",
    "        tokens_in_checkpoint = 0\n",
    "        curriculum_checkpoint = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_checkpoint < WORDS_PER_CHECKPOINT: # the dataset assumes each stage is repeated for multiple epochs. we will therefore run out of documents before reaching the alloted number of tokens. just re-shuffle and continue\n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            checkpoint_df_shuffled = df.sort_values(by=checkpoint, ascending=False).head(int(len(df)*50*0.01)).sample(frac=1, random_state=42+i)\n",
    "            for idx, row in checkpoint_df_shuffled.iterrows():            \n",
    "                if (tokens_in_checkpoint + row[\"words\"]) <= WORDS_PER_CHECKPOINT:\n",
    "                    curriculum_checkpoint.append(idx)\n",
    "                    tokens_in_checkpoint += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_checkpoint += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_checkpoint)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_checkpoint)) # so that there are 2 epochs per stage\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "def make_tracin_sandwich(df):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    beneficial_chunks = chunks[0:len(chunks)//2]\n",
    "    harmful_chunks = chunks[len(chunks)//2:]\n",
    "\n",
    "    l = [tensor for pair in zip(beneficial_chunks, harmful_chunks) for tensor in pair] * 10\n",
    "\n",
    "\n",
    "    influential_examples_sandwich = list(torch.tensor_split(torch.cat([chunk[torch.randperm(len(chunk))] for chunk in l]), len(influence_cols)))\n",
    "    return influential_examples_sandwich\n",
    "\n",
    "def influence_epoch_repetition(df):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 10) # 10 stages\n",
    "    WORDS_PER_CHUNK = 10000000\n",
    "    df_chunks = [df.iloc[chunk] for chunk in chunks]\n",
    "    curriculum = []\n",
    "    for df_chunk in df_chunks:\n",
    "        tokens_in_chunk = 0\n",
    "        curriculum_chunk = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_chunk < WORDS_PER_CHUNK: \n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            chunk_df_shuffled = df_chunk.sample(frac=1, random_state=42+i)\n",
    "            for idx, row in chunk_df_shuffled.iterrows():            \n",
    "                if (tokens_in_chunk + row[\"words\"]) <= WORDS_PER_CHUNK:\n",
    "                    curriculum_chunk.append(idx)\n",
    "                    tokens_in_chunk += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_chunk += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_chunk)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_chunk)) # so that there are 2 epochs per stage\n",
    "\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "functions = {\n",
    "\n",
    "    \"influence_epoch_repetition\": influence_epoch_repetition,\n",
    "\n",
    "    \"influence_incr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=True),\n",
    "    \"influence_decr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=False),\n",
    "\n",
    "    \"influence_incr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=True),\n",
    "     \"influence_decr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=False),\n",
    "\n",
    "    \"influence_incr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "    \"influence_decr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "#     \"influence_incr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "#     \"influence_decr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "   \"influence_top_50_cp_shuffled\" : get_influence_top_50_twice_cp_shuffled,\n",
    "    \"influence_tracin_sandwich\" : make_tracin_sandwich\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "def create_curricula(df, influence_cols, dataset_name, model_type, curriculum=\"random.pt\"):\n",
    "    for name, function in functions.items():\n",
    "        print(f\"{model_type}_{name}.pt\")\n",
    "      \n",
    "        try: # skip existing\n",
    "            hf_hub_download(repo_id=dataset_name, filename=f\"{model_type}_{name}.pt\", repo_type=\"dataset\")\n",
    "            print(\"skipping\")\n",
    "        except:\n",
    "            new_curriculum = function(df)\n",
    "            print(new_curriculum)\n",
    "            plt.show()\n",
    "            validate_training_duration_limitation(dataset_name, new_curriculum)\n",
    "            upload(f\"{model_type}_{name}.pt\", new_curriculum, dataset_name, DEBUG)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/babylm_2024_10m_curriculum_llama_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n",
      "llama_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462e783e4c004e72a929f89dc79207af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_epoch_repetition.pt:   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_incr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6125f916a8c4ad796b6502eee6a35f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_incr_bins_dirac.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_decr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19628c0bcce4b768e73dd54d601da0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_decr_bins_dirac.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_incr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016e12af9b1241d59ccbc28276aca621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_incr_bins_lognorm.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_decr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_incr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfabda75aea744bdab240abe18c272b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_incr_cp_dirac.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_decr_cp_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f54d06860ed471c807b78cec9982736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_decr_cp_dirac.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "llama_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/babylm_2024_10m_curriculum_roberta_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n",
      "roberta_influence_epoch_repetition.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "roberta_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "roberta_influence_decr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a07f2dde7345fe9070f96985d2df47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_bins_lognorm.pt:   0%|          | 0.00/94.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "roberta_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "roberta_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "roberta_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_llama_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d326ee46e439409e83531904e05f2b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b63936aea6c4b769fecd3112ea8d926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/53457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_influence_epoch_repetition.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_dirac.pt\n",
      "skipping\n",
      "llama_influence_incr_bins_lognorm.pt\n",
      "skipping\n",
      "llama_influence_decr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0552d9a18e4c428996643bb52db3de38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_influence_decr_bins_lognorm.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "llama_influence_incr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_decr_cp_dirac.pt\n",
      "skipping\n",
      "llama_influence_top_50_cp_shuffled.pt\n",
      "skipping\n",
      "llama_influence_tracin_sandwich.pt\n",
      "skipping\n",
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_roberta_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n",
      "roberta_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75de877a5d04dcd86a5d30821fcdaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_epoch_repetition.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_incr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4431e51953474488be8efa7f90cd6d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_bins_dirac.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_bins_dirac.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585f2b3e4bd94b14ba8c0a8274f35ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_decr_bins_dirac.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_incr_bins_lognorm.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd1c0c192c94e059f573aaae8bf0192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_influence_incr_bins_lognorm.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n",
      "roberta_influence_decr_bins_lognorm.pt\n"
     ]
    },
    {
     "ename": "EntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-67fcd2f2-42744898180aa01a54bc3bcc;a3645f8c-5b22-40ef-8d02-08949b0a0e57)\n\nEntry Not Found for url: https://huggingface.co/datasets/loris3/stratified_equitoken_10m_curriculum/resolve/main/roberta_influence_decr_bins_lognorm.pt.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/datasets/loris3/stratified_equitoken_10m_curriculum/resolve/main/roberta_influence_decr_bins_lognorm.pt",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     25\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_lenght\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mcreate_curricula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfluence_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurriculum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_CURRICULUM_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 148\u001b[0m, in \u001b[0;36mcreate_curricula\u001b[0;34m(df, influence_cols, dataset_name, model_type, curriculum)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, function \u001b[38;5;129;01min\u001b[39;00m functions\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# skip existing\u001b[39;00m\n\u001b[1;32m    150\u001b[0m         hf_hub_download(repo_id\u001b[38;5;241m=\u001b[39mdataset_name, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, repo_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:925\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:280\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:304\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 304\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py:420\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntryNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m     )\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67fcd2f2-42744898180aa01a54bc3bcc;a3645f8c-5b22-40ef-8d02-08949b0a0e57)\n\nEntry Not Found for url: https://huggingface.co/datasets/loris3/stratified_equitoken_10m_curriculum/resolve/main/roberta_influence_decr_bins_lognorm.pt."
     ]
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load influence data\n",
    "for model_name, dataset_name, model_type in jobs:\n",
    "\n",
    "    assert dataset_name in model_name\n",
    "\n",
    "\n",
    "    RANDOM_CURRICULUM_NAME =\"random.pt\"\n",
    "\n",
    "\n",
    "    influence_output_dir = os.path.join(\"./influence_mean_normalized\", os.path.basename(model_name), \"_\".join([(os.path.basename(dataset_name) +\"_\"+f\"train[0%:100%]\")]*2))\n",
    "    print(influence_output_dir)\n",
    "    dataset = load_dataset(dataset_name)[\"train\"]\n",
    "    curriculum = util.get_curriculum(dataset_name, RANDOM_CURRICULUM_NAME)\n",
    "\n",
    "    df = pd.DataFrame({int(result_checkpoint.replace(\"checkpoint-\",\"\")): torch.load(os.path.join(influence_output_dir,result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(influence_output_dir)})\n",
    "    df.sort_index(axis=1)\n",
    "\n",
    "    df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "    influence_cols = df.columns\n",
    "    df[\"total\"] = df.sum(axis=1)\n",
    "    df[[\"text\", \"source\",\"stage\"]] = dataset.to_pandas()\n",
    "    df[\"document_lenght\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "    create_curricula(df, influence_cols, dataset_name, model_type, curriculum=RANDOM_CURRICULUM_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #############\n",
    "\n",
    "    \n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order = torch.repeat_interleave(lognorm_order,len(curriculum)//len(lognorm_order), dim=0)\n",
    "        \n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order)\n",
    "    # upload(lognorm_order, \"lognorm\")\n",
    "\n",
    "\n",
    "    # lognorm_order_top_50_pct = get_order_top_n_pct(50,reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct = torch.repeat_interleave(lognorm_order_top_50_pct,len(curriculum)//len(lognorm_order_top_50_pct), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct)\n",
    "\n",
    "    # upload(lognorm_order_top_50_pct, \"lognorm_top_50_pct\")\n",
    "\n",
    "\n",
    "\n",
    "    # torch.manual_seed(42)\n",
    "    # lognorm_order_top_50_pct_shuffled= torch.stack([row[torch.randperm(row.shape[0])] for row in lognorm_order_top_50_pct])\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct_shuffled = torch.repeat_interleave(lognorm_order_top_50_pct_shuffled,len(curriculum)//len(lognorm_order_top_50_pct_shuffled), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct_shuffled)\n",
    "    # upload(lognorm_order_top_50_pct_shuffled, \"lognorm_top_50_pct_shuffled\")\n",
    "\n",
    "\n",
    "    # dirac_order = get_order_full(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     dirac_order = torch.repeat_interleave(dirac_order,len(curriculum)//len(dirac_order), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, dirac_order)\n",
    "    # upload(dirac_order, \"dirac\")\n",
    "\n",
    "\n",
    "\n",
    "    # get_order_positive_only = lambda df: [torch.tensor(df[checkpoint][df[checkpoint] >= 0].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]\n",
    "    # positive_only_order = get_order_positive_only(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     positive_only_order = [positive_only_order[i] for i in torch.arange(0,len(positive_only_order)).repeat_interleave(len(curriculum)//len(positive_only_order))]\n",
    "    # plotting.plot_per_document_in_order(df, positive_only_order)\n",
    "    # upload(positive_only_order, \"dirac_positive_only\")\n",
    "\n",
    "\n",
    "\n",
    "    # epochs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    # epochs = [epoch.repeat(10) for epoch in epochs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_early_order = [epoch[torch.randperm(epoch.shape[0])] for epoch in epochs]\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_early_order)\n",
    "    # upload(influential_examples_early_order, \"influential_examples_early\")\n",
    "\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # thirds = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 3)\n",
    "    # thirds = [third.repeat(10) for third in thirds]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_third_order = list(torch.tensor_split(torch.cat([third[torch.randperm(len(third))] for third in thirds]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_third_order)\n",
    "    # upload(influential_examples_first_third_order, \"influential_examples_first_third\")\n",
    "\n",
    "\n",
    "\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_sandwich)\n",
    "    # upload(influential_examples_sandwich, \"influential_examples_sandwich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
