{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "# validate_training_duration_limitation(\"loris3/babylm_2024_10m_curriculum\", \"random.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import config\n",
    "\n",
    "jobs = [(d+\"_\"+t+\"_\"+\"random\", d,t) for d, t in product(config.datasets, config.model_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loris3/babylm_2024_10m_curriculum_llama_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/babylm_2024_10m_curriculum_roberta_random',\n",
       "  'loris3/babylm_2024_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_equitoken_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_equitoken_10m_curriculum',\n",
       "  'roberta'),\n",
       " ('loris3/stratified_10m_curriculum_llama_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'llama'),\n",
       " ('loris3/stratified_10m_curriculum_roberta_random',\n",
       "  'loris3/stratified_10m_curriculum',\n",
       "  'roberta')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_curricula import upload, split_into_epochs, repeat_for_ten_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########old\n",
    "get_order_top_n_pct = lambda top_n_pct, df: torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).head(int(len(df)*top_n_pct*0.01)).index.to_numpy()) for checkpoint in influence_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "dirac_filter = lambda size: np.eye(1, (size*2)+1, size, dtype=float).flatten()\n",
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "apply_lognorm = lambda df : reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5))\n",
    "apply_dirac = lambda df : reweight_df(df, influence_cols, dirac_filter(len(influence_cols)))\n",
    "\n",
    "def get_shuffled_bins(df, bin_size=1000, ascending=False):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    curriculum =  [torch.tensor(\n",
    "        np.concat(\n",
    "            [\n",
    "                np.random.permutation(bin) for bin in \n",
    "                np.array_split(df[checkpoint].sort_values(ascending=ascending).index.to_numpy(), indices_or_sections=len(df) // 1000)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for checkpoint in influence_cols]\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "def get_influence_top_50_twice_cp_shuffled(df): \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "\n",
    "\n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "\n",
    "    WORDS_PER_CHECKPOINT = 10000000 # there are 5 stages: 2*10M words = 100M total seen during training\n",
    "\n",
    "    \n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "    curriculum = []\n",
    "    for checkpoint in influence_cols:\n",
    "        print(\"checkpoint\", checkpoint)\n",
    "        tokens_in_checkpoint = 0\n",
    "        curriculum_checkpoint = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_checkpoint < WORDS_PER_CHECKPOINT: # the dataset assumes each stage is repeated for multiple epochs. we will therefore run out of documents before reaching the alloted number of tokens. just re-shuffle and continue\n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            checkpoint_df_shuffled = df.sort_values(by=checkpoint, ascending=False).head(int(len(df)*50*0.01)).sample(frac=1, random_state=42+i)\n",
    "            for idx, row in checkpoint_df_shuffled.iterrows():            \n",
    "                if (tokens_in_checkpoint + row[\"words\"]) <= WORDS_PER_CHECKPOINT:\n",
    "                    curriculum_checkpoint.append(idx)\n",
    "                    tokens_in_checkpoint += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_checkpoint += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_checkpoint)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_checkpoint)) # so that there are 2 epochs per stage\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "def make_tracin_sandwich(df):\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    beneficial_chunks = chunks[0:len(chunks)//2]\n",
    "    harmful_chunks = chunks[len(chunks)//2:]\n",
    "\n",
    "    l = [tensor for pair in zip(beneficial_chunks, harmful_chunks) for tensor in pair] * 10\n",
    "\n",
    "\n",
    "    influential_examples_sandwich = list(torch.tensor_split(torch.cat([chunk[torch.randperm(len(chunk))] for chunk in l]), len(influence_cols)))\n",
    "    return influential_examples_sandwich\n",
    "\n",
    "def influence_epoch_repetition(df):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 10) # 10 stages\n",
    "    WORDS_PER_CHUNK = 10000000\n",
    "    df_chunks = [df.iloc[chunk] for chunk in chunks]\n",
    "    curriculum = []\n",
    "    for df_chunk in df_chunks:\n",
    "        tokens_in_chunk = 0\n",
    "        curriculum_chunk = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_chunk < WORDS_PER_CHUNK: \n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            chunk_df_shuffled = df_chunk.sample(frac=1, random_state=42+i)\n",
    "            for idx, row in chunk_df_shuffled.iterrows():            \n",
    "                if (tokens_in_chunk + row[\"words\"]) <= WORDS_PER_CHUNK:\n",
    "                    curriculum_chunk.append(idx)\n",
    "                    tokens_in_chunk += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_chunk += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_chunk)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_chunk)) # so that there are 2 epochs per stage\n",
    "\n",
    "    return curriculum\n",
    "def incr_influence_epoch_repetition(df):\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    word_count = lambda d: {\"words\" : len(d[\"text\"].split())}\n",
    "    df[\"words\"] = load_dataset(dataset_name)[\"train\"].map(word_count, num_proc=100).to_pandas()[\"words\"]\n",
    "\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=True)[\"total\"].index.to_numpy()), 10) # 10 stages\n",
    "    WORDS_PER_CHUNK = 10000000\n",
    "    df_chunks = [df.iloc[chunk] for chunk in chunks]\n",
    "    curriculum = []\n",
    "    for df_chunk in df_chunks:\n",
    "        tokens_in_chunk = 0\n",
    "        curriculum_chunk = []\n",
    "        \n",
    "        i = 0\n",
    "        while tokens_in_chunk < WORDS_PER_CHUNK: \n",
    "                                                #    we double-check the total number of tokens seen trough the curriculum below\n",
    "            chunk_df_shuffled = df_chunk.sample(frac=1, random_state=42+i)\n",
    "            for idx, row in chunk_df_shuffled.iterrows():            \n",
    "                if (tokens_in_chunk + row[\"words\"]) <= WORDS_PER_CHUNK:\n",
    "                    curriculum_chunk.append(idx)\n",
    "                    tokens_in_chunk += row[\"words\"]\n",
    "                else:\n",
    "                    tokens_in_chunk += row[\"words\"] # so that outer loop breaks\n",
    "                    break\n",
    "                \n",
    "            print(i,tokens_in_chunk)\n",
    "            i+=1\n",
    "        curriculum.append(torch.tensor(curriculum_chunk)) # so that there are 2 epochs per stage\n",
    "\n",
    "    return curriculum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "functions = {\n",
    "    \"incr_influence_epoch_repetition\": incr_influence_epoch_repetition,\n",
    "#     \"influence_epoch_repetition\": influence_epoch_repetition,\n",
    "\n",
    "#     \"influence_incr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=True),\n",
    "#     \"influence_decr_bins_dirac\": lambda df: get_shuffled_bins(apply_dirac(df), bin_size=1000, ascending=False),\n",
    "\n",
    "#     \"influence_incr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=True),\n",
    "#      \"influence_decr_bins_lognorm\": lambda df: get_shuffled_bins(apply_lognorm(df), bin_size=1000, ascending=False),\n",
    "\n",
    "#     \"influence_incr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "#     \"influence_decr_cp_dirac\" : lambda df: [torch.tensor(apply_dirac(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "# #     \"influence_incr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=True).index.to_numpy()) for checkpoint in influence_cols],\n",
    "# #     \"influence_decr_cp_lognorm\" : lambda df: [torch.tensor(apply_lognorm(df)[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols],\n",
    "\n",
    "#    \"influence_top_50_cp_shuffled\" : get_influence_top_50_twice_cp_shuffled,\n",
    "#     \"influence_tracin_sandwich\" : make_tracin_sandwich\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product\n",
    "import config \n",
    "\n",
    "def create_curricula(df, influence_cols, dataset_name, model_type, curriculum=\"random.pt\"):\n",
    "    for name, function in functions.items():\n",
    "        print(f\"{model_type}_{name}.pt\")\n",
    "      \n",
    "        try: # skip existing\n",
    "            hf_hub_download(repo_id=dataset_name, filename=f\"{model_type}_{name}.pt\", repo_type=\"dataset\")\n",
    "            print(\"skipping\")\n",
    "        except:\n",
    "            new_curriculum = function(df)\n",
    "            print(new_curriculum)\n",
    "            plt.show()\n",
    "            validate_training_duration_limitation(dataset_name, new_curriculum)\n",
    "            upload(f\"{model_type}_{name}.pt\", new_curriculum, dataset_name, DEBUG)\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/babylm_2024_10m_curriculum_llama_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n",
      "llama_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742e0090b386470c9069ddc1680f6030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 910036\n",
      "1 1820072\n",
      "2 2730108\n",
      "3 3640144\n",
      "4 4550180\n",
      "5 5460216\n",
      "6 6370252\n",
      "7 7280288\n",
      "8 8190324\n",
      "9 9100360\n",
      "10 10000049\n",
      "0 1967734\n",
      "1 3935468\n",
      "2 5903202\n",
      "3 7870936\n",
      "4 9838670\n",
      "5 10000002\n",
      "0 2079488\n",
      "1 4158976\n",
      "2 6238464\n",
      "3 8317952\n",
      "4 10000004\n",
      "0 1513748\n",
      "1 3027496\n",
      "2 4541244\n",
      "3 6054992\n",
      "4 7568740\n",
      "5 9082488\n",
      "6 10000012\n",
      "0 692316\n",
      "1 1384632\n",
      "2 2076948\n",
      "3 2769264\n",
      "4 3461580\n",
      "5 4153896\n",
      "6 4846212\n",
      "7 5538528\n",
      "8 6230844\n",
      "9 6923160\n",
      "10 7615476\n",
      "11 8307792\n",
      "12 9000108\n",
      "13 9692424\n",
      "14 10000005\n",
      "0 798011\n",
      "1 1596022\n",
      "2 2394033\n",
      "3 3192044\n",
      "4 3990055\n",
      "5 4788066\n",
      "6 5586077\n",
      "7 6384088\n",
      "8 7182099\n",
      "9 7980110\n",
      "10 8778121\n",
      "11 9576132\n",
      "12 10000002\n",
      "0 740050\n",
      "1 1480100\n",
      "2 2220150\n",
      "3 2960200\n",
      "4 3700250\n",
      "5 4440300\n",
      "6 5180350\n",
      "7 5920400\n",
      "8 6660450\n",
      "9 7400500\n",
      "10 8140550\n",
      "11 8880600\n",
      "12 9620650\n",
      "13 10000001\n",
      "0 558152\n",
      "1 1116304\n",
      "2 1674456\n",
      "3 2232608\n",
      "4 2790760\n",
      "5 3348912\n",
      "6 3907064\n",
      "7 4465216\n",
      "8 5023368\n",
      "9 5581520\n",
      "10 6139672\n",
      "11 6697824\n",
      "12 7255976\n",
      "13 7814128\n",
      "14 8372280\n",
      "15 8930432\n",
      "16 9488584\n",
      "17 10000003\n",
      "0 411363\n",
      "1 822726\n",
      "2 1234089\n",
      "3 1645452\n",
      "4 2056815\n",
      "5 2468178\n",
      "6 2879541\n",
      "7 3290904\n",
      "8 3702267\n",
      "9 4113630\n",
      "10 4524993\n",
      "11 4936356\n",
      "12 5347719\n",
      "13 5759082\n",
      "14 6170445\n",
      "15 6581808\n",
      "16 6993171\n",
      "17 7404534\n",
      "18 7815897\n",
      "19 8227260\n",
      "20 8638623\n",
      "21 9049986\n",
      "22 9461349\n",
      "23 9872712\n",
      "24 10000005\n",
      "0 282875\n",
      "1 565750\n",
      "2 848625\n",
      "3 1131500\n",
      "4 1414375\n",
      "5 1697250\n",
      "6 1980125\n",
      "7 2263000\n",
      "8 2545875\n",
      "9 2828750\n",
      "10 3111625\n",
      "11 3394500\n",
      "12 3677375\n",
      "13 3960250\n",
      "14 4243125\n",
      "15 4526000\n",
      "16 4808875\n",
      "17 5091750\n",
      "18 5374625\n",
      "19 5657500\n",
      "20 5940375\n",
      "21 6223250\n",
      "22 6506125\n",
      "23 6789000\n",
      "24 7071875\n",
      "25 7354750\n",
      "26 7637625\n",
      "27 7920500\n",
      "28 8203375\n",
      "29 8486250\n",
      "30 8769125\n",
      "31 9052000\n",
      "32 9334875\n",
      "33 9617750\n",
      "34 9900625\n",
      "35 10000003\n",
      "[tensor([ 591423,  680919, 1055519,  ..., 1125340,  849750,  633695]), tensor([ 781840, 1156499,  923522,  ...,  657202, 1169786, 1038572]), tensor([ 822750,  818298, 1134821,  ...,  868143,  685075, 1096645]), tensor([743739, 111665, 920784,  ..., 686005, 966440, 615864]), tensor([ 714131,  865115,  953370,  ..., 1039783,  833279,  851107]), tensor([365815, 649232, 686185,  ..., 300095, 675762,   3406]), tensor([352782, 277397,  97481,  ..., 475395, 520330, 558700]), tensor([516251, 191543,  75056,  ..., 214877, 222049, 300309]), tensor([175395, 396355, 385034,  ..., 287330, 539026, 348892]), tensor([175808, 144178, 239012,  ..., 269777, 231421, 271543])]\n",
      "total_words_seen loris3/babylm_2024_10m_curriculum 99999892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b0fb34c87c450a90f4a6a7ee51fe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f26418094e8480b90c356dc3bff79c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_incr_influence_epoch_repetition.pt:   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/babylm_2024_10m_curriculum_roberta_random/babylm_2024_10m_curriculum_train[0%:100%]_babylm_2024_10m_curriculum_train[0%:100%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7f4ab2e5cf4eb0ab4c7bc44facb5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdac3326fbb34bd9a98f5366b6878c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/58950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62f28db554f486d9ea77fdc474751a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1249891\n",
      "1 2499782\n",
      "2 3749673\n",
      "3 4999564\n",
      "4 6249455\n",
      "5 7499346\n",
      "6 8749237\n",
      "7 9999128\n",
      "8 10000018\n",
      "0 1571959\n",
      "1 3143918\n",
      "2 4715877\n",
      "3 6287836\n",
      "4 7859795\n",
      "5 9431754\n",
      "6 10000049\n",
      "0 1893183\n",
      "1 3786366\n",
      "2 5679549\n",
      "3 7572732\n",
      "4 9465915\n",
      "5 10000126\n",
      "0 1489176\n",
      "1 2978352\n",
      "2 4467528\n",
      "3 5956704\n",
      "4 7445880\n",
      "5 8935056\n",
      "6 10000004\n",
      "0 898587\n",
      "1 1797174\n",
      "2 2695761\n",
      "3 3594348\n",
      "4 4492935\n",
      "5 5391522\n",
      "6 6290109\n",
      "7 7188696\n",
      "8 8087283\n",
      "9 8985870\n",
      "10 9884457\n",
      "11 10000002\n",
      "0 655018\n",
      "1 1310036\n",
      "2 1965054\n",
      "3 2620072\n",
      "4 3275090\n",
      "5 3930108\n",
      "6 4585126\n",
      "7 5240144\n",
      "8 5895162\n",
      "9 6550180\n",
      "10 7205198\n",
      "11 7860216\n",
      "12 8515234\n",
      "13 9170252\n",
      "14 9825270\n",
      "15 10000006\n",
      "0 651283\n",
      "1 1302566\n",
      "2 1953849\n",
      "3 2605132\n",
      "4 3256415\n",
      "5 3907698\n",
      "6 4558981\n",
      "7 5210264\n",
      "8 5861547\n",
      "9 6512830\n",
      "10 7164113\n",
      "11 7815396\n",
      "12 8466679\n",
      "13 9117962\n",
      "14 9769245\n",
      "15 10000010\n",
      "0 595093\n",
      "1 1190186\n",
      "2 1785279\n",
      "3 2380372\n",
      "4 2975465\n",
      "5 3570558\n",
      "6 4165651\n",
      "7 4760744\n",
      "8 5355837\n",
      "9 5950930\n",
      "10 6546023\n",
      "11 7141116\n",
      "12 7736209\n",
      "13 8331302\n",
      "14 8926395\n",
      "15 9521488\n",
      "16 10000001\n",
      "0 530705\n",
      "1 1061410\n",
      "2 1592115\n",
      "3 2122820\n",
      "4 2653525\n",
      "5 3184230\n",
      "6 3714935\n",
      "7 4245640\n",
      "8 4776345\n",
      "9 5307050\n",
      "10 5837755\n",
      "11 6368460\n",
      "12 6899165\n",
      "13 7429870\n",
      "14 7960575\n",
      "15 8491280\n",
      "16 9021985\n",
      "17 9552690\n",
      "18 10000002\n",
      "0 418878\n",
      "1 837756\n",
      "2 1256634\n",
      "3 1675512\n",
      "4 2094390\n",
      "5 2513268\n",
      "6 2932146\n",
      "7 3351024\n",
      "8 3769902\n",
      "9 4188780\n",
      "10 4607658\n",
      "11 5026536\n",
      "12 5445414\n",
      "13 5864292\n",
      "14 6283170\n",
      "15 6702048\n",
      "16 7120926\n",
      "17 7539804\n",
      "18 7958682\n",
      "19 8377560\n",
      "20 8796438\n",
      "21 9215316\n",
      "22 9634194\n",
      "23 10000009\n",
      "[tensor([ 930324,  645365,  816226,  ...,  908874,  921616, 1104408]), tensor([1068069,  765612,  587106,  ..., 1063474,  640606,  745495]), tensor([ 958202, 1021425, 1125032,  ..., 1031677,  785229,  978134]), tensor([ 891767,  679068,  804506,  ...,  339873, 1011459, 1104285]), tensor([ 679808,  987277, 1164292,  ...,  838731, 1130873,  924721]), tensor([345695, 781004, 762254,  ..., 783524, 759936, 517905]), tensor([126330, 514269,   6618,  ..., 396529,  83802, 182495]), tensor([351428,  50914, 207381,  ..., 394222, 546460, 466941]), tensor([171107, 479408, 548538,  ..., 173409, 307793,  63801]), tensor([101646, 231462, 282745,  ...,  62170,   7578, 202528])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dba5229a747401fb8ef31878a05a0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1179014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_seen loris3/babylm_2024_10m_curriculum 99999809\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26199bbe5e154d13a06d69d9faa2ec6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6a44cd981d41d6b3703f07f0c6ad35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)berta_incr_influence_epoch_repetition.pt:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_llama_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n",
      "llama_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be954c7d008474dae4e1195f274cfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "[tensor([18966, 78391, 13778,  ..., 18146, 73402,  7519]), tensor([79327,  9926,  1110,  ..., 38756, 16371, 59226]), tensor([ 1003, 38176, 29089,  ..., 75856,  2726, 24208]), tensor([19021, 63759, 63100,  ..., 77703, 78184, 61929]), tensor([71485, 85768, 64929,  ..., 73161,  3439, 35244]), tensor([96397, 31672, 24752,  ..., 88052, 57941, 66443]), tensor([66594, 83748, 44148,  ..., 25122, 87031, 70348]), tensor([43051, 29915, 83497,  ..., 82046, 76327, 33673]), tensor([98106, 91864, 55869,  ..., 61588, 97719, 99125]), tensor([41606, 45023, 41882,  ..., 47395, 55293, 50813])]\n",
      "total_words_seen loris3/stratified_equitoken_10m_curriculum 100000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052b00ff783744d3970c87932bb229a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23e3b15841a43178a9d6f5d64b1213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_incr_influence_epoch_repetition.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/stratified_equitoken_10m_curriculum_roberta_random/stratified_equitoken_10m_curriculum_train[0%:100%]_stratified_equitoken_10m_curriculum_train[0%:100%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04217ca817844c9b8b593518cbbfaed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d336a971ae2a437da9b48fbe8cdc04b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/53457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cbab50c68344cc83d95d0a95f4506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "0 1000000\n",
      "1 2000000\n",
      "2 3000000\n",
      "3 4000000\n",
      "4 5000000\n",
      "5 6000000\n",
      "6 7000000\n",
      "7 8000000\n",
      "8 9000000\n",
      "9 10000000\n",
      "[tensor([ 7849, 19045, 34208,  ..., 13456, 72171, 14464]), tensor([28778,  3221, 91378,  ..., 62495, 90311, 36309]), tensor([67745, 71760, 84874,  ..., 12412, 68671, 21193]), tensor([15287, 10540, 74091,  ..., 32827, 34463, 19970]), tensor([91089, 73542, 17204,  ..., 17349, 91483, 14563]), tensor([79448, 19196, 85331,  ..., 53996, 48505, 60598]), tensor([71724, 89183,  2593,  ..., 41032, 78657, 50270]), tensor([96096, 50226, 61107,  ..., 65131, 38316, 52705]), tensor([47876,   181, 52418,  ..., 33262, 18339,  1766]), tensor([66085, 12924, 87587,  ..., 68001, 69553, 76482])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4119dcc9264714a300c9c0743c49b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_seen loris3/stratified_equitoken_10m_curriculum 100000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb14ca0ab574a478fa609c6c08085f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e371841f6dd4371ab8237c69e140854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "roberta_incr_influence_epoch_repetition.pt:   0%|          | 0.00/8.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/stratified_10m_curriculum_llama_random/stratified_10m_curriculum_train[0%:100%]_stratified_10m_curriculum_train[0%:100%]\n",
      "llama_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe32da6716b24601aa1a506c8da5ceb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 385076\n",
      "1 770152\n",
      "2 1155228\n",
      "3 1540304\n",
      "4 1925380\n",
      "5 2310456\n",
      "6 2695532\n",
      "7 3080608\n",
      "8 3465684\n",
      "9 3850760\n",
      "10 4235836\n",
      "11 4620912\n",
      "12 5005988\n",
      "13 5391064\n",
      "14 5776140\n",
      "15 6161216\n",
      "16 6546292\n",
      "17 6931368\n",
      "18 7316444\n",
      "19 7701520\n",
      "20 8086596\n",
      "21 8471672\n",
      "22 8856748\n",
      "23 9241824\n",
      "24 9626900\n",
      "25 10000006\n",
      "0 963017\n",
      "1 1926034\n",
      "2 2889051\n",
      "3 3852068\n",
      "4 4815085\n",
      "5 5778102\n",
      "6 6741119\n",
      "7 7704136\n",
      "8 8667153\n",
      "9 9630170\n",
      "10 10000009\n",
      "0 1257036\n",
      "1 2514072\n",
      "2 3771108\n",
      "3 5028144\n",
      "4 6285180\n",
      "5 7542216\n",
      "6 8799252\n",
      "7 10000182\n",
      "0 1364401\n",
      "1 2728802\n",
      "2 4093203\n",
      "3 5457604\n",
      "4 6822005\n",
      "5 8186406\n",
      "6 9550807\n",
      "7 10000019\n",
      "0 1402951\n",
      "1 2805902\n",
      "2 4208853\n",
      "3 5611804\n",
      "4 7014755\n",
      "5 8417706\n",
      "6 9820657\n",
      "7 10000021\n",
      "0 1341203\n",
      "1 2682406\n",
      "2 4023609\n",
      "3 5364812\n",
      "4 6706015\n",
      "5 8047218\n",
      "6 9388421\n",
      "7 10000013\n",
      "0 1210556\n",
      "1 2421112\n",
      "2 3631668\n",
      "3 4842224\n",
      "4 6052780\n",
      "5 7263336\n",
      "6 8473892\n",
      "7 9684448\n",
      "8 10000014\n",
      "0 978055\n",
      "1 1956110\n",
      "2 2934165\n",
      "3 3912220\n",
      "4 4890275\n",
      "5 5868330\n",
      "6 6846385\n",
      "7 7824440\n",
      "8 8802495\n",
      "9 9780550\n",
      "10 10000002\n",
      "0 713163\n",
      "1 1426326\n",
      "2 2139489\n",
      "3 2852652\n",
      "4 3565815\n",
      "5 4278978\n",
      "6 4992141\n",
      "7 5705304\n",
      "8 6418467\n",
      "9 7131630\n",
      "10 7844793\n",
      "11 8557956\n",
      "12 9271119\n",
      "13 9984282\n",
      "14 10000007\n",
      "0 384467\n",
      "1 768934\n",
      "2 1153401\n",
      "3 1537868\n",
      "4 1922335\n",
      "5 2306802\n",
      "6 2691269\n",
      "7 3075736\n",
      "8 3460203\n",
      "9 3844670\n",
      "10 4229137\n",
      "11 4613604\n",
      "12 4998071\n",
      "13 5382538\n",
      "14 5767005\n",
      "15 6151472\n",
      "16 6535939\n",
      "17 6920406\n",
      "18 7304873\n",
      "19 7689340\n",
      "20 8073807\n",
      "21 8458274\n",
      "22 8842741\n",
      "23 9227208\n",
      "24 9611675\n",
      "25 9996142\n",
      "26 10000002\n",
      "[tensor([ 981586, 1046400,  453503,  ...,  625530,  679986,  657955]), tensor([699034, 384583, 917384,  ..., 705382, 329408, 303977]), tensor([822660, 185515, 206638,  ..., 459500, 649285, 996408]), tensor([ 606969,  652888, 1058725,  ...,  534797,  393429,  892467]), tensor([1022123,  479782,   37649,  ...,  502003,  596416, 1049769]), tensor([375496, 978340, 952251,  ..., 877961, 313799, 847391]), tensor([714712, 679504, 590264,  ...,  56228, 439924, 379132]), tensor([574743, 965268, 219837,  ..., 234191,  73488, 900394]), tensor([ 69016, 896802, 616614,  ..., 888143, 653617, 101700]), tensor([731723, 163034, 175000,  ..., 582029, 694970, 351194])]\n",
      "total_words_seen loris3/stratified_10m_curriculum 99999851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908b6981d54c40c2a95d9ec4a55d5aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1051d2a83b47a6884de347237a1d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama_incr_influence_epoch_repetition.pt:   0%|          | 0.00/107M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./influence_mean_normalized/stratified_10m_curriculum_roberta_random/stratified_10m_curriculum_train[0%:100%]_stratified_10m_curriculum_train[0%:100%]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79bf162f1ca44fb7b504fc344e3d0327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45313982229d42faae05718bafdb32ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/53457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_incr_influence_epoch_repetition.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8fd8b1a2a4f4f4f88809f8d6a7e9876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 550072\n",
      "1 1100144\n",
      "2 1650216\n",
      "3 2200288\n",
      "4 2750360\n",
      "5 3300432\n",
      "6 3850504\n",
      "7 4400576\n",
      "8 4950648\n",
      "9 5500720\n",
      "10 6050792\n",
      "11 6600864\n",
      "12 7150936\n",
      "13 7701008\n",
      "14 8251080\n",
      "15 8801152\n",
      "16 9351224\n",
      "17 9901296\n",
      "18 10000016\n",
      "0 1323938\n",
      "1 2647876\n",
      "2 3971814\n",
      "3 5295752\n",
      "4 6619690\n",
      "5 7943628\n",
      "6 9267566\n",
      "7 10000020\n",
      "0 1616246\n",
      "1 3232492\n",
      "2 4848738\n",
      "3 6464984\n",
      "4 8081230\n",
      "5 9697476\n",
      "6 10000078\n",
      "0 1494845\n",
      "1 2989690\n",
      "2 4484535\n",
      "3 5979380\n",
      "4 7474225\n",
      "5 8969070\n",
      "6 10000010\n",
      "0 1239962\n",
      "1 2479924\n",
      "2 3719886\n",
      "3 4959848\n",
      "4 6199810\n",
      "5 7439772\n",
      "6 8679734\n",
      "7 9919696\n",
      "8 10000003\n",
      "0 1027313\n",
      "1 2054626\n",
      "2 3081939\n",
      "3 4109252\n",
      "4 5136565\n",
      "5 6163878\n",
      "6 7191191\n",
      "7 8218504\n",
      "8 9245817\n",
      "9 10000003\n",
      "0 866906\n",
      "1 1733812\n",
      "2 2600718\n",
      "3 3467624\n",
      "4 4334530\n",
      "5 5201436\n",
      "6 6068342\n",
      "7 6935248\n",
      "8 7802154\n",
      "9 8669060\n",
      "10 9535966\n",
      "11 10000002\n",
      "0 742982\n",
      "1 1485964\n",
      "2 2228946\n",
      "3 2971928\n",
      "4 3714910\n",
      "5 4457892\n",
      "6 5200874\n",
      "7 5943856\n",
      "8 6686838\n",
      "9 7429820\n",
      "10 8172802\n",
      "11 8915784\n",
      "12 9658766\n",
      "13 10000003\n",
      "0 668055\n",
      "1 1336110\n",
      "2 2004165\n",
      "3 2672220\n",
      "4 3340275\n",
      "5 4008330\n",
      "6 4676385\n",
      "7 5344440\n",
      "8 6012495\n",
      "9 6680550\n",
      "10 7348605\n",
      "11 8016660\n",
      "12 8684715\n",
      "13 9352770\n",
      "14 10000003\n",
      "0 469606\n",
      "1 939212\n",
      "2 1408818\n",
      "3 1878424\n",
      "4 2348030\n",
      "5 2817636\n",
      "6 3287242\n",
      "7 3756848\n",
      "8 4226454\n",
      "9 4696060\n",
      "10 5165666\n",
      "11 5635272\n",
      "12 6104878\n",
      "13 6574484\n",
      "14 7044090\n",
      "15 7513696\n",
      "16 7983302\n",
      "17 8452908\n",
      "18 8922514\n",
      "19 9392120\n",
      "20 9861726\n",
      "21 10000003\n",
      "[tensor([ 701333,  484321,  382932,  ..., 1041094, 1061848, 1033120]), tensor([307544, 435515, 466295,  ..., 991912, 813389, 556601]), tensor([ 854377,  809859,  858336,  ...,  530171,  518113, 1029049]), tensor([375789, 312328, 187380,  ..., 316436, 432386, 762843]), tensor([443710, 365641, 991547,  ..., 799053,  86646,   5396]), tensor([778357, 856060, 950839,  ..., 619591, 154148, 784711]), tensor([341547, 352594, 120355,  ...,  99976, 175860, 452134]), tensor([100442, 957243, 501269,  ..., 324997, 653432, 785169]), tensor([548470, 486928, 347513,  ..., 465236,  83760, 107836]), tensor([197059, 350717, 857360,  ..., 348530, 636824, 190580])]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfd8bc2cdd3422ea90c60bc224b9baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=100):   0%|          | 0/1070321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_words_seen loris3/stratified_10m_curriculum 99999825\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5cca9f17a7440fa3f2f680abe10b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2568f1406c7490d87c0a0f2af67b86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)berta_incr_influence_epoch_repetition.pt:   0%|          | 0.00/101M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load influence data\n",
    "for model_name, dataset_name, model_type in jobs:\n",
    "\n",
    "    assert dataset_name in model_name\n",
    "\n",
    "\n",
    "    RANDOM_CURRICULUM_NAME =\"random.pt\"\n",
    "\n",
    "\n",
    "    influence_output_dir = os.path.join(\"./influence_mean_normalized\", os.path.basename(model_name), \"_\".join([(os.path.basename(dataset_name) +\"_\"+f\"train[0%:100%]\")]*2))\n",
    "    print(influence_output_dir)\n",
    "    dataset = load_dataset(dataset_name)[\"train\"]\n",
    "    curriculum = util.get_curriculum(dataset_name, RANDOM_CURRICULUM_NAME)\n",
    "\n",
    "    df = pd.DataFrame({int(result_checkpoint.replace(\"checkpoint-\",\"\")): torch.load(os.path.join(influence_output_dir,result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(influence_output_dir)})\n",
    "    df.sort_index(axis=1)\n",
    "\n",
    "    df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "    influence_cols = df.columns\n",
    "    df[\"total\"] = df.sum(axis=1)\n",
    "    df[[\"text\", \"source\",\"stage\"]] = dataset.to_pandas()\n",
    "    df[\"document_lenght\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "    create_curricula(df, influence_cols, dataset_name, model_type, curriculum=RANDOM_CURRICULUM_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #############\n",
    "\n",
    "    \n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order = torch.repeat_interleave(lognorm_order,len(curriculum)//len(lognorm_order), dim=0)\n",
    "        \n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order)\n",
    "    # upload(lognorm_order, \"lognorm\")\n",
    "\n",
    "\n",
    "    # lognorm_order_top_50_pct = get_order_top_n_pct(50,reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct = torch.repeat_interleave(lognorm_order_top_50_pct,len(curriculum)//len(lognorm_order_top_50_pct), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct)\n",
    "\n",
    "    # upload(lognorm_order_top_50_pct, \"lognorm_top_50_pct\")\n",
    "\n",
    "\n",
    "\n",
    "    # torch.manual_seed(42)\n",
    "    # lognorm_order_top_50_pct_shuffled= torch.stack([row[torch.randperm(row.shape[0])] for row in lognorm_order_top_50_pct])\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     lognorm_order_top_50_pct_shuffled = torch.repeat_interleave(lognorm_order_top_50_pct_shuffled,len(curriculum)//len(lognorm_order_top_50_pct_shuffled), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct_shuffled)\n",
    "    # upload(lognorm_order_top_50_pct_shuffled, \"lognorm_top_50_pct_shuffled\")\n",
    "\n",
    "\n",
    "    # dirac_order = get_order_full(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     dirac_order = torch.repeat_interleave(dirac_order,len(curriculum)//len(dirac_order), dim=0)\n",
    "    # plotting.plot_per_document_in_order(df, dirac_order)\n",
    "    # upload(dirac_order, \"dirac\")\n",
    "\n",
    "\n",
    "\n",
    "    # get_order_positive_only = lambda df: [torch.tensor(df[checkpoint][df[checkpoint] >= 0].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]\n",
    "    # positive_only_order = get_order_positive_only(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    # if \"equitoken\" in dataset_name:\n",
    "    #     positive_only_order = [positive_only_order[i] for i in torch.arange(0,len(positive_only_order)).repeat_interleave(len(curriculum)//len(positive_only_order))]\n",
    "    # plotting.plot_per_document_in_order(df, positive_only_order)\n",
    "    # upload(positive_only_order, \"dirac_positive_only\")\n",
    "\n",
    "\n",
    "\n",
    "    # epochs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    # epochs = [epoch.repeat(10) for epoch in epochs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_early_order = [epoch[torch.randperm(epoch.shape[0])] for epoch in epochs]\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_early_order)\n",
    "    # upload(influential_examples_early_order, \"influential_examples_early\")\n",
    "\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    # halfs = [half.repeat(10) for half in halfs]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    # upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    # thirds = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 3)\n",
    "    # thirds = [third.repeat(10) for third in thirds]\n",
    "    # torch.manual_seed(42)\n",
    "    # influential_examples_first_third_order = list(torch.tensor_split(torch.cat([third[torch.randperm(len(third))] for third in thirds]), len(influence_cols)))\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_first_third_order)\n",
    "    # upload(influential_examples_first_third_order, \"influential_examples_first_third\")\n",
    "\n",
    "\n",
    "\n",
    "    # plotting.plot_per_document_in_order(df, influential_examples_sandwich)\n",
    "    # upload(influential_examples_sandwich, \"influential_examples_sandwich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
