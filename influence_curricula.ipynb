{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the official [preprocessing repo](https://github.com/babylm/babylm_data_preprocessing), we get the number of words via `line.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# word_count = lambda d: len([w for w in tokenizer.tokenize(d) if w.isalnum()])\n",
    "# sum([word_count(d) for d in dataset[\"text\"]])\n",
    "word_count = lambda d: len(d.split())\n",
    "word_count_dataset = lambda dataset:sum([word_count(d) for d in dataset[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Curriculum based on Influence during learning on randomly shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import util\n",
    "\n",
    "HF_USERNAME = None\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "if HF_USERNAME is None:\n",
    "    HF_USERNAME = api.whoami()[\"name\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    weights =  norm.pdf(indices, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "\n",
    "\n",
    "def f_filter(size, **args):\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    print(indices)\n",
    "    weights = f.pdf(indices+1, **args)\n",
    "    return weights# / np.sum(weights)\n",
    "def lognorm_filter(size, **args):\n",
    "    #print(args)\n",
    "    indices = np.arange(-size, size+1, 1)\n",
    "    #print(indices)\n",
    "    weights = lognorm.pdf(indices+1, **args)\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dirac_filter = lambda size: np.eye(1, (size*2)+1, size, dtype=float).flatten()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reweight_df(df, influence_cols, filter_weights):\n",
    "    scores = pd.DataFrame(np.apply_along_axis(lambda m: np.convolve(m,filter_weights, mode=\"valid\")[1:-1], axis=1, arr=df[influence_cols].to_numpy()))\n",
    "    scores.columns = influence_cols\n",
    "    df_reweighted = df.copy()\n",
    "    df_reweighted[influence_cols] = scores\n",
    "    df_reweighted[\"total\"] = df_reweighted[influence_cols].sum(axis=1)\n",
    "    return df_reweighted\n",
    "\n",
    "\n",
    "def upload(tensor, name):\n",
    "    p = name + \"_\" + os.path.basename(model_name) + \".pt\"\n",
    "    if PUSH_TO_HF:\n",
    "        torch.save(tensor, p)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=p,\n",
    "            path_in_repo=p,\n",
    "            repo_id=dataset_name,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        os.remove(p)\n",
    "    else:\n",
    "        print(\"skipping upload for\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "\n",
    "username = \"loris3\"\n",
    "\n",
    "tasks = [(model.id, model.id.replace(\"_random\",\"\")) for model in list_models(author=username) if \"random\" in model.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './mean_influence/stratified_10m_curriculum_random/stratified_10m_curriculum_train_stratified_10m_curriculum_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(dataset_name)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m curriculum \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_curriculum(dataset_name, RANDOM_CURRICULUM_NAME)\n\u001b[0;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;28mint\u001b[39m(result_checkpoint\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint-\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)): torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(influence_output_dir,result_checkpoint),weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mfor\u001b[39;00m result_checkpoint \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfluence_output_dir\u001b[49m\u001b[43m)\u001b[49m})\n\u001b[1;32m     17\u001b[0m df\u001b[38;5;241m.\u001b[39msort_index(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;28msorted\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './mean_influence/stratified_10m_curriculum_random/stratified_10m_curriculum_train_stratified_10m_curriculum_train'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load influence data\n",
    "for model_name,dataset_name in tasks:\n",
    "\n",
    "    assert dataset_name in model_name\n",
    "\n",
    "\n",
    "    RANDOM_CURRICULUM_NAME =\"random.pt\"\n",
    "\n",
    "\n",
    "    influence_output_dir = os.path.join(\"./mean_influence\", os.path.basename(model_name), \"_\".join([os.path.basename(dataset_name) +\"_\"+\"train\"]*2))\n",
    "\n",
    "    dataset = load_dataset(dataset_name)[\"train\"]\n",
    "    curriculum = util.get_curriculum(dataset_name, RANDOM_CURRICULUM_NAME)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({int(result_checkpoint.replace(\"checkpoint-\",\"\")): torch.load(os.path.join(influence_output_dir,result_checkpoint),weights_only=True,map_location=\"cpu\").numpy().flatten() for result_checkpoint in os.listdir(influence_output_dir)})\n",
    "    df.sort_index(axis=1)\n",
    "\n",
    "    df = df.reindex(sorted(df.columns, reverse=False), axis=1)\n",
    "    influence_cols = df.columns\n",
    "    df[\"total\"] = df.sum(axis=1)\n",
    "    df[[\"text\", \"source\",\"stage\"]] = dataset.to_pandas()\n",
    "    df[\"document_lenght\"] = df[\"text\"].str.split().str.len()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    get_order_full = lambda df: torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols])\n",
    "    get_order_top_n_pct = lambda top_n_pct, df: torch.stack([torch.tensor(df[checkpoint].sort_values(ascending=False).head(int(len(df)*top_n_pct*0.01)).index.to_numpy()) for checkpoint in influence_cols])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #############\n",
    "\n",
    "    lognorm_order = get_order_full(reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)))\n",
    "    if \"equitoken\" in dataset_name:\n",
    "        lognorm_order = torch.repeat_interleave(lognorm_order,len(curriculum)//len(lognorm_order), dim=0)\n",
    "        \n",
    "    plotting.plot_per_document_in_order(df, lognorm_order)\n",
    "    upload(lognorm_order, \"lognorm\")\n",
    "\n",
    "\n",
    "    lognorm_order_top_50_pct = get_order_top_n_pct(50,reweight_df(df, influence_cols, lognorm_filter(len(influence_cols), s=1, loc=0, scale=0.5)))\n",
    "    if \"equitoken\" in dataset_name:\n",
    "        lognorm_order_top_50_pct = torch.repeat_interleave(lognorm_order_top_50_pct,len(curriculum)//len(lognorm_order_top_50_pct), dim=0)\n",
    "    plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct)\n",
    "\n",
    "    upload(lognorm_order_top_50_pct, \"lognorm_top_50_pct\")\n",
    "\n",
    "\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    lognorm_order_top_50_pct_shuffled= torch.stack([row[torch.randperm(row.shape[0])] for row in lognorm_order_top_50_pct])\n",
    "    if \"equitoken\" in dataset_name:\n",
    "        lognorm_order_top_50_pct_shuffled = torch.repeat_interleave(lognorm_order_top_50_pct_shuffled,len(curriculum)//len(lognorm_order_top_50_pct_shuffled), dim=0)\n",
    "    plotting.plot_per_document_in_order(df, lognorm_order_top_50_pct_shuffled)\n",
    "    upload(lognorm_order_top_50_pct_shuffled, \"lognorm_top_50_pct_shuffled\")\n",
    "\n",
    "\n",
    "    dirac_order = get_order_full(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    if \"equitoken\" in dataset_name:\n",
    "        dirac_order = torch.repeat_interleave(dirac_order,len(curriculum)//len(dirac_order), dim=0)\n",
    "    plotting.plot_per_document_in_order(df, dirac_order)\n",
    "    upload(dirac_order, \"dirac\")\n",
    "\n",
    "\n",
    "\n",
    "    get_order_positive_only = lambda df: [torch.tensor(df[checkpoint][df[checkpoint] >= 0].sort_values(ascending=False).index.to_numpy()) for checkpoint in influence_cols]\n",
    "    positive_only_order = get_order_positive_only(reweight_df(df, influence_cols, dirac_filter(len(influence_cols))))\n",
    "    if \"equitoken\" in dataset_name:\n",
    "        positive_only_order = [positive_only_order[i] for i in torch.arange(0,len(positive_only_order)).repeat_interleave(len(curriculum)//len(positive_only_order))]\n",
    "    plotting.plot_per_document_in_order(df, positive_only_order)\n",
    "    upload(positive_only_order, \"dirac_positive_only\")\n",
    "\n",
    "\n",
    "\n",
    "    epochs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    epochs = [epoch.repeat(10) for epoch in epochs]\n",
    "    torch.manual_seed(42)\n",
    "    influential_examples_early_order = [epoch[torch.randperm(epoch.shape[0])] for epoch in epochs]\n",
    "    plotting.plot_per_document_in_order(df, influential_examples_early_order)\n",
    "    upload(influential_examples_early_order, \"influential_examples_early\")\n",
    "\n",
    "\n",
    "\n",
    "    halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    halfs = [half.repeat(10) for half in halfs]\n",
    "    torch.manual_seed(42)\n",
    "    influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    halfs = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 2)\n",
    "    halfs = [half.repeat(10) for half in halfs]\n",
    "    torch.manual_seed(42)\n",
    "    influential_examples_first_half_order = list(torch.tensor_split(torch.cat([half[torch.randperm(len(half))] for half in halfs]), len(influence_cols)))\n",
    "    plotting.plot_per_document_in_order(df, influential_examples_first_half_order)\n",
    "    upload(influential_examples_first_half_order, \"influential_examples_first_half\")\n",
    "\n",
    "\n",
    "    thirds = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), 3)\n",
    "    thirds = [third.repeat(10) for third in thirds]\n",
    "    torch.manual_seed(42)\n",
    "    influential_examples_first_third_order = list(torch.tensor_split(torch.cat([third[torch.randperm(len(third))] for third in thirds]), len(influence_cols)))\n",
    "    plotting.plot_per_document_in_order(df, influential_examples_first_third_order)\n",
    "    upload(influential_examples_first_third_order, \"influential_examples_first_third\")\n",
    "\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    chunks = torch.tensor_split(torch.tensor(df.sort_values(by=\"total\", ascending=False)[\"total\"].index.to_numpy()), len(influence_cols))\n",
    "    beneficial_chunks = chunks[0:len(chunks)//2]\n",
    "    harmful_chunks = chunks[len(chunks)//2:]\n",
    "    beneficial_chunks_start = [beneficial_chunk.repeat(10) for beneficial_chunk in beneficial_chunks]\n",
    "    harmful_chunks = [harmful_chunk.repeat(5) for harmful_chunk in harmful_chunks]\n",
    "    beneficial_chunks_end = [beneficial_chunk.repeat(5) for beneficial_chunk in beneficial_chunks]\n",
    "    influential_examples_sandwich = list(torch.tensor_split(torch.cat([chunk[torch.randperm(len(chunk))] for chunk in beneficial_chunks_start + harmful_chunks + beneficial_chunks_end]), len(influence_cols)))\n",
    "    plotting.plot_per_document_in_order(df, influential_examples_sandwich)\n",
    "    upload(influential_examples_sandwich, \"influential_examples_sandwich\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
