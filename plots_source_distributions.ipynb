{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf25bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from itertools import product\n",
    "from datasets import load_dataset,load_from_disk\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import util\n",
    "import torch\n",
    "import plotting\n",
    "from scipy.stats import power_divergence\n",
    "\n",
    "import config\n",
    "from itertools import product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c24a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import threading\n",
    "from multiprocessing import Manager\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "# Use Manager to create shared cache and stats list\n",
    "manager = Manager()\n",
    "dataset_cache = manager.dict()\n",
    "df_cache = manager.dict()\n",
    "stats = manager.list()  # Shared list to store stats from all processes\n",
    "\n",
    "def get_cached_dataset(dataset_name):\n",
    "    if dataset_name not in dataset_cache:\n",
    "        print(f\"[Dataset] Loading: {dataset_name}\")\n",
    "        dataset_cache[dataset_name] = load_dataset(dataset_name)[\"train\"].select_columns(\"stage\")\n",
    "    return dataset_cache[dataset_name]\n",
    "\n",
    "def get_prepared_df(dataset_name, model_type):\n",
    "    cache_key = (dataset_name, model_type)\n",
    "\n",
    "    if cache_key not in df_cache:\n",
    "        print(f\"[DataFrame] Preparing for: {dataset_name} / {model_type}\")\n",
    "        \n",
    "        dataset = get_cached_dataset(dataset_name)\n",
    "        model_name = dataset_name + \"_\" + model_type + \"_random\"\n",
    "        \n",
    "        influence_output_dir = os.path.join(\n",
    "            \"./influence_mean_normalized\",\n",
    "            os.path.basename(model_name),\n",
    "            \"_\".join([(os.path.basename(dataset_name) + \"_\" + \"train[0%:100%]\")] * 2),\n",
    "        )\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            int(result_checkpoint.replace(\"checkpoint-\", \"\")): torch.load(\n",
    "                os.path.join(influence_output_dir, result_checkpoint),\n",
    "                weights_only=True,\n",
    "                map_location=\"cpu\"\n",
    "            ).numpy().flatten()\n",
    "            for result_checkpoint in os.listdir(influence_output_dir)\n",
    "        })\n",
    "\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "        df[\"stage\"] = dataset.to_pandas()\n",
    "\n",
    "        df_cache[cache_key] = df\n",
    "        print(f\"[DataFrame] Cached for: {dataset_name} / {model_type}\")\n",
    "    return df_cache[cache_key]\n",
    "\n",
    "# Generate combinations\n",
    "all_combinations = []\n",
    "\n",
    "for model_type in config.model_types: \n",
    "    curricula = [model_type + c for c in config.influence_curricula] + config.baseline_curricula\n",
    "    for a, b in product(curricula, curricula):\n",
    "        for dataset in config.datasets:\n",
    "            all_combinations.append((dataset, model_type, a, b))\n",
    "print(all_combinations)\n",
    "def process_combination(combination):\n",
    "    dataset_name, model_type, curriculum_a_name, curriculum_b_name = combination\n",
    "    local_stats = []\n",
    "\n",
    "    try:\n",
    "        df = get_prepared_df(dataset_name, model_type)\n",
    "\n",
    "        epsilon = 1e-100\n",
    "        bins = 1000\n",
    "\n",
    "        curriculum_a = util.get_curriculum(dataset_name, curriculum_a_name)\n",
    "        curriculum_b = util.get_curriculum(dataset_name, curriculum_b_name)\n",
    "\n",
    "        examples_a = df.iloc[torch.cat(curriculum_a).flatten()]\n",
    "        examples_b = df.iloc[torch.cat(curriculum_b).flatten()]\n",
    "\n",
    "        all_stages = examples_a[\"stage\"].unique()\n",
    "\n",
    "        max_len = min(len(examples_a), len(examples_b))\n",
    "        bin_size = max_len // bins\n",
    "\n",
    "        chunks_to_compare = [\n",
    "            (examples_a[\"stage\"][i:i+bin_size], examples_b[\"stage\"][i:i+bin_size])\n",
    "            for i in range(0, max_len, bin_size) if i + bin_size < max_len\n",
    "        ]\n",
    "        \n",
    "        local_stats.extend([\n",
    "            (\n",
    "                idx, curriculum_a_name, curriculum_b_name, dataset_name,\n",
    "                model_type, len(a), len(b),\n",
    "                jensenshannon(\n",
    "                    p=a.value_counts().reindex(all_stages, fill_value=epsilon),\n",
    "                    q=b.value_counts().reindex(all_stages, fill_value=epsilon),\n",
    "                    \n",
    "                )**2,\n",
    "                entropy(pk=a.value_counts().reindex(all_stages, fill_value=epsilon)),\n",
    "                \n",
    "                \n",
    "                \n",
    "            )\n",
    "            for idx, (a, b) in enumerate(chunks_to_compare)\n",
    "        ])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Skipping: {dataset_name}, {model_type}, {curriculum_a_name}, {curriculum_b_name}\")\n",
    "        print(\"Reason:\", str(e))\n",
    "  \n",
    "    # Append local stats to the shared stats list safely\n",
    "    with stats_lock:\n",
    "        stats.extend(local_stats)\n",
    "\n",
    "# Prepare interleaved batches\n",
    "import random\n",
    "from itertools import chain, zip_longest\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "first_batch = all_combinations[:3]\n",
    "remaining_batch = all_combinations[3:]\n",
    "\n",
    "# Use ThreadPoolExecutor for the first batch\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    futures = [executor.submit(process_combination, c) for c in first_batch]\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing (3 workers)\") :\n",
    "        pass\n",
    "print(stats)\n",
    "\n",
    "print(len(stats))\n",
    "with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "    futures = [executor.submit(process_combination, c) for c in remaining_batch]\n",
    "    for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing (64 workers)\") :\n",
    "        pass\n",
    "print(len(stats))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f51aa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stats_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurriculum_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcurriculum_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_size_a\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_size_b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentropy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:843\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    841\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/managers.py:818\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    815\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tls\u001b[38;5;241m.\u001b[39mconnection\n\u001b[1;32m    817\u001b[0m conn\u001b[38;5;241m.\u001b[39msend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id, methodname, args, kwds))\n\u001b[0;32m--> 818\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:250\u001b[0m, in \u001b[0;36m_ConnectionBase.recv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 250\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler\u001b[38;5;241m.\u001b[39mloads(buf\u001b[38;5;241m.\u001b[39mgetbuffer())\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats_df = pd.DataFrame(list(stats),columns=[\"chunk\",\"curriculum_a\", \"curriculum_b\",\"dataset\", \"model_type\", \"chunk_size_a\", \"chunk_size_b\", \"stat\",\"entropy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26092fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.influence_curricula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f84e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"source_difficulty\", \"_influence_top_50_cp_shuffled\", \"_influence_epoch_repetition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4084c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1639eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming stats_df is your data frame, you can calculate the std for the 'power_divergence_stat' column\n",
    "std_dev = stats_df['stat'].std()\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "g = sns.FacetGrid(stats_df, row=\"model_type\", height=8, sharex=False)\n",
    "\n",
    "def plot_heatmap(data, **kwargs):\n",
    "    pivot_df = data.pivot_table(index=\"curriculum_a\", \n",
    "                                columns=\"curriculum_b\", \n",
    "                                values=\"stat\", \n",
    "                                aggfunc='mean')\n",
    "    pivot_df = pivot_df.fillna(float('nan'))\n",
    "    pivot_df = pivot_df.rename(index=util.rename, columns=util.rename)\n",
    "    pivot_df = pivot_df.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "\n",
    "    pivot_df = pivot_df[pivot_df.index.isin([util.rename(c) for c in config.baseline_curricula])]\n",
    "    pivot_df = pivot_df[[col for col in pivot_df.columns if col not in [util.rename(c) for c in config.baseline_curricula]]]\n",
    "\n",
    "\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws=dict(use_gridspec=True, location=\"top\", fraction=0.089, pad=0.04,label=r\"mean JSD\"),cbar=True, robust=True, **kwargs)\n",
    "\n",
    "g.map_dataframe(plot_heatmap)\n",
    "g.set_titles(col_template=\"{col_name}\", fontsize=16, fontweight=\"bold\")\n",
    "g.set_axis_labels(\"expected\", \"observed\", fontsize=16)\n",
    "\n",
    "g.tick_params(axis='y', rotation=0, which='major', labelsize=24)\n",
    "g.tick_params(axis='x', rotation=90, which='major', labelsize=24)\n",
    "for ax in g.axes.flat:\n",
    "    ax.tick_params(axis='both', which='major', labelsize=19)\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(\"./autogenerated_figures\", \"source_distribution_heatmap_both.pdf\"), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9de52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 14))\n",
    "data = stats_df[stats_df[\"model_type\"] == \"llama\"]\n",
    "pivot_df = data.pivot_table(index=\"curriculum_b\", \n",
    "                                columns=\"curriculum_a\", \n",
    "                                values=\"stat\", \n",
    "                                aggfunc='mean',fill_value=np.nan)\n",
    "display(pivot_df)\n",
    "\n",
    "pivot_df = pivot_df.rename(index=util.rename, columns=util.rename)\n",
    "# pivot_df = pivot_df.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "# Set vmax to 3 standard deviations\n",
    "\n",
    "pivot_df = pivot_df[pivot_df.index.isin([util.rename(c) for c in config.baseline_curricula])]\n",
    "\n",
    "# pivot_df = pivot_df[[ (col not in [util.rename(c) for c in config.baseline_curricula])]]\n",
    "pivot_df = pivot_df.T\n",
    "mask = np.triu(np.ones_like(pivot_df, dtype=bool), k=1)\n",
    "sns.heatmap(pivot_df, ax=ax, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar=True, \n",
    "                    cbar_kws=dict(use_gridspec=True, location=\"top\", fraction=0.02, pad=0.02,label=r\"mean JSD\"))\n",
    "\n",
    "ax.set(xlabel=\"\", ylabel=\"\")\n",
    "\n",
    "ax.tick_params(axis='y', rotation=0, which='major', labelsize=24)\n",
    "ax.tick_params(axis='x', rotation=0, which='major', labelsize=24)\n",
    "# Increase label sizes for both x and y axes\n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize=25)\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize=25)\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_facecolor(\"white\")\n",
    "plt.tight_layout(pad=3)\n",
    "plt.savefig(os.path.join(\"./autogenerated_figures\", \"source_distribution_llama.pdf\"), dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc08e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = stats_df[[\"chunk\",\"dataset\", \"model_type\", \"curriculum_a\",\"entropy\"]].groupby([\"dataset\", \"model_type\", \"curriculum_a\"]).mean().reset_index()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = pd.read_pickle(\"./benchmark_results.pkl\")\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results.sort_values(by=\"average_improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"curriculum\"] = d[\"curriculum_a\"].apply(util.rename)\n",
    "stats_df_merged = benchmark_results.merge(d, left_on=\"curriculum\", right_on=\"curriculum\")\n",
    "stats_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_merged[\"entropy\"].corr(stats_df_merged[\"average_improvement\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9bdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "stats_df_merged\n",
    "entropy = stats_df_merged[\"entropy\"]\n",
    "average_improvement = stats_df_merged[\"model_acc\"]\n",
    "label = stats_df_merged[\"model\"]\n",
    "\n",
    "corr_coefficient, p_value = stats.pearsonr(entropy, average_improvement)\n",
    "\n",
    "print(f\"Correlation Coefficient: {corr_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2995da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "unique_curriculum = stats_df_merged['curriculum'].unique()\n",
    "\n",
    "available_markers = ['o', 's', '^', 'D', 'P', '*', 'X', 'H', 'v', '<', '>']\n",
    "\n",
    "markers = {unique_curriculum[i]: available_markers[i % len(available_markers)] for i in range(len(unique_curriculum))}\n",
    "\n",
    "sns.scatterplot(data=stats_df_merged, x=\"entropy\", y=\"model_acc\", hue=\"curriculum\", style=\"curriculum\", markers=markers)\n",
    "\n",
    "sns.regplot(data=stats_df_merged, x=\"entropy\", y=\"model_acc\", scatter=False, color='black', line_kws={'lw': 2, 'ls': '--'})\n",
    "\n",
    "plt.xlabel(\"Entropy\")\n",
    "plt.ylabel(\"Acc\")\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f2c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(d.sort_values(by=\"entropy\", ascending=False), x=\"curriculum_a\", y=\"entropy\")\n",
    "plt.xticks(rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27beaf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19784d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df[\"$C_{rand}$\"].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e24de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df[pivot_df.index.isin(['$C_{MATTR}$',\n",
    "       '$C_{PPL}$', '$C_{\\searrow}$', '$C_{rand}$', '$C_{source}$'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ccd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(stats))[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83557cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ddccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
