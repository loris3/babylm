import os

from dotenv import load_dotenv
load_dotenv()
import datasets
from datasets import DatasetDict
from datasets import load_dataset
import datasets
import torch
import json
from huggingface_hub import HfApi
import numpy as np


from scipy.stats import norm
from scipy.stats import f
from scipy.stats import lognorm
import matplotlib.pyplot as plt
import json
import os
import pandas as pd
import torch
import datasets
from datasets import load_dataset
import util
from scipy.stats import zscore
from torch.nn.utils.rnn import pad_sequence
import matplotlib.pyplot as plt
def load_data_for_plotting(dataset_name, dataset_split, model_name, curriculum_name="", influence_output_dir="./mean_influence"):
    # model_name = os.path.join(os.path.basename(dataset_name)+"_"+("" if model_type == "" else model_type +"_")+curriculum_name.split(".")[0])
    influence_output_dir = os.path.join(influence_output_dir, os.path.basename(model_name), os.path.basename(dataset_name) + "_" +dataset_split + "_" + os.path.basename(dataset_name) + "_" +dataset_split)
    print("influence_output_dir",influence_output_dir)
    dataset = load_dataset(dataset_name)["train"]

  

    df = pd.DataFrame({result_checkpoint: torch.load(os.path.join(influence_output_dir,result_checkpoint),weights_only=True,map_location="cpu").numpy().flatten() for result_checkpoint in os.listdir(influence_output_dir)})
    df.sort_index(axis=1)

    df = df.reindex(sorted(df.columns, reverse=False), axis=1)
    influence_cols = df.columns
    df["total"] = df.sum(axis=1)
    if "text" in dataset.features:
        df[["text", "source","stage"]] = dataset.to_pandas()
        df["document_lenght"] = df["text"].str.split().str.len()
    else:
        df[["output", "input","instruction"]] = dataset.to_pandas() # TODO hotfix
    return df, util.get_curriculum(dataset_name, curriculum_name) if curriculum_name != "" else None

from mpl_toolkits.axes_grid1 import make_axes_locatable
def plot_heatmap(type, data, curriculum_name, model_type, dataset_name, y_labels, x_ticks=None, x_tick_labels=None, x_ticks_minor=None, x_tick_labels_minor=None):

    # Create the plot
    fig, ax = plt.subplots(figsize=(5, 4))
    z = 3
    im = ax.imshow(data, aspect="auto", interpolation="none", cmap="RdGy" if "random" in curriculum_name else "RdBu",
                   vmin=np.nanmean(data) - z * np.nanstd(data),
                   vmax=np.nanmean(data) + z * np.nanstd(data))

    if x_ticks is not None:
        ax.set_xticks(x_ticks)
        if x_tick_labels is not None:
            ax.set_xticklabels(x_tick_labels, fontsize=8)
    else:
        ax.set_xticks([])

    if x_ticks_minor is not None and x_tick_labels_minor is not None:
        ax.set_xticks(x_ticks_minor, minor=True)
        ax.set_xticklabels(x_tick_labels_minor, minor=True, rotation=45, fontsize=8)

    ax.set_yticks(np.arange(0.5, len(y_labels), 1))
    ax.set_yticklabels([])

    ax.set_yticks(np.arange(0.0, len(y_labels), 1), minor=True)
    ax.set_yticklabels(["CP {}".format(i+1) for i in range(len(y_labels))], minor=True, fontsize=10)

    ax.grid(color='w', linestyle='-', linewidth=0.5)
    
    # Draw vertical dashed gray lines at the major x-ticks
    if x_ticks is not None:
        for tick in x_ticks:
            ax.axvline(x=tick, color='gray', linestyle='--', linewidth=1)  

    # Create colorbar
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("top", size="5%", pad=0.2)
    cbar = fig.colorbar(im, cax=cax, orientation='horizontal')
    cbar.ax.tick_params(labelsize=7)

    if "random" in curriculum_name:
        cbar.set_label(r"Measured Influence (mean cos. sim. ($\nabla\ell(z), \overline{\nabla\ell(z')}$)", fontsize=9)
    else:
        cbar.set_label(r"Anticipated Influence (mean cos. sim. ($\nabla\ell(z), \overline{\nabla\ell(z')}$)", fontsize=9)
    cbar.ax.xaxis.set_label_position('top')

    plt.tight_layout()

    # Save the figure with optimized parameters
    save_path = os.path.join("./autogenerated_figures", "influence_plots",type, 
                             f"{os.path.basename(dataset_name)}_{os.path.basename(model_type)}_{curriculum_name.replace('.pt','')}.png")
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    fig.savefig(save_path, dpi=600, bbox_inches='tight', transparent=True, format='png')

    # Show the plot
    plt.show()


def plot_per_document_in_order(dff, curriculum_name, model_type, dataset_name, curriculum):
    if isinstance(curriculum, list):
        curriculum = pad_sequence(curriculum, padding_value=-1).T

    out = np.full((len(dff.columns), len(curriculum[0])), np.nan)
    for i in range(len(dff.columns)):
        idxs = curriculum[i].numpy()
        out[i, :] = np.where(idxs != -1, dff.iloc[idxs, i], np.nan)

    plot_heatmap("per_doc_in_order",out, curriculum_name, model_type, dataset_name, list(range(len(dff.columns))))


def plot_per_document_per_stage(dff, curriculum_name, model_type, dataset_name, curriculum):
    if isinstance(curriculum, list):
        curriculum = pad_sequence(curriculum, padding_value=-1).T
        
    dff = dff.sort_values(by="stage")
    vc = dff.groupby("stage")["stage"].count()
    counts, sources = vc.to_numpy(), vc.index.tolist()
    ranges = [(start, stop) for start, stop in zip(np.cumsum(np.hstack((0, counts))), np.cumsum(counts))]
    x_ticks, x_ticks_end = zip(*ranges)

    out = np.empty((len(dff.columns) - 1, len(dff)))
    for i, (source, (start, stop)) in enumerate(zip(sources, ranges)):
        out[:, start:stop] = dff[dff["stage"] == source][dff.columns[:-1]].to_numpy().T

    x_ticks_minor = np.array(x_ticks_end) - (np.array(x_ticks_end) - np.array(x_ticks)) / 2
    plot_heatmap("per_doc_per_stage",out, curriculum_name, model_type, dataset_name, list(range(len(dff.columns) - 1)), 
                 x_ticks=list(x_ticks), x_tick_labels=[""] * len(x_ticks), 
                 x_ticks_minor=x_ticks_minor, x_tick_labels_minor=sources)


def plot_per_token_per_stage(dff, curriculum_name, model_type, dataset_name, curriculum):
    dff = dff.sort_values(by="stage")
    vc = dff.groupby(by="stage")["document_lenght"].sum()
    counts, sources = vc.to_numpy(), vc.index.tolist()
    ranges = [(start, stop) for start, stop in zip(np.cumsum(np.hstack((0, counts))), np.cumsum(counts))]

    out = np.empty((len(dff.columns) - 2, ranges[-1][-1]))
    for i, (source, (start, stop)) in enumerate(zip(sources, ranges)):
        a = dff[dff["stage"] == source][dff.columns[0:-2]].to_numpy().T
        dl = dff[dff["stage"] == source]["document_lenght"].to_numpy().T
        for j in range(a.shape[0]):
            out[j, start:stop] = np.repeat(a[j], dl, axis=0)

    chunk_size = 2
    new_length = out.shape[1] // chunk_size
    out = out[:, :new_length * chunk_size].reshape(out.shape[0], new_length, chunk_size).mean(axis=2)

    x_ticks, x_ticks_end = zip(*[(start // chunk_size, stop // chunk_size) for start, stop in ranges])
    x_ticks_minor = np.array(x_ticks_end) - ((np.array(x_ticks_end) - np.array(x_ticks)) / 2)

    plot_heatmap("per_token_per_stage",out, curriculum_name, model_type, dataset_name, list(range(out.shape[0])),
                 x_ticks=list(x_ticks), x_tick_labels=[""] * len(x_ticks),
                 x_ticks_minor=x_ticks_minor, x_tick_labels_minor=sources)


def plot_per_token_in_order(dff, curriculum_name, model_type, dataset_name, curriculum):
    max_tokens_epoch = max([dff["document_lenght"].iloc[curriculum[i]].sum() for i in range(len(curriculum))])


    out = np.zeros((len(curriculum), max_tokens_epoch))
    for i in range(len(curriculum)):
        idxs = curriculum[i].numpy()
        a = dff.iloc[idxs, i].to_numpy().T
        dl = dff["document_lenght"].iloc[idxs].to_numpy().T
       
        aa = np.repeat(a, dl, axis=0)

        out[i, 0:len(aa)] = aa # some epochs have less than max_tokens_epoch tokens

    chunk_size = 2
    new_length = out.shape[1] // chunk_size
    out = out[:, :new_length * chunk_size].reshape(out.shape[0], new_length, chunk_size).mean(axis=2)

    plot_heatmap("per_token_in_order", out, curriculum_name, model_type, dataset_name, list(range(out.shape[0])))
