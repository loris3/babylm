{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "import torch\n",
    "from scipy.stats import kendalltau\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product, chain\n",
    "import config \n",
    "from util import get_curriculum\n",
    "\n",
    "jobs =[(dataset, curriculum,model_type) for dataset, model_type, curriculum in product(config.datasets, config.model_types, config.baseline_curricula)]\n",
    "jobs.extend([(dataset, model_type + curriculum,model_type) for dataset, model_type, curriculum  in (product(config.datasets, config.model_types, config.influence_curricula))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f528bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import RobertaTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1307c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/stratified_10m_curriculum_roberta_roberta_influence_incr_bins_lognorm_babylm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path, max_len=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(os.path.basename(model_path), revision=\"main\")\n",
    "tokenizer.push_to_hub(os.path.basename(model_path), revision=\"main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(['\"chck_'+ a+'\"' for a in [\"1M\",\"2M\",\"3M\",\"4M\",\"5M\",\"6M\", \"7M\", \"8M\", \"9M\", \"10M\", \"20M\", \"30M\", \"40M\", \"50M\",\"60M\", \"70M\", \"80M\", \"90M\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([(checkpoint, name) for checkpoint, name in zip(sorted([a for a in os.listdir(os.path.join(model_path)) if \"checkpoint\" in a],key=lambda x: int(x.split(\"-\")[-1])),[\"1M\",\"2M\",\"3M\",\"4M\",\"5M\",\"6M\", \"7M\", \"8M\", \"9M\", \"10M\", \"20M\", \"30M\", \"40M\", \"50M\",\"60M\", \"70M\", \"80M\", \"90M\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6099c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for checkpoint_dir, name in [(checkpoint, name) for checkpoint, name in zip(sorted([a for a in os.listdir(os.path.join(model_path)) if \"checkpoint\" in a],key=lambda x: int(x.split(\"-\")[-1])),[\"1M\",\"2M\",\"3M\",\"4M\",\"5M\",\"6M\", \"7M\", \"8M\", \"9M\", \"10M\", \"20M\", \"30M\", \"40M\", \"50M\",\"60M\", \"70M\", \"80M\", \"90M\"])][0:11]:\n",
    "    model = RobertaForMaskedLM.from_pretrained(os.path.join(model_path,checkpoint_dir))\n",
    "    model.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n",
    "    tokenizer.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73239f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_curriculum\n",
    "word_count = lambda d: {\"words\": len(d[\"text\"].split())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"loris3/stratified_10m_curriculum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = get_curriculum(dataset_name, \"roberta_influence_incr_bins_lognorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name)[\"train\"]\n",
    "word_counts_dataset = dataset.map(word_count, num_proc=100)\n",
    "\n",
    "\n",
    "wc = 0\n",
    "wc_total = 0\n",
    "\n",
    "dataset_indices_after_wich_to_checkpoint = []\n",
    "\n",
    "indices = torch.cat(curriculum).flatten().tolist()\n",
    "selected = word_counts_dataset.select(indices)\n",
    "\n",
    "for i,row in enumerate(selected):\n",
    "\n",
    "    wc += row[\"words\"]\n",
    "    wc_total += row[\"words\"]\n",
    "    if (wc >= 1_000_000 and wc_total <= 10_000_000):\n",
    "        dataset_indices_after_wich_to_checkpoint.append(i)\n",
    "        wc = 0\n",
    "    elif (wc >= 10_000_000):\n",
    "        dataset_indices_after_wich_to_checkpoint.append(i)\n",
    "        wc = 0\n",
    "dataset_indices_after_wich_to_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_indices_after_wich_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "effective_batch_size = 32 * 4 * 16\n",
    "save_steps = [math.ceil(i / effective_batch_size) for i in dataset_indices_after_wich_to_checkpoint]\n",
    "save_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f253703",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(save_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
