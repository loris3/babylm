{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ada71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "import torch\n",
    "from scipy.stats import kendalltau\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product, chain\n",
    "import config \n",
    "from util import get_curriculum\n",
    "\n",
    "jobs =[(dataset, curriculum,model_type) for dataset, model_type, curriculum in product(config.datasets, config.model_types, config.baseline_curricula)]\n",
    "jobs.extend([(dataset, model_type + curriculum,model_type) for dataset, model_type, curriculum  in (product(config.datasets, config.model_types, config.influence_curricula))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f528bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from transformers import RobertaTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1307c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/stratified_10m_curriculum_roberta_roberta_influence_incr_bins_lognorm_babylm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0748b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_path, max_len=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf1752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/loris3/stratified_10m_curriculum_roberta_roberta_influence_incr_bins_lognorm_babylm/commit/dfdf7cae4dd03beb0f2db4b9bd6e70638f7a23be', commit_message='Upload tokenizer', commit_description='', oid='dfdf7cae4dd03beb0f2db4b9bd6e70638f7a23be', pr_url=None, repo_url=RepoUrl('https://huggingface.co/loris3/stratified_10m_curriculum_roberta_roberta_influence_incr_bins_lognorm_babylm', endpoint='https://huggingface.co', repo_type='model', repo_id='loris3/stratified_10m_curriculum_roberta_roberta_influence_incr_bins_lognorm_babylm'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(os.path.basename(model_path), revision=\"main\")\n",
    "tokenizer.push_to_hub(os.path.basename(model_path), revision=\"main\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6099c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for checkpoint_dir, name in [(checkpoint, name) for checkpoint, name in zip(sorted([a for a in os.listdir(os.path.join(model_path)) if \"checkpoint\" in a],key=lambda x: int(x.split(\"-\")[-1])),[\"1M\",\"2M\",\"3M\",\"4M\",\"5M\",\"6M\", \"7M\", \"8M\", \"9M\", \"10M\", \"20M\", \"30M\", \"40M\", \"50M\",\"60M\", \"70M\", \"80M\", \"90M\"])][0:11]:\n",
    "#     model = RobertaForMaskedLM.from_pretrained(os.path.join(model_path,checkpoint_dir))\n",
    "#     model.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n",
    "#     tokenizer.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cf6fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import upload_folder\n",
    "api = HfApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23685e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2873992587.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    upload_folder(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_dir in [\"qqp\",\n",
    "\"boolq\",\n",
    "\"multirc\",\n",
    "\"rte\",\n",
    "\"wsc\",\n",
    "\"mnli\",\n",
    "\"mrpc\"]:\n",
    "      print(os.path.join(model_path,checkpoint_dir))\n",
    "     # model.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n",
    "    # tokenizer.push_to_hub(os.path.basename(model_path), revision=\"chck_\"+name)\n",
    "    upload_folder(\n",
    "            folder_path=checkpoint_dir,\n",
    "            path_in_repo=os.path.basename(checkpoint_dir),\n",
    "            repo_id=api.whoami()[\"name\"] + \"/\" + os.path.basename(model_path),\n",
    "            repo_type=\"model\",\n",
    "            create_pr=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b475b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73239f6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madfs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adfs' is not defined"
     ]
    }
   ],
   "source": [
    "adfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068d2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_curriculum\n",
    "word_count = lambda d: {\"words\": len(d[\"text\"].split())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"loris3/stratified_10m_curriculum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec99d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curriculum = get_curriculum(dataset_name, \"roberta_influence_incr_bins_lognorm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name)[\"train\"]\n",
    "word_counts_dataset = dataset.map(word_count, num_proc=100)\n",
    "\n",
    "\n",
    "wc = 0\n",
    "wc_total = 0\n",
    "\n",
    "dataset_indices_after_wich_to_checkpoint = []\n",
    "\n",
    "indices = torch.cat(curriculum).flatten().tolist()\n",
    "selected = word_counts_dataset.select(indices)\n",
    "\n",
    "for i,row in enumerate(selected):\n",
    "\n",
    "    wc += row[\"words\"]\n",
    "    wc_total += row[\"words\"]\n",
    "    if (wc >= 1_000_000 and wc_total <= 10_000_000):\n",
    "        dataset_indices_after_wich_to_checkpoint.append(i)\n",
    "        wc = 0\n",
    "    elif (wc >= 10_000_000):\n",
    "        dataset_indices_after_wich_to_checkpoint.append(i)\n",
    "        wc = 0\n",
    "dataset_indices_after_wich_to_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_indices_after_wich_to_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "effective_batch_size = 32 * 4 * 16\n",
    "save_steps = [math.ceil(i / effective_batch_size) for i in dataset_indices_after_wich_to_checkpoint]\n",
    "save_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f253703",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(save_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
