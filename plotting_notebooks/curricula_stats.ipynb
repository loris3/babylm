{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_PROC_MAP = 150 # expect 30 min with single process\n",
    "PUSH_TO_HF = False\n",
    "\n",
    "DEBUG = True\n",
    "EPOCHS = 10\n",
    "HF_USERNAME = None\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import datasets\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import f\n",
    "from scipy.stats import lognorm\n",
    "import plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEBUG = True\n",
    "\n",
    "import torch\n",
    "from scipy.stats import kendalltau\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from baseline_curricula import validate_training_duration_limitation\n",
    "from itertools import product, chain\n",
    "import config \n",
    "from util import get_curriculum\n",
    "\n",
    "jobs =[(dataset, curriculum,model_type) for dataset, model_type, curriculum in product(config.datasets, config.model_types, config.baseline_curricula)]\n",
    "jobs.extend([(dataset, model_type + curriculum,model_type) for dataset, model_type, curriculum  in (product(config.datasets, config.model_types, config.influence_curricula))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_curriculum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "word_count = lambda d: {\"words\": len(d[\"text\"].split())}\n",
    "cache_path = (os.path.join(\"./plotting_notebooks/cache/\",\"curricula_metrics_counts.pkl\"))\n",
    "\n",
    "\n",
    "def get_tokens_seen(word_counts_dataset, curriculum):\n",
    "    indices = torch.cat(curriculum).flatten().tolist()\n",
    "    selected = word_counts_dataset.select(indices)\n",
    "    return selected[\"words\"]\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "    dataset_cache = {}\n",
    "    unique_datasets = set(dataset for dataset, _,_ in jobs)\n",
    "\n",
    "    # Preprocess and cache each dataset once\n",
    "    for dataset_name in tqdm(unique_datasets, desc=\"Preprocessing datasets\"):\n",
    "        dataset = load_dataset(dataset_name)[\"train\"]\n",
    "        word_counts_dataset = dataset.map(word_count, num_proc=100)\n",
    "        dataset_cache[dataset_name] = {\n",
    "            \"word_counts\": word_counts_dataset,\n",
    "\n",
    "        }\n",
    "\n",
    "    def process_experiment(args):\n",
    "        dataset_name, curriculum_name, model_type,  data_info = args\n",
    "        curriculum = get_curriculum(dataset_name, curriculum_name)\n",
    "        \n",
    "        r = {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"curriculum\": curriculum_name,\n",
    "            \"tokens seen by model\": get_tokens_seen(data_info[\"word_counts\"], curriculum),\n",
    "        }\n",
    "        # print( r, flush=True)\n",
    "        return r\n",
    "\n",
    "    job_args = [(dataset, curriculum, model_type, dataset_cache[dataset]) for dataset, curriculum, model_type in jobs]\n",
    "\n",
    "    with Pool(10) as pool:\n",
    "        results = list(tqdm(pool.imap(process_experiment, job_args), total=len(job_args)))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_pickle(cache_path)\n",
    "\n",
    "else:\n",
    "    print(\"Reusing cached results\")\n",
    "\n",
    "df = pd.read_pickle(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens seen by model\"] = df[\"tokens seen by model\"].apply(lambda x: [np.mean(a) for a in np.array_split(x,1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "df[\"curriculum\"] = df[\"curriculum\"].apply(util.rename)\n",
    "df[\"dataset\"] = df[\"dataset\"].apply(util.rename_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_tokens_over_time(df, save_path=\"./autogenerated_figures/tokens_over_time_facet_grid.pdf\"):\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    df_exploded = df[ (df[\"dataset\"] != \"$D_{equitoken}$\")].explode('tokens seen by model').reset_index(drop=True)\n",
    "\n",
    "    g = sns.FacetGrid(df_exploded, row=\"dataset\", col=\"curriculum\", height=5, aspect=1.5, sharey=True, margin_titles=True)\n",
    "\n",
    "    def plot_func(data, **kwargs):\n",
    "        x = data.groupby('model_type').cumcount()\n",
    "        sns.lineplot(x=x, y=data['tokens seen by model'], hue=data['model_type'], markers=False, palette=\"Set2\", legend=\"full\", linewidth=0.7, **kwargs)\n",
    "\n",
    "    g.map_dataframe(plot_func)\n",
    "\n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xticks([])\n",
    "\n",
    "    g.fig.text(0.5, -0.10, 'Time $\\\\rightarrow$', ha='center', fontsize=12, fontweight='bold')\n",
    "    g.fig.text(-0.02, 0.5, 'Tokens per doc', va='center', rotation='vertical', fontsize=12, fontweight='bold')\n",
    "\n",
    "    g.fig.subplots_adjust(wspace=0)\n",
    "    g.fig.subplots_adjust(hspace=0)\n",
    "\n",
    "    g.fig.set_size_inches(10, 2.5)\n",
    "    g.tight_layout(pad=0)\n",
    "\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_tokens_over_time(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df[\"tokens seen by model var\"] = df['tokens seen by model'].apply(np.var)\n",
    "pd.DataFrame(df.groupby([\"curriculum\"])['tokens seen by model var'].mean().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_results = pd.read_pickle(\"./plotting_notebooks/cache/benchmark_results.pkl\")\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "word_count = lambda d: {\"words\": len(d[\"text\"].split())}\n",
    "cache_path = (os.path.join(\"./plotting_notebooks/cache/\",\"curricula_metrics.csv\"))\n",
    "\n",
    "\n",
    "def count_tokens_seen(word_counts_dataset, curriculum):\n",
    "    indices = torch.cat(curriculum).flatten().tolist()\n",
    "    selected = word_counts_dataset.select(indices)\n",
    "    return sum(selected[\"words\"])\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "    dataset_cache = {}\n",
    "    unique_datasets = set(dataset for dataset, _,_ in jobs)\n",
    "\n",
    "    # Preprocess and cache each dataset once\n",
    "    for dataset_name in tqdm(unique_datasets, desc=\"Preprocessing datasets\"):\n",
    "        dataset = load_dataset(dataset_name)[\"train\"]\n",
    "        word_counts_dataset = dataset.map(word_count, num_proc=100)\n",
    "        dataset_cache[dataset_name] = {\n",
    "            \"word_counts\": word_counts_dataset,\n",
    "            \"tokens_in_dataset\": sum(word_counts_dataset[\"words\"]),\n",
    "            \"dataset_len\": len(dataset),\n",
    "        }\n",
    "\n",
    "    def process_experiment(args):\n",
    "        dataset_name, curriculum_name, model_type,  data_info = args\n",
    "        curriculum = get_curriculum(dataset_name, curriculum_name)\n",
    "        \n",
    "        r = {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"model_type\": model_type,\n",
    "            \"curriculum\": curriculum_name,\n",
    "            \"documents in dataset\": data_info[\"dataset_len\"],\n",
    "            \"documents seen by model\": sum(len(epoch) for epoch in curriculum),\n",
    "            \"tokens in dataset\": data_info[\"tokens_in_dataset\"],\n",
    "            \"tokens seen by model\": count_tokens_seen(data_info[\"word_counts\"], curriculum),\n",
    "        }\n",
    "        # print( r, flush=True)\n",
    "        return r\n",
    "\n",
    "    job_args = [(dataset, curriculum, model_type, dataset_cache[dataset]) for dataset, curriculum, model_type in jobs]\n",
    "\n",
    "    with Pool(10) as pool:\n",
    "        results = list(tqdm(pool.imap(process_experiment, job_args), total=len(job_args)))\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_pickle(cache_path)\n",
    "\n",
    "else:\n",
    "    print(\"Reusing cached results\")\n",
    "\n",
    "df = pd.read_pickle(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.set_index([\"dataset\",\"curriculum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "def process_job(d):\n",
    "    dataset_name, curriculum_name, model_type = d\n",
    "    curriculum = get_curriculum(dataset_name,curriculum_name)\n",
    "    return (d, curriculum)\n",
    "\n",
    "\n",
    "data = []\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(process_job, d) for d in jobs]\n",
    "    for future in tqdm(as_completed(futures), total=len(jobs), desc=\"Processing jobs\"):\n",
    "        data.append(future.result())\n",
    "\n",
    "\n",
    "tensors = [item[1] for item in data]\n",
    "file_names = [item[0] for item in data]\n",
    "\n",
    "\n",
    "\n",
    "def compute_corr(i, j):\n",
    "    a = tensors[i]\n",
    "    b = tensors[j]\n",
    "    stacked =[]\n",
    "    for ta, tb in zip(a, b):\n",
    "        min_len = min(ta.shape[0], tb.shape[0])\n",
    "        stacked.append((ta[:min_len], tb[:min_len]))\n",
    "    return (i, j, np.mean([kendalltau(s[0],s[1], \n",
    "        variant=\"b\", # there will be ties\n",
    "        alternative=\"two-sided\", \n",
    "    )[0] for s in stacked]))\n",
    "\n",
    "n = len(tensors)\n",
    "pairs = [(i, j) for i in range(n) for j in range(i, n)]\n",
    "\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "    results = list(tqdm(executor.map(lambda pair: compute_corr(pair[0], pair[1]), pairs), total=len(pairs), desc=\"Computing correlations\"))\n",
    "\n",
    "\n",
    "corr_matrix = np.zeros((n, n))\n",
    "for i, j, corr in results:\n",
    "    corr_matrix[i, j] = corr\n",
    "    if i != j:\n",
    "        corr_matrix[j, i] = corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = pd.MultiIndex.from_tuples(file_names, names=['dataset', 'curriculum',\"model_type\"])\n",
    "\n",
    "corr_df = pd.DataFrame(corr_matrix, index=multi_index, columns=multi_index)\n",
    "\n",
    "\n",
    "corr_df = corr_df.sort_index(level=1, ascending=True,key=lambda x: x.map(util.rename))\n",
    "corr_df = corr_df.sort_index(level='curriculum', axis=1, ascending=True, key=lambda x: x.map(util.rename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\n",
    "def p(group_by, melted,alternative):\n",
    "\n",
    "    grouped = melted.groupby(group_by)['value_b']\n",
    "\n",
    "    group_combinations = [x for x in grouped.groups.keys()]\n",
    "\n",
    "    ttest_results = []\n",
    " \n",
    "    for i in range(len(group_combinations)):\n",
    "        for j in range(len(group_combinations)):\n",
    "            group1 = grouped.get_group(group_combinations[i])\n",
    "            group2 = grouped.get_group(group_combinations[j])\n",
    "    \n",
    "            if len(group_by) > 1 and (group_combinations[i][0:-1] != group_combinations[j][0:-1]):\n",
    "                continue\n",
    "            elif len(group_by) == 1 and group_combinations[i][0] == group_combinations[j][0]:\n",
    "                    continue\n",
    "            else:\n",
    "                pass\n",
    "  \n",
    "            t_stat, p_value = ttest_ind(group1, group2,alternative=alternative,equal_var=False)\n",
    "            ttest_results.append({\n",
    "                'group_1': group_combinations[i],\n",
    "                'group_2': group_combinations[j],\n",
    "                'mean_1': np.mean(group1),\n",
    "                'mean_2': np.mean(group2),\n",
    "                't_stat': t_stat,\n",
    "                'mean(1)-mean(2)': np.mean(group1) - np.mean(group2) ,\n",
    "                'p_value': p_value,\n",
    "                \"significant\": p_value < 0.05\n",
    "            })\n",
    "\n",
    "    ttest_results_df = pd.DataFrame(ttest_results)\n",
    "\n",
    "    ttest_results_df['pair_key'] = ttest_results_df.apply(lambda row: tuple(sorted([row['group_1'], row['group_2']])), axis=1)\n",
    "    ttest_results_df = ttest_results_df.drop_duplicates(subset='pair_key')\n",
    "    ttest_results_df = ttest_results_df.drop(columns='pair_key')\n",
    "    return ttest_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what baseline curriculum are (increasing, decreasing) curricula most similar to? \n",
    "import util\n",
    "\n",
    "melted = corr_df.melt(ignore_index=False).add_suffix(\"_b\").reset_index()\n",
    "melted= melted[melted.apply(lambda row: \"influence\" not in row[\"curriculum_b\"],axis=1)]\n",
    "\n",
    "melted= melted[melted.apply(lambda row: (\"bins\" in row[\"curriculum\"]) or (\"_cp_dirac\" in row[\"curriculum\"]),axis=1)]\n",
    "melted[\"group\"] = melted.apply(lambda row: \"incr\" if \"incr\" in row[\"curriculum\"] else \"decr\" ,axis=1)\n",
    "\n",
    "group_by = [\"curriculum_b\", \"group\"]  \n",
    "f = p(group_by, melted,alternative=\"greater\")\n",
    "a = f[f[\"group_1\"] != f[\"group_2\"]]\n",
    "numeric_cols = a.select_dtypes(include=[np.number]).columns\n",
    "a.style.format({col: \"{:.3f}\" for col in numeric_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does filtering with lognorm filter increase similarity to baselines?\n",
    "\n",
    "melted = corr_df.melt(ignore_index=False).add_suffix(\"_b\").reset_index()\n",
    "melted= melted[melted.apply(lambda row: \"influence\" not in row[\"curriculum_b\"],axis=1)]\n",
    "melted= melted[melted.apply(lambda row: \"influence\" in row[\"curriculum\"],axis=1)]\n",
    "\n",
    "melted= melted[melted.apply(lambda row: (\"decr_bins_lognorm\" in row[\"curriculum\"]) or (\"decr_bins_dirac\" in row[\"curriculum\"]),axis=1)]\n",
    "\n",
    "\n",
    "melted[\"group\"] = melted.apply(lambda row: \"lognorm\" if \"lognorm\" in row[\"curriculum\"] else \"raw\" ,axis=1)\n",
    "\n",
    "group_by = [\"curriculum_b\", \"group\"]  \n",
    "f = p(group_by, melted,alternative=\"greater\")\n",
    "a = f[f[\"group_1\"] != f[\"group_2\"]]\n",
    "numeric_cols = a.select_dtypes(include=[np.number]).columns\n",
    "a.style.format({col: \"{:.3f}\" for col in numeric_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is similarity to baselines for a curriculum greater for RoBERTa than Llama?\n",
    "\n",
    "melted = corr_df.melt(ignore_index=False).add_suffix(\"_b\").reset_index()\n",
    "melted= melted[melted.apply(lambda row: \"influence\" not in row[\"curriculum_b\"],axis=1)]\n",
    "melted= melted[melted.apply(lambda row: \"influence\" in row[\"curriculum\"],axis=1)]\n",
    "\n",
    "melted= melted[melted.apply(lambda row: (\"_cp_dirac\" in row[\"curriculum\"]),axis=1)]\n",
    "\n",
    "\n",
    "melted[\"group\"] = melted.apply(lambda row: \"RoBERTa\" if \"roberta\" in row[\"curriculum\"] else \"Llama\" ,axis=1)\n",
    "for a in (melted.groupby(\"group\")[\"curriculum\"].unique()):\n",
    "    print(a)\n",
    "\n",
    "group_by = [\"curriculum_b\", \"group\"]  \n",
    "f = p(group_by, melted,alternative=\"two-sided\")\n",
    "a = f[f[\"group_1\"] != f[\"group_2\"]]\n",
    "numeric_cols = a.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "a.style.format({col: \"{:.3f}\" for col in numeric_cols})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do curricula for roberta correlate with those for llama?\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_df, annot=False, cbar=True)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.yticks(rotation=0)  \n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how similar are the curricula across model types?\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_matrices = []\n",
    "\n",
    "unique_datasets = corr_df.index.get_level_values('dataset').unique()\n",
    "unique_model_types = corr_df.index.get_level_values('model_type').unique()\n",
    "\n",
    "for dataset in unique_datasets:\n",
    "\n",
    "    dataset_corr = corr_df.loc[\n",
    "        (dataset, [\"roberta\" + c for c in config.influence_curricula], \"roberta\"),\n",
    "        (dataset, [\"llama\" + c for c in config.influence_curricula], \"llama\")\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    midx = pd.MultiIndex.from_tuples(list(zip(dataset_corr.index,dataset_corr.columns)))\n",
    "    dataset_corr = pd.DataFrame(data=np.diag(dataset_corr), index=midx).T\n",
    "\n",
    "\n",
    "    # index_labels = [util.rename(i) for i in dataset_corr.index.get_level_values(1).unique()]\n",
    "    # column_labels = [util.rename(i) for i in dataset_corr.columns.get_level_values(1).unique()]\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[5, 1], hspace=0)\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "    sns.heatmap(\n",
    "        dataset_corr,\n",
    "        ax=ax,\n",
    "        cbar_ax=cbar_ax,\n",
    "        annot=True,\n",
    "        cmap='RdGy',\n",
    "        fmt='.2f',\n",
    "        cbar_kws={'orientation': 'horizontal'},\n",
    "        linewidths=0.5,\n",
    "        annot_kws={'size': 12},\n",
    "        vmin=-0.5,\n",
    "        vmax=0.5\n",
    "    )\n",
    "\n",
    "   # ax.set_yticklabels(index_labels, rotation=0, fontsize=14)\n",
    "    # ax.set_xticklabels(column_labels, rotation=45, fontsize=14)\n",
    "    print(util.rename_dataset(dataset))\n",
    "    plt.xlabel(\"\", fontsize=14)\n",
    "    plt.ylabel(\"$\\\\tau$\", fontsize=14)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    save_path = os.path.join(\"./autogenerated_figures\", dataset ,\"kendalltau_cross_model_type.pdf\")\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "unique_datasets = corr_df.index.get_level_values('dataset').unique()\n",
    "unique_model_types = corr_df.index.get_level_values('model_type').unique()\n",
    "\n",
    "for dataset in unique_datasets:\n",
    "    for model_type in unique_model_types:\n",
    "        dataset_corr = corr_df.loc[\n",
    "            (dataset, config.baseline_curricula, model_type),\n",
    "            (dataset, [model_type + c for c in config.influence_curricula], model_type)\n",
    "        ]\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[20, 1], hspace=-0.10)\n",
    "        ax = fig.add_subplot(gs[0])\n",
    "        cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "        sns.heatmap(\n",
    "            dataset_corr,\n",
    "            ax=ax,\n",
    "            cbar_ax=cbar_ax,\n",
    "            annot=True,\n",
    "            cmap='RdGy',\n",
    "            fmt='.2f',\n",
    "            cbar_kws={'orientation': 'horizontal'},\n",
    "            linewidths=0.5,\n",
    "            annot_kws={'size': 12},\n",
    "            vmin=-1,\n",
    "            vmax=1\n",
    "        )\n",
    "\n",
    "        ax.set_yticklabels(\n",
    "            [util.rename(i) for i in dataset_corr.index.get_level_values(1).unique()],\n",
    "            rotation=0,\n",
    "            fontsize=14\n",
    "        )\n",
    "        ax.set_xticklabels(\n",
    "            [util.rename(i) for i in dataset_corr.columns.get_level_values(1).unique()],\n",
    "            rotation=45,\n",
    "            fontsize=14\n",
    "        )\n",
    "\n",
    "\n",
    "        plt.xlabel(\"\", fontsize=14)\n",
    "        plt.ylabel(\"\", fontsize=14)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        save_path = os.path.join(\"./autogenerated_figures\", \"kendalltau\", dataset, model_type + \".pdf\")\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "all_matrices = []\n",
    "\n",
    "unique_datasets = corr_df.index.get_level_values('dataset').unique()\n",
    "unique_model_types = corr_df.index.get_level_values('model_type').unique()\n",
    "\n",
    "for dataset in unique_datasets:\n",
    "    for model_type in unique_model_types:\n",
    "        try:\n",
    "            dataset_corr = corr_df.loc[\n",
    "                (dataset, config.baseline_curricula, model_type),\n",
    "                (dataset, [model_type + c for c in config.influence_curricula], model_type)\n",
    "            ]\n",
    "            all_matrices.append(dataset_corr.values)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "mean_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "example = all_matrices[0]\n",
    "index_labels = [util.rename(i) for i in dataset_corr.index.get_level_values(1).unique()]\n",
    "column_labels = [util.rename(i) for i in dataset_corr.columns.get_level_values(1).unique()]\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[20, 1], hspace=-0.10)\n",
    "ax = fig.add_subplot(gs[0])\n",
    "cbar_ax = fig.add_subplot(gs[1])\n",
    "\n",
    "sns.heatmap(\n",
    "    mean_matrix,\n",
    "    ax=ax,\n",
    "    cbar_ax=cbar_ax,\n",
    "    annot=True,\n",
    "    cmap='RdGy',\n",
    "    fmt='.2f',\n",
    "    cbar_kws={'orientation': 'horizontal'},\n",
    "    linewidths=0.5,\n",
    "    annot_kws={'size': 12},\n",
    "    vmin=-0.2,\n",
    "    vmax=0.2\n",
    ")\n",
    "\n",
    "ax.set_yticklabels(index_labels, rotation=0, fontsize=14)\n",
    "ax.set_xticklabels(column_labels, rotation=45, fontsize=14)\n",
    "\n",
    "plt.xlabel(\"\", fontsize=14)\n",
    "plt.ylabel(\"$\\\\tau$\", fontsize=14)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "save_path = os.path.join(\"./autogenerated_figures\", \"kendalltau_mean.pdf\")\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
