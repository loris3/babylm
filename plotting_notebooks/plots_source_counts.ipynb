{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0282fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from itertools import product\n",
    "from datasets import load_dataset,load_from_disk\n",
    "import config\n",
    "import os\n",
    "import pandas as pd\n",
    "import util\n",
    "import torch\n",
    "import plotting\n",
    "from scipy.stats import power_divergence\n",
    "\n",
    "import config\n",
    "from itertools import product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import threading\n",
    "from multiprocessing import Manager\n",
    "\n",
    "from scipy.stats import entropy\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "\n",
    "manager = Manager()\n",
    "dataset_cache = manager.dict()\n",
    "df_cache = manager.dict()\n",
    "stats = manager.list()  \n",
    "\n",
    "def get_cached_dataset(dataset_name):\n",
    "    if dataset_name not in dataset_cache:\n",
    "        dataset_cache[dataset_name] = load_dataset(dataset_name)[\"train\"].select_columns(\"stage\")\n",
    "    return dataset_cache[dataset_name]\n",
    "\n",
    "def get_prepared_df(dataset_name, model_type):\n",
    "    cache_key = (dataset_name, model_type)\n",
    "\n",
    "    if cache_key not in df_cache:        \n",
    "        dataset = get_cached_dataset(dataset_name)\n",
    "        model_name = dataset_name + \"_\" + model_type + \"_random\"\n",
    "        \n",
    "        influence_output_dir = os.path.join(\n",
    "            \"./influence_mean_normalized\",\n",
    "            os.path.basename(model_name),\n",
    "            \"_\".join([(os.path.basename(dataset_name) + \"_\" + \"train[0%:100%]\")] * 2),\n",
    "        )\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            int(result_checkpoint.replace(\"checkpoint-\", \"\")): torch.load(\n",
    "                os.path.join(influence_output_dir, result_checkpoint),\n",
    "                weights_only=True,\n",
    "                map_location=\"cpu\"\n",
    "            ).numpy().flatten()\n",
    "            for result_checkpoint in os.listdir(influence_output_dir)\n",
    "        })\n",
    "\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "        df[\"stage\"] = dataset.to_pandas()\n",
    "\n",
    "        df_cache[cache_key] = df\n",
    "\n",
    "    return df_cache[cache_key]\n",
    "\n",
    "\n",
    "all_combinations = []\n",
    "\n",
    "for model_type in config.model_types: \n",
    "    curricula = [model_type + c for c in config.influence_curricula] + config.baseline_curricula\n",
    "    for curriculum in curricula:\n",
    "        for dataset in config.datasets:\n",
    "            all_combinations.append((dataset, model_type, curriculum))\n",
    "\n",
    "\n",
    "def process_combination(combination):\n",
    "\n",
    "    dataset_name, model_type, curriculum_a_name = combination\n",
    "    local_stats = []\n",
    "    \n",
    "    # try:\n",
    "    df = get_prepared_df(dataset_name, model_type)\n",
    "\n",
    "    epsilon = 1e-100\n",
    "    bins = 1000\n",
    "\n",
    "    curriculum_a = util.get_curriculum(dataset_name, curriculum_a_name)\n",
    "\n",
    "    examples_a = df.iloc[torch.cat(curriculum_a).flatten()]\n",
    "    \n",
    "    all_stages = examples_a[\"stage\"].unique()\n",
    "    \n",
    "    \n",
    "    bin_size = len(examples_a) // bins\n",
    "\n",
    "    chunks = [\n",
    "        examples_a[\"stage\"][i:i+bin_size]\n",
    "        for i in range(0, len(examples_a), bin_size) if i + bin_size < len(examples_a)\n",
    "    ]\n",
    "\n",
    "    for idx, a in enumerate(chunks):\n",
    "        d = a.value_counts().reindex(all_stages, fill_value=epsilon)\n",
    "        d[\"chunk\"] = idx\n",
    "        d[\"curriculum\"] = curriculum_a_name\n",
    "        d[\"dataset\"] = dataset_name\n",
    "        d[\"model_type\"] = model_type\n",
    "        local_stats.append(d)\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[Error] Skipping: {dataset_name}, {model_type}, {curriculum_a_name}, {curriculum_b_name}\")\n",
    "    #     print(\"Reason:\", str(e))\n",
    "\n",
    "    with stats_lock:\n",
    "        stats.extend(local_stats)\n",
    "\n",
    "\n",
    "import random\n",
    "from itertools import chain, zip_longest\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "cache_path = (os.path.join(\"./plotting_notebooks/cache/\",\"source_counts.csv\"))\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "\n",
    "    first_batch = all_combinations[:3]\n",
    "    remaining_batch = all_combinations[3:]\n",
    "\n",
    "\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = [executor.submit(process_combination, c) for c in first_batch]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing (3 workers)\") :\n",
    "            pass\n",
    "\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=32) as executor:\n",
    "        futures = [executor.submit(process_combination, c) for c in remaining_batch]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Processing (64 workers)\") :\n",
    "            pass\n",
    "\n",
    "\n",
    "    stats_df = pd.concat([pd.DataFrame(s).T for s in stats], axis=0)\n",
    "    stats_df.to_pickle(cache_path)\n",
    "\n",
    "\n",
    "stats_df = pd.read_pickle(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99aa76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49eefee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28959c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd401107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def plot_proportions(model_type, skip_redundant=True):\n",
    "    save_path = os.path.join(\"./autogenerated_figures\", \"source_distribution_over_time\", f\"{model_type}.pdf\")\n",
    "\n",
    "    norm_df = stats_df[stats_df[\"model_type\"] == model_type].copy()\n",
    "\n",
    "    datasets = norm_df['dataset'].unique()\n",
    "    stages = sorted([c for c in norm_df.columns if \"C\" in c])\n",
    "    left_group = ['random.pt']\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(datasets), 1,\n",
    "        figsize=(16, 5),\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "    if len(datasets) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for row_idx, dataset in enumerate(datasets):\n",
    "        axes[row_idx].set_title(util.rename_dataset(dataset), loc='left', fontsize=14, pad=2, fontweight='bold')\n",
    "\n",
    "        random_df = None\n",
    "        data = norm_df[norm_df['dataset'] == dataset]\n",
    "\n",
    "        ax = axes[row_idx]\n",
    "\n",
    "        pivot_df = data[stages + [\"curriculum\", \"chunk\"]].fillna(0)\n",
    "        pivot_df = pivot_df.groupby([\"curriculum\", \"chunk\"]).mean()\n",
    "\n",
    "        pivot_df = pivot_df.apply(lambda row: row / row.sum(), axis=1)\n",
    "\n",
    "        pivot_df = pivot_df.loc[\n",
    "                sorted(\n",
    "                        pivot_df.index,\n",
    "                        key=lambda idx: (\n",
    "                            idx[0] not in config.baseline_curricula,  # baseline first (False < True)\n",
    "                            idx[0],  # sort alphabetically by curriculum\n",
    "                            idx[1]   # then by chunk\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "\n",
    "        \n",
    "        s = pivot_df.groupby(\"curriculum\").size()\n",
    "\n",
    "        padding = 10\n",
    "        bottom = pd.Series([0] * len(pivot_df), index=pivot_df.index)\n",
    "        for ii, stage in enumerate(stages):\n",
    "            if stage in pivot_df.columns:\n",
    "                vals = pivot_df[stage]\n",
    "                x_positions = []\n",
    "\n",
    "                x = -0\n",
    "                for i in range(len(s)):\n",
    "                    x_positions.extend(range(x, x + s[i]))\n",
    "                    x = x + s[i] + padding\n",
    "\n",
    "                ax.bar(x_positions, vals, bottom=bottom, label=stage, width=1.0, color=palette[ii])\n",
    "                bottom += vals\n",
    "\n",
    "        ax.set_ylabel(\"Proportion\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        n_curricula = len(pivot_df.index.get_level_values(0).unique())\n",
    "        cumulative_sum = np.cumsum(s)\n",
    "\n",
    "        x_positions = cumulative_sum + np.arange(len(s)) * padding\n",
    "\n",
    "\n",
    "        midpoints = x_positions - s / 2\n",
    "        ax.set_xlim(0, x_positions[-1])\n",
    "        ax.set_xticks(midpoints)\n",
    "     \n",
    "        ax.set_xticklabels([util.rename(i) for i in pivot_df.index.get_level_values(0).unique()], rotation=0,fontsize=14)\n",
    "\n",
    "        a = axes[row_idx]\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        unique_labels = []\n",
    "        unique_handles = []\n",
    "        for handle, label in zip(handles, labels):\n",
    "            if label not in unique_labels:\n",
    "                if data[label].sum() > 0:\n",
    "                    unique_labels.append(label)\n",
    "                    unique_handles.append(handle)\n",
    "        sorted_labels_handles = sorted(zip(unique_labels, unique_handles))\n",
    "        sorted_labels, sorted_handles = zip(*sorted_labels_handles)\n",
    "\n",
    "        a.legend(\n",
    "            handles=sorted_handles,\n",
    "            labels=sorted_labels,\n",
    "            bbox_to_anchor=(0.5, 1.8),  \n",
    "            loc='upper center',\n",
    "            ncol=5,\n",
    "            fontsize=12,\n",
    "        )\n",
    "        \n",
    "    axes[-1].set_xlabel(f\"time $\\\\rightarrow$\")\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(hspace=1.2, top=0.9) \n",
    "\n",
    "\n",
    "    plt.rcParams.update({\n",
    "        \"text.usetex\": False,\n",
    "    })\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "for model_type in config.model_types:\n",
    "    plot_proportions(model_type, skip_redundant=True)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
