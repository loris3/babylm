{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile util.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "def get_epoch_checkpoints(model_dir):\n",
    "    c = sorted([int(str(x).split(\"-\")[-1]) for x in Path(model_dir).glob(\"checkpoint-*\")])\n",
    "    epoch_checkpoints = c[5::6]\n",
    "    if c[-1] not in epoch_checkpoints:\n",
    "        epoch_checkpoints.pop()\n",
    "        epoch_checkpoints.append(c[-1])\n",
    "    return epoch_checkpoints\n",
    "\n",
    "def get_all_chunks(checkpoint_path,gradient_input_dir, gradients_per_file):\n",
    "    return [ os.path.join(gradient_input_dir, checkpoint_path.split(\"-\")[-1] + \"_\" + str(i) + \"_\" + str(i + gradients_per_file)) for i in range(0, len(dataset[\"train\"]), args.gradients_per_file)]\n",
    "\n",
    "\n",
    "\n",
    "import xxhash\n",
    "\n",
    "h = xxhash.xxh64()\n",
    "def get_seed_for_document(document, epoch):\n",
    "    h.update(document.cpu().numpy())\n",
    "    h.update(bytes(epoch))\n",
    "    seed = h.intdigest()\n",
    "    h.reset()\n",
    "    return seed\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "class DeterministicDataCollatorForLanguageModeling (DataCollatorForLanguageModeling): \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask = None):\n",
    "        \"\"\"\n",
    "        Adapted to make masking determinsitic based on (text, epoch). \n",
    "        Just wrapped the original implementation in a for loop where a seed based on (labels, epoch) is set for each individual example before masking.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = inputs.clone()\n",
    "\n",
    "\n",
    "        import torch\n",
    "\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        for i in range(0, labels.shape[0]):\n",
    "            torch.manual_seed(get_seed_for_document(labels[i], self.epoch))\n",
    "\n",
    "            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "\n",
    "            probability_matrix = torch.full(labels[i:i+1].shape, self.mlm_probability)\n",
    "\n",
    "\n",
    "           \n",
    "            probability_matrix.masked_fill_(special_tokens_mask[i:i+1], value=0.0)\n",
    "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "            labels[i:i+1][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.8)).bool() & masked_indices\n",
    "            inputs[i:i+1][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, we replace masked input tokens with random word\n",
    "            indices_random = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            random_words = torch.randint(len(self.tokenizer), labels[i:i+1].shape, dtype=torch.long)\n",
    "            inputs[i:i+1][indices_random] = random_words[indices_random]\n",
    "\n",
    "        ######################\n",
    "        \n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"dataset_folder\", help=\"Path to a dataset folder of .train files that can be read by calling load_dataset('text', <path>)\")\n",
    "parser.add_argument(\"model_output_dir\", help=\"Where the model and checkpoints should be stored\")\n",
    "# parser.add_argument(\"epochs\", help=\"Number of epochs\", type=int)\n",
    "# parser.add_argument(\"mode\", help=\"Set to 'curriculum' for curriculum training, 'shuffle' for default Trainer behaviour\")\n",
    "parser.add_argument(\"curriculum_path\", help=\"A path to a torch tensor of shape (epochs, training examples) with training data ids\")\n",
    "parser.add_argument(\"dataset_folder_eval\", help=\"Path to a dataset folder of .train files that can be read by calling load_dataset('text', <path>)\")\n",
    "\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=64) # TODO\n",
    "# parser.add_argument(\"--checkpoints_per_epoch\", help=\"Checkpoints to store per epoch\", type=int, nargs=\"?\", const=1, default=1)\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"0,1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = 1\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"babylm_pretraining\"\n",
    "\n",
    "if not os.path.exists(args.model_output_dir):\n",
    "    os.makedirs(args.model_output_dir)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(args.dataset_folder).glob(\"**/*.train\")]\n",
    "\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "# # Save files to disk\n",
    "# tokenizer.save_model(args.model_output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from random import randrange\n",
    "import cloudpickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers.trainer_utils import (\n",
    "\n",
    "    seed_worker,\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "#https://discuss.huggingface.co/t/non-shuffle-training/6986/3\n",
    "from torch.utils.data import SequentialSampler\n",
    "class CurriculumTrainer(Trainer):\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Adapted to use EpochVariableDataLoader (skips accelerator!)\n",
    "        \"\"\"\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = OrderedSampler(self.train_dataset, self.state.epoch if self.state.epoch is not None else 0)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "\n",
    "        return EpochVariableDataLoader(train_dataset, data_collator.set_epoch, **dataloader_params) # the Trainer class calls set_epoch on the dataloader, but we also need it in the data_collator\n",
    "        \n",
    "class EpochVariableDataLoader(DataLoader):\n",
    "    def __init__(self, train_dataset, passtrough_function, **dataloader_params):\n",
    "        self.passtrough_function = passtrough_function\n",
    "        super().__init__(train_dataset, **dataloader_params)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.sampler.epoch = epoch    \n",
    "        self.passtrough_function(epoch)    \n",
    "\n",
    "class OrderedSampler(SequentialSampler):\n",
    "    def __init__(self, data_source, epoch):\n",
    "        self.data_source = data_source\n",
    "        self.epoch = epoch\n",
    "        self.curriculum = torch.load(args.curriculum_path, weights_only=True)\n",
    "       \n",
    "    def __iter__(self):\n",
    "        print(\"getting new iterator in epoch\", self.epoch,flush=True)\n",
    "        return iter(self.curriculum[self.epoch].tolist())\n",
    "    \n",
    "\n",
    "\n",
    "# class BrownTrainer(Trainer):\n",
    "#     def _get_train_sampler(self):\n",
    "#         return EpochVariableSampler(self.train_dataset, self.state.epoch, torch.randperm(len(self.train_dataset)).tolist())\n",
    "        \n",
    "\n",
    "# trainer = None\n",
    "\n",
    "# if args.mode == \"shuffle\":\n",
    "#     print(\"Random order!\")\n",
    "\n",
    "# elif args.mode == \"curriculum\":\n",
    "#     print(\"Curriculum!\")\n",
    "#     trainer = CurriculumTrainer( \n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         train_dataset=dataset[\"train\"],\n",
    "        \n",
    "\n",
    "#         )\n",
    "# elif args.mode == \"brown\":\n",
    "#     print(\"Brown!\")\n",
    "#     trainer = BrownTrainer( \n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         data_collator=data_collator,\n",
    "#         train_dataset=dataset[\"train\"],\n",
    "\n",
    "#         )\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(args.dataset_folder).glob(\"**/*.train\")]\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "try:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(args.model_output_dir, max_len=512)\n",
    "except:\n",
    "    # Initialize a tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    # Customize training\n",
    "    tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(args.model_output_dir)\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(args.model_output_dir, max_len=512)\n",
    "data_collator = util.DeterministicDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "import datasets\n",
    "t = lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = datasets.load_from_disk(args.dataset_folder)\n",
    "dataset.set_transform(t)\n",
    "\n",
    "\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "dataset_eval = datasets.load_from_disk(args.dataset_folder_eval)\n",
    "dataset_eval.set_transform(t)\n",
    "\n",
    "data_collator.set_epoch(1000)\n",
    "# # https://github.com/ayoolaolafenwa/TrainNLP\n",
    "# def insert_random_mask(batch):\n",
    "#     features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "#     masked_inputs = data_collator(features)\n",
    "#     return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "# dataset_eval = dataset_eval.map(insert_random_mask,batched=True,)\n",
    "\n",
    "# dataset_eval = dataset_eval.rename_columns({\"masked_input_ids\": \"input_ids\",\n",
    "# \"masked_attention_mask\": \"attention_mask\",\"masked_labels\": \"labels\"})\n",
    "\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=41130,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-05,\n",
    "    # pad_token_id=1  TODO change if using old tokenizers\n",
    "    attention_probs_dropout_prob = 0.1,\n",
    "    # bos_token_id = 0,\n",
    "    # \"eos_token_id\": 2,\n",
    "    hidden_act = \"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size =768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    ")\n",
    "\n",
    "EPOCHS = len(torch.load(args.curriculum_path, weights_only=True))\n",
    "print(\"Detected {} epochs\".format(EPOCHS))\n",
    "\n",
    "\n",
    "#steps_per_epoch = ((len(dataset) / (torch.cuda.device_count()*args.per_device_train_batch_size)) // args.checkpoints_per_epoch ), # roughly N times per epoch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.model_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS, #1000 !?!?!\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    prediction_loss_only=False,\n",
    "    remove_unused_columns=False,\n",
    "    warmup_steps=10000,\n",
    "    learning_rate=3e-4,\n",
    "     report_to=\"wandb\",\n",
    "    eval_on_start=True,\n",
    "     label_names=[\"labels\"],\n",
    "     batch_eval_metrics=True,\n",
    "     per_device_eval_batch_size=128\n",
    "\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "from datasets import load_metric\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "batch_metrics = defaultdict(list) \n",
    "def compute_metrics(eval_pred, compute_result):\n",
    "    global batch_metrics \n",
    "    \n",
    "    #print(\"before\", flush=True)\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    \n",
    "    # https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py\n",
    "    label_mask = torch.where(labels > 0, 1.0, 0.0)\n",
    "    # loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n",
    "\n",
    "    # compute accuracy\n",
    "    accuracy = (torch.equal(torch.argmax(logits, axis=-1), labels) * label_mask)\n",
    "    #print(\"at\")\n",
    "    \n",
    "    # Accumulate batch-level metrics\n",
    "    batch_metrics[\"accuracy\"].append(accuracy.flatten(0))\n",
    "    batch_metrics[\"normalizer\"].append(label_mask.flatten(0))\n",
    "    #print(logits.flatten().shape, accuracy.flatten(0).shape,flush=True)\n",
    "    #print(\"bm\",batch_metrics, flush=True)\n",
    "    \n",
    "    if compute_result:\n",
    "\n",
    "        # for name, score in batch_metrics.items():\n",
    "        #     for s in score:\n",
    "        #         print(name, s.shape, flush=True)\n",
    "        result = {name: torch.cat(score).mean() for name, score in batch_metrics.items()}\n",
    "        batch_metrics = defaultdict(list) \n",
    "        #print(\"r\",result)\n",
    "        return result\n",
    "    else:\n",
    "        return {}\n",
    "    #metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n",
    "\n",
    "trainer = CurriculumTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    "   \n",
    "    )\n",
    "# for batch in trainer.get_eval_dataloader(dataset_eval):\n",
    "#     print(batch[\"labels\"])\n",
    "#     break\n",
    "# print(data_collator(dataset_eval))\n",
    "# print(\"DO NOT TRUST TQDM's time estimates: some modes have varying numbers of steps per epoch\")\n",
    "trainer.train()  \n",
    "trainer.save_model(args.model_output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <module 'collections.abc' from '/usr/lib/python3.10/collections/abc.py'>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "KeyboardInterrupt: \n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: Cython module failed to patch module with custom type\n",
      "2024-10-24 12:40:45.418419: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 12:40:45.435208: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 12:40:45.454169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 12:40:45.459757: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 12:40:45.474788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 12:40:46.321254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run pretrain.py curricula/datasets/curriculum_10M_2024 ./Curriculum_10M_2024 ./curricula/curriculum_10M_2024 curricula/datasets/curriculum_10M_2024_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0,1,2])[0:0+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# clustering = None\n",
    "# with open('brown_clustering', \"rb\") as handle:\n",
    "#     clustering = cloudpickle.load(handle)\n",
    "\n",
    "# class BrownDataset(Dataset):\n",
    "#     def rewrite(self, x):\n",
    "#         result = []\n",
    "#         for doc in x:\n",
    "#            # print(\"doc\", doc, flush=True)\n",
    "#             tokenized = tokenizer.tokenize(doc)\n",
    "#             if len(tokenized) == 0:\n",
    "#                 result.append(doc)\n",
    "#                 continue\n",
    "#             #print(\"tokenized\", tokenized, flush=True)\n",
    "#             IDX = randrange(len(tokenized))\n",
    "\n",
    "#             r = []\n",
    "#             for i, word in enumerate(tokenized):\n",
    "#                 replacement = clustering.get_similar(word)\n",
    "                \n",
    "#                 if i == IDX and len(replacement):\n",
    "#                     r.append(random.choice(replacement)[0])\n",
    "#                 else:\n",
    "#                     r.append(word)\n",
    "#             print(doc,tokenizer.convert_tokens_to_string(r), flush=True)\n",
    "#             result.append(tokenizer.convert_tokens_to_string(r))\n",
    "            \n",
    "\n",
    "#         return result\n",
    "       \n",
    "        \n",
    "\n",
    "#     def __init__(self, data_dir):\n",
    "#         self.size = float('inf')\n",
    "#         self.data = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "#         self.data.set_transform(lambda x : tokenizer(self.rewrite(x[\"text\"]), return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "#     def __len__(self):\n",
    "#         return float('inf')\n",
    "#         # TODO argue that an infinite training dataset is cognitively plausible \n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#      #   print(self.transform(self.data[idx]))\n",
    "#      #   print(self.data[idx], idx)\n",
    "#         return self.data[idx]#tokenizer(, return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "# if args.mode == \"brown\":\n",
    "#     dataset = BrownDataset(args.dataset_folder)\n",
    "#     print(dataset[\"train\"][0])\n",
    "#     exit\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ./train_test is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/data/loriss21dm/babylm/pretrain.py:195\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m    193\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : tokenizer(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_special_tokens_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m--> 195\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_transform(t)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# from huggingface_hub import login\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# login()\u001b[39;00m\n",
      "File \u001b[0;32m/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/datasets/load.py:2671\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2672\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2673\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ./train_test is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "%run pretrain.py ./train_test ./Test curriculum test_random_curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./train_10M ./10MModel 10 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4314|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4831|±  |0.0019|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./10MCurriculum ./10MModelCurriculum 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
