{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"dataset_folder\", help=\"Path to a dataset folder of .train files that can be read by calling load_dataset('text', <path>)\")\n",
    "parser.add_argument(\"model_output_dir\", help=\"Where the model and checkpoints should be stored\")\n",
    "parser.add_argument(\"epochs\", help=\"Number of epochs\", type=int)\n",
    "parser.add_argument(\"mode\", help=\"Set to 'curriculum' for curriculum training, 'shuffle' for default Trainer behaviour\")\n",
    "\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=8)\n",
    "parser.add_argument(\"--checkpoints_per_epoch\", help=\"Checkpoints to store per epoch\", type=int, nargs=\"?\", const=1, default=3)\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"0,1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "\n",
    "\n",
    "if not os.path.exists(args.model_output_dir):\n",
    "    os.makedirs(args.model_output_dir)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(args.dataset_folder).glob(\"**/*.train\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "# # Save files to disk\n",
    "# tokenizer.save_model(args.model_output_dir)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(args.model_output_dir, max_len=512)\n",
    "\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from random import randrange\n",
    "import cloudpickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "clustering = None\n",
    "with open('brown_clustering', \"rb\") as handle:\n",
    "    clustering = cloudpickle.load(handle)\n",
    "\n",
    "class BrownDataset(Dataset):\n",
    "    def rewrite(self, x):\n",
    "        result = []\n",
    "        for doc in x:\n",
    "           # print(\"doc\", doc, flush=True)\n",
    "            tokenized = tokenizer.tokenize(doc)\n",
    "            if len(tokenized) == 0:\n",
    "                result.append(doc)\n",
    "                continue\n",
    "            #print(\"tokenized\", tokenized, flush=True)\n",
    "            IDX = randrange(len(tokenized))\n",
    "\n",
    "            r = []\n",
    "            for i, word in enumerate(tokenized):\n",
    "                replacement = clustering.get_similar(word)\n",
    "                \n",
    "                if i == IDX and len(replacement):\n",
    "                    r.append(random.choice(replacement)[0])\n",
    "                else:\n",
    "                    r.append(word)\n",
    "            print(doc,tokenizer.convert_tokens_to_string(r), flush=True)\n",
    "            result.append(tokenizer.convert_tokens_to_string(r))\n",
    "            \n",
    "\n",
    "        return result\n",
    "       \n",
    "        \n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.size = float('inf')\n",
    "        self.data = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "        self.data.set_transform(lambda x : tokenizer(self.rewrite(x[\"text\"]), return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "    def __len__(self):\n",
    "        return float('inf')\n",
    "        # TODO argue that an infinite training dataset is cognitively plausible \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     #   print(self.transform(self.data[idx]))\n",
    "     #   print(self.data[idx], idx)\n",
    "        return self.data[idx]#tokenizer(, return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "if args.mode == \"brown\":\n",
    "    dataset = BrownDataset(args.dataset_folder)\n",
    "    print(dataset[\"train\"][0])\n",
    "    exit\n",
    "else:\n",
    "    dataset = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "    dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.model_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=((len(dataset[\"train\"]) / (torch.cuda.device_count()*args.per_device_train_batch_size)) // args.checkpoints_per_epoch ), # roughly N times per epoch\n",
    "    seed=42,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    ")\n",
    "#https://discuss.huggingface.co/t/non-shuffle-training/6986/3\n",
    "from torch.utils.data import SequentialSampler\n",
    "class CurriculumTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return SequentialSampler(self.train_dataset)\n",
    "        \n",
    "\n",
    "class BrownTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return SequentialSampler(self.train_dataset)\n",
    "        \n",
    "\n",
    "trainer = None\n",
    "\n",
    "if args.mode == \"shuffle\":\n",
    "    print(\"Random order!\")\n",
    "    trainer = Trainer( # shuffles the data at each epoch by default!\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "elif args.mode == \"curriculum\":\n",
    "    print(\"Curriculum!\")\n",
    "    trainer = CurriculumTrainer( \n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "elif args.mode == \"brown\":\n",
    "    print(\"Brown!\")\n",
    "    trainer = BrownTrainer( \n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(args.model_output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run pretrain.py ./train_test ./10MModelBrown 1 brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./train_10M ./10MModel 10 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4314|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4831|±  |0.0019|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./10MCurriculum ./10MModelCurriculum 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
