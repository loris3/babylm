{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile util.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "def get_epoch_checkpoints(model_dir):\n",
    "    checkpoint_ids = sorted([int(str(x).split(\"-\")[-1]) for x in Path(model_dir).glob(\"checkpoint-*\")])\n",
    "\n",
    "    return [os.path.join(model_dir,\"checkpoint-{}\".format(c)) for c in checkpoint_ids]\n",
    "    # epoch_checkpoints = c[5::6]\n",
    "    # if c[-1] not in epoch_checkpoints:\n",
    "    #     epoch_checkpoints.pop()\n",
    "    #     epoch_checkpoints.append(c[-1])\n",
    "    return epoch_checkpoints\n",
    "\n",
    "# def get_all_chunks(checkpoint_path,gradient_input_dir, gradients_per_file):\n",
    "#     return [ os.path.join(gradient_input_dir, checkpoint_path.split(\"-\")[-1] + \"_\" + str(i) + \"_\" + str(i + gradients_per_file)) for i in range(0, len(dataset[\"train\"]), args.gradients_per_file)]\n",
    "def get_epoch(checkpoint_path):\n",
    "    checkpoint_ids = sorted([int(str(x).split(\"-\")[-1]) for x in Path(os.path.dirname(checkpoint_path)).glob(\"checkpoint-*\")])\n",
    "    return checkpoint_ids.index(int(str(checkpoint_path).split(\"-\")[-1]))\n",
    "\n",
    "\n",
    "import xxhash\n",
    "\n",
    "h = xxhash.xxh64()\n",
    "def get_seed_for_document(document, epoch):\n",
    "    h.update(document.cpu().numpy())\n",
    "    h.update(bytes(epoch))\n",
    "    seed = h.intdigest()\n",
    "    h.reset()\n",
    "    return seed\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "class DeterministicDataCollatorForLanguageModeling (DataCollatorForLanguageModeling): \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask = None):\n",
    "        \"\"\"\n",
    "        Adapted to make masking determinsitic based on (text, epoch). \n",
    "        Just wrapped the original implementation in a for loop where a seed based on (labels, epoch) is set for each individual example before masking.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = inputs.clone()\n",
    "\n",
    "\n",
    "        import torch\n",
    "\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        for i in range(0, labels.shape[0]):\n",
    "            torch.manual_seed(get_seed_for_document(labels[i], self.epoch))\n",
    "\n",
    "            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "\n",
    "            probability_matrix = torch.full(labels[i:i+1].shape, self.mlm_probability)\n",
    "\n",
    "\n",
    "           \n",
    "            probability_matrix.masked_fill_(special_tokens_mask[i:i+1], value=0.0)\n",
    "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "            labels[i:i+1][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.8)).bool() & masked_indices\n",
    "            inputs[i:i+1][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, we replace masked input tokens with random word\n",
    "            indices_random = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            random_words = torch.randint(len(self.tokenizer), labels[i:i+1].shape, dtype=torch.long)\n",
    "            inputs[i:i+1][indices_random] = random_words[indices_random]\n",
    "\n",
    "        ######################\n",
    "        \n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUs = 8\n",
    "per_device_batch_size = 16\n",
    "update_freq = 16\n",
    "NUM_GPUs*per_device_batch_size*update_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUs = 2\n",
    "per_device_batch_size = 64\n",
    "update_freq = 16\n",
    "NUM_GPUs*per_device_batch_size*update_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"config\", help=\"Path to a config.json file\")\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=64) # TODO\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"0,1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "import json\n",
    "config = None\n",
    "with open(args.config) as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "config[\"model_path\"] = os.path.join(\"./models/\",os.path.basename(config[\"curriculum_path\"]))   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"babylm_pretraining\"\n",
    "\n",
    "if not os.path.exists(config[\"model_path\"]):\n",
    "    os.makedirs(config[\"model_path\"])\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from random import randrange\n",
    "import cloudpickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers.trainer_utils import (\n",
    "\n",
    "    seed_worker,\n",
    "\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "#https://discuss.huggingface.co/t/non-shuffle-training/6986/3\n",
    "from torch.utils.data import SequentialSampler\n",
    "class CurriculumTrainer(Trainer):\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Adapted to use EpochVariableDataLoader (skips accelerator!)\n",
    "        \"\"\"\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = OrderedSampler(self.train_dataset, self.state.epoch if self.state.epoch is not None else 0)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "\n",
    "        return EpochVariableDataLoader(train_dataset, data_collator.set_epoch, **dataloader_params) # the Trainer class calls set_epoch on the dataloader, but we also need it in the data_collator\n",
    "        \n",
    "class EpochVariableDataLoader(DataLoader):\n",
    "    def __init__(self, train_dataset, passtrough_function, **dataloader_params):\n",
    "        self.passtrough_function = passtrough_function\n",
    "        super().__init__(train_dataset, **dataloader_params)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.sampler.epoch = epoch    \n",
    "        self.passtrough_function(epoch)    \n",
    "\n",
    "class OrderedSampler(SequentialSampler):\n",
    "    def __init__(self, data_source, epoch):\n",
    "        self.data_source = data_source\n",
    "        self.epoch = epoch\n",
    "        self.curriculum = torch.load(config[\"curriculum_path\"], weights_only=True)\n",
    "       \n",
    "    def __iter__(self):\n",
    "        print(\"getting new iterator in epoch\", self.epoch,flush=True)\n",
    "        return iter(self.curriculum[self.epoch].tolist())\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "\n",
    "import datasets\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = None\n",
    "try:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(config[\"model_path\"], max_len=512)\n",
    "except:\n",
    "\n",
    "    dataset_tokenizer = datasets.load_from_disk(config[\"dataset_folder\"]) # without set_transform\n",
    "    # https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#train-tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    def batch_iterator(batch_size=1000):\n",
    "        for i in range(0, len(dataset_tokenizer), batch_size):\n",
    "            yield dataset_tokenizer[i: i + batch_size][\"text\"]\n",
    "\n",
    "    # Customized training\n",
    "    tokenizer.train_from_iterator(batch_iterator(), vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(config[\"model_path\"])\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(config[\"model_path\"], max_len=512)\n",
    "\n",
    "# we still use dynamic masking (mask differently at each epoch) as in the original RoBERTa paper, but do so deterministically\n",
    "# we do not use sentence packing as that would defeat the purpouse of applying an influence estimation method on a per-document basis\n",
    "# we compensate by increasing batch size\n",
    "data_collator = util.DeterministicDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "t = lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = datasets.load_from_disk(config[\"dataset_folder\"])\n",
    "dataset = dataset.map(t)\n",
    "dataset = dataset.remove_columns([\"text\"]) \n",
    "dataset.set_format(\"torch\")\n",
    "print(dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "dataset_eval = datasets.load_from_disk(config[\"eval_dataset_folder\"])\n",
    "dataset_eval = dataset_eval.map(t)\n",
    "dataset_eval = dataset_eval.remove_columns([\"text\"]) \n",
    "dataset_eval.set_format(\"torch\")\n",
    "\n",
    "data_collator.set_epoch(0)\n",
    "# # https://github.com/ayoolaolafenwa/TrainNLP\n",
    "# def insert_random_mask(batch):\n",
    "#     features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "#     masked_inputs = data_collator(features)\n",
    "#     return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "# dataset_eval = dataset_eval.map(insert_random_mask,batched=True,)\n",
    "\n",
    "# dataset_eval = dataset_eval.rename_columns({\"masked_input_ids\": \"input_ids\",\n",
    "# \"masked_attention_mask\": \"attention_mask\",\"masked_labels\": \"labels\"})\n",
    "\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "tokenizer\n",
    "roberta_config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-05,\n",
    "    attention_probs_dropout_prob = 0.1,\n",
    "    # bos_token_id = 0,\n",
    "    # \"eos_token_id\": 2,\n",
    "    hidden_act = \"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size =768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    ")\n",
    "\n",
    "EPOCHS = len(torch.load(config[\"curriculum_path\"], weights_only=True))\n",
    "print(\"Detected {} epochs\".format(EPOCHS))\n",
    "\n",
    "\n",
    "#steps_per_epoch = ((len(dataset) / (torch.cuda.device_count()*args.per_device_train_batch_size)) // args.checkpoints_per_epoch ), # roughly N times per epoch\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config[\"model_path\"],\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS, #1000 !?!?!\n",
    "    per_device_train_batch_size=64,#args.per_device_train_batch_size,\n",
    "    eval_strategy=\"epoch\",\n",
    "    dataloader_num_workers=10,\n",
    "    \n",
    "   # eval_steps=2,#1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=False, # was True https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/config/pretraining/base.yaml\n",
    "    logging_steps=50,\n",
    "    seed=42,\n",
    "    prediction_loss_only=False,\n",
    "    gradient_accumulation_steps=16, \n",
    "    remove_unused_columns=True,\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md\n",
    "    learning_rate=5e-4, # effective batch size is 2048=16*64*2\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-06,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"polynomial\",\n",
    "    warmup_steps=10000, \n",
    "    report_to=\"wandb\",\n",
    "    #eval_on_start=True,\n",
    "     label_names=[\"labels\"],\n",
    "     batch_eval_metrics=True,\n",
    "    # eval_accumulation_steps=5,\n",
    "     per_device_eval_batch_size=64\n",
    "\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=roberta_config)\n",
    "\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_metric\n",
    "intermediate_logits = [] \n",
    "intermediate_labels = []\n",
    "def compute_metrics(eval_pred, compute_result=True):\n",
    "    global intermediate_logits\n",
    "    global intermediate_labels\n",
    "    \n",
    "    #print(\"before\", flush=True)\n",
    "    logits, labels = eval_pred\n",
    "    # if not torch.is_tensor(logits):\n",
    "    #     logits = torch.tensor(logits)\n",
    "    # if not torch.is_tensor(labels):\n",
    "    #     labels = torch.tensor(labels)\n",
    "    intermediate_logits.append(logits.cpu())\n",
    "    intermediate_labels.append(labels.cpu())\n",
    "\n",
    "\n",
    "    \n",
    "    if compute_result:\n",
    "        import math\n",
    "        # https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py\n",
    "        \n",
    "        # loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n",
    "       \n",
    "        logits = torch.stack(intermediate_logits)\n",
    "        labels = torch.stack(intermediate_labels)\n",
    "        label_mask = torch.where(labels > 0, 1.0, 0.0)\n",
    "        predictions = torch.argmax(logits, axis=-1)\n",
    "        \n",
    "\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, torch.nn.functional.one_hot((labels*label_mask).to(torch.int64), logits.shape[-1]).to(torch.float64))*label_mask\n",
    "           \n",
    "        ##################\n",
    " \n",
    "        result = {\n",
    "            \"accuracy\": accuracy.compute(predictions=predictions.flatten(), references=labels.flatten(), sample_weight=label_mask.flatten())[\"accuracy\"],\n",
    "            \"mlm_perplexity\": math.exp(loss.mean()),\n",
    "            \"mlm_loss\": loss.mean() # TODO just a sanity check for testing\n",
    "            }\n",
    "        intermediate_logits = []\n",
    "        intermediate_labels = []\n",
    "        return result\n",
    "    else:\n",
    "        return {}\n",
    "    #metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n",
    "\n",
    "trainer = CurriculumTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    "   \n",
    "    )\n",
    "\n",
    "# print(\"DO NOT TRUST TQDM's time estimates: some modes have varying numbers of steps per epoch\")\n",
    "trainer.train()  \n",
    "trainer.save_model(config[\"model_path\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: torchvision in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: networkx in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: triton==3.0.0 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: fsspec in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: sympy in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: filelock in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n",
      "Requirement already satisfied: numpy in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /data/loriss21dm/TracInVenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2608791077.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[55], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    python -c \"import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__);\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import torch; import intel_extension_for_pytorch as ipex; print(torch.__version__); print(ipex.__version__);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 07:48:49.555328: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-25 07:48:49.572431: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 07:48:49.593136: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 07:48:49.599024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 07:48:49.615469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 07:48:50.492766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run pretrain.py curricula/datasets/curriculum_10M_2024 ./curriculum_10M_2024_random ./curricula/curriculum_10M_2024_random curricula/datasets/curriculum_10M_2024_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0,1,2])[0:0+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# clustering = None\n",
    "# with open('brown_clustering', \"rb\") as handle:\n",
    "#     clustering = cloudpickle.load(handle)\n",
    "\n",
    "# class BrownDataset(Dataset):\n",
    "#     def rewrite(self, x):\n",
    "#         result = []\n",
    "#         for doc in x:\n",
    "#            # print(\"doc\", doc, flush=True)\n",
    "#             tokenized = tokenizer.tokenize(doc)\n",
    "#             if len(tokenized) == 0:\n",
    "#                 result.append(doc)\n",
    "#                 continue\n",
    "#             #print(\"tokenized\", tokenized, flush=True)\n",
    "#             IDX = randrange(len(tokenized))\n",
    "\n",
    "#             r = []\n",
    "#             for i, word in enumerate(tokenized):\n",
    "#                 replacement = clustering.get_similar(word)\n",
    "                \n",
    "#                 if i == IDX and len(replacement):\n",
    "#                     r.append(random.choice(replacement)[0])\n",
    "#                 else:\n",
    "#                     r.append(word)\n",
    "#             print(doc,tokenizer.convert_tokens_to_string(r), flush=True)\n",
    "#             result.append(tokenizer.convert_tokens_to_string(r))\n",
    "            \n",
    "\n",
    "#         return result\n",
    "       \n",
    "        \n",
    "\n",
    "#     def __init__(self, data_dir):\n",
    "#         self.size = float('inf')\n",
    "#         self.data = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "#         self.data.set_transform(lambda x : tokenizer(self.rewrite(x[\"text\"]), return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "#     def __len__(self):\n",
    "#         return float('inf')\n",
    "#         # TODO argue that an infinite training dataset is cognitively plausible \n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#      #   print(self.transform(self.data[idx]))\n",
    "#      #   print(self.data[idx], idx)\n",
    "#         return self.data[idx]#tokenizer(, return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "# if args.mode == \"brown\":\n",
    "#     dataset = BrownDataset(args.dataset_folder)\n",
    "#     print(dataset[\"train\"][0])\n",
    "#     exit\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ./train_test is neither a `Dataset` directory nor a `DatasetDict` directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/data/loriss21dm/babylm/pretrain.py:200\u001b[0m\n\u001b[1;32m    193\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mDeterministicDataCollatorForLanguageModeling(\n\u001b[1;32m    194\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mlm_probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m\n\u001b[1;32m    195\u001b[0m )\n\u001b[1;32m    198\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x : tokenizer(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_special_tokens_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_transform(t)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# from huggingface_hub import login\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# login()\u001b[39;00m\n",
      "File \u001b[0;32m/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/datasets/load.py:2671\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   2672\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is neither a `Dataset` directory nor a `DatasetDict` directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2673\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ./train_test is neither a `Dataset` directory nor a `DatasetDict` directory."
     ]
    }
   ],
   "source": [
    "%run pretrain.py ./train_test ./Test curriculum test_random_curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./train_10M ./10MModel 10 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4314|Â±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4831|Â±  |0.0019|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: ðŸš€ View run \u001b[33m./asdfa13sdafra\u001b[0m at: \u001b[34mhttps://wandb.ai/loriss/babylm_pretraining/runs/t677v40s\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241025_073016-t677v40s/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %run pretrain.py ./10MCurriculum ./10MModelCurriculum 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
