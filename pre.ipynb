{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"dataset_folder\", help=\"Path to a dataset folder of .train files that can be read by calling load_dataset('text', <path>)\")\n",
    "parser.add_argument(\"model_output_dir\", help=\"Where the model and checkpoints should be stored\")\n",
    "parser.add_argument(\"epochs\", help=\"Number of epochs\", type=int)\n",
    "parser.add_argument(\"mode\", help=\"Set to 'curriculum' for curriculum training, 'shuffle' for default Trainer behaviour\")\n",
    "\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=8)\n",
    "parser.add_argument(\"--checkpoints_per_epoch\", help=\"Checkpoints to store per epoch\", type=int, nargs=\"?\", const=1, default=3)\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"0,1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "\n",
    "\n",
    "if not os.path.exists(args.model_output_dir):\n",
    "    os.makedirs(args.model_output_dir)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(args.dataset_folder).glob(\"**/*.train\")]\n",
    "\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "# ])\n",
    "\n",
    "# # Save files to disk\n",
    "# tokenizer.save_model(args.model_output_dir)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(args.model_output_dir, max_len=512)\n",
    "\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from random import randrange\n",
    "import cloudpickle\n",
    "\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "clustering = None\n",
    "with open('brown_clustering', \"rb\") as handle:\n",
    "    clustering = cloudpickle.load(handle)\n",
    "\n",
    "class BrownDataset(Dataset):\n",
    "    def rewrite(self, x):\n",
    "        result = []\n",
    "        for doc in x:\n",
    "           # print(\"doc\", doc, flush=True)\n",
    "            tokenized = tokenizer.tokenize(doc)\n",
    "            if len(tokenized) == 0:\n",
    "                result.append(doc)\n",
    "                continue\n",
    "            #print(\"tokenized\", tokenized, flush=True)\n",
    "            IDX = randrange(len(tokenized))\n",
    "\n",
    "            r = []\n",
    "            for i, word in enumerate(tokenized):\n",
    "                replacement = clustering.get_similar(word)\n",
    "                \n",
    "                if i == IDX and len(replacement):\n",
    "                    r.append(random.choice(replacement)[0])\n",
    "                else:\n",
    "                    r.append(word)\n",
    "            print(doc,tokenizer.convert_tokens_to_string(r), flush=True)\n",
    "            result.append(tokenizer.convert_tokens_to_string(r))\n",
    "            \n",
    "\n",
    "        return result\n",
    "       \n",
    "        \n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.size = float('inf')\n",
    "        self.data = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "        self.data.set_transform(lambda x : tokenizer(self.rewrite(x[\"text\"]), return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "    def __len__(self):\n",
    "        return float('inf')\n",
    "        # TODO argue that an infinite training dataset is cognitively plausible \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "     #   print(self.transform(self.data[idx]))\n",
    "     #   print(self.data[idx], idx)\n",
    "        return self.data[idx]#tokenizer(, return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "if args.mode == \"brown\":\n",
    "    dataset = BrownDataset(args.dataset_folder)\n",
    "    print(dataset[\"train\"][0])\n",
    "    exit\n",
    "else:\n",
    "    dataset = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "    dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.model_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=((len(dataset[\"train\"]) / (torch.cuda.device_count()*args.per_device_train_batch_size)) // args.checkpoints_per_epoch ), # roughly N times per epoch\n",
    "    seed=42,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    \n",
    "    \n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers.trainer_utils import (\n",
    "\n",
    "    seed_worker,\n",
    "\n",
    ")\n",
    "#https://discuss.huggingface.co/t/non-shuffle-training/6986/3\n",
    "from torch.utils.data import SequentialSampler\n",
    "class CurriculumTrainer(Trainer):\n",
    "    \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Adapted to use EpochVariableDataLoader (skips accelerator!)\n",
    "        \"\"\"\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = OrderedSampler(self.train_dataset, self.state.epoch if self.state.epoch is not None else 0)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "        return EpochVariableDataLoader(train_dataset, **dataloader_params)\n",
    "        \n",
    "class EpochVariableDataLoader(DataLoader):\n",
    "    def set_epoch(self, epoch):\n",
    "        self.sampler.epoch = epoch        \n",
    "\n",
    "class OrderedSampler(SequentialSampler):\n",
    "    def __init__(self, data_source, epoch):\n",
    "        self.data_source = data_source\n",
    "        self.epoch = epoch\n",
    "    def __iter__(self):\n",
    "        print(\"getting new iterator\", flush=True)\n",
    "        return iter(torch.randperm(len(self.data_source)).tolist())\n",
    "    \n",
    "\n",
    "\n",
    "class BrownTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return EpochVariableSampler(self.train_dataset, self.state.epoch, torch.randperm(len(self.train_dataset)).tolist())\n",
    "        \n",
    "\n",
    "trainer = None\n",
    "\n",
    "if args.mode == \"shuffle\":\n",
    "    print(\"Random order!\")\n",
    "    trainer = Trainer( # shuffles the data at each epoch by default!\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        \n",
    "\n",
    "        )\n",
    "elif args.mode == \"curriculum\":\n",
    "    print(\"Curriculum!\")\n",
    "    trainer = CurriculumTrainer( \n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        \n",
    "\n",
    "        )\n",
    "elif args.mode == \"brown\":\n",
    "    print(\"Brown!\")\n",
    "    trainer = BrownTrainer( \n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(args.model_output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum!\n",
      "aaaaaaaaaaaaa 0\n",
      "getting new iterator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 00:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaaaaaa 1\n",
      "getting new iterator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "%run pretrain.py ./train_test ./10MModelBrown 2 curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./train_10M ./10MModel 10 shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Groups     |Version|Filter|n-shot|Metric|Value |   |Stderr|\n",
    "|----------------|-------|------|-----:|------|-----:|---|-----:|\n",
    "|blimp_supplement|N/A    |none  |     0|acc   |0.4314|±  |0.0069|\n",
    "|blimp_filtered  |N/A    |none  |     0|acc   |0.4831|±  |0.0019|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./10MCurriculum ./10MModelCurriculum 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
