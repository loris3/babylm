{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"dataset_folder\", help=\"Path to a dataset folder of .train files that can be read by calling load_dataset('text', <path>)\")\n",
    "parser.add_argument(\"model_output_dir\", help=\"Where the model and checkpoints should be stored\")\n",
    "parser.add_argument(\"epochs\", help=\"Number of epochs\", type=int)\n",
    "parser.dadd_argument(\"shuffle\", help=\"Set to false for curriculum training\", type=bool)\n",
    "\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=8)\n",
    "parser.add_argument(\"--checkpoints_per_epoch\", help=\"Checkpoints to store per epoch\", type=int, nargs=\"?\", const=1, default=3)\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"0,1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "\n",
    "\n",
    "if not os.path.exists(args.model_output_dir):\n",
    "    os.makedirs(args.model_output_dir)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(args.dataset_folder).glob(\"**/*.train\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_model(args.model_output_dir)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(args.model_output_dir, max_len=512)\n",
    "\n",
    "\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"text\", data_dir=args.dataset_folder)\n",
    "dataset.set_transform(lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512))\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=args.model_output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=((len(dataset[\"train\"]) / (torch.cuda.device_count()*args.per_device_train_batch_size)) // args.checkpoints_per_epoch ), # roughly N times per epoch\n",
    "    seed=42,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    ")\n",
    "#https://discuss.huggingface.co/t/non-shuffle-training/6986/3\n",
    "from torch.utils.data import SequentialSampler\n",
    "class CurriculumTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        return SequentialSampler(self.train_dataset)\n",
    "        \n",
    "\n",
    "trainer = None\n",
    "\n",
    "if args.shuffle == False:\n",
    "    print(\"Random order!\")\n",
    "    trainer = Trainer( # shuffles the data at each epoch by default!\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "else:\n",
    "    print(\"Curriculum!\")\n",
    "    trainer = CurriculumTrainer( # shuffles the data at each epoch by default!\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "\n",
    "        )\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(args.model_output_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./train_10M ./10MModel 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pretrain.py ./10MCurriculum ./10MModelCurriculum 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
