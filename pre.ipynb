{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile util.py\n",
    "from pathlib import Path\n",
    "import os\n",
    "def get_epoch_checkpoints(model_dir):\n",
    "    checkpoint_ids = sorted([int(str(x).split(\"-\")[-1]) for x in Path(model_dir).glob(\"checkpoint-*\")])\n",
    "\n",
    "    return [os.path.join(model_dir,\"checkpoint-{}\".format(c)) for c in checkpoint_ids]\n",
    "    # epoch_checkpoints = c[5::6]\n",
    "    # if c[-1] not in epoch_checkpoints:\n",
    "    #     epoch_checkpoints.pop()\n",
    "    #     epoch_checkpoints.append(c[-1])\n",
    "    return epoch_checkpoints\n",
    "\n",
    "# def get_all_chunks(checkpoint_path,gradient_input_dir, gradients_per_file):\n",
    "#     return [ os.path.join(gradient_input_dir, checkpoint_path.split(\"-\")[-1] + \"_\" + str(i) + \"_\" + str(i + gradients_per_file)) for i in range(0, len(dataset[\"train\"]), args.gradients_per_file)]\n",
    "def get_epoch(checkpoint_path):\n",
    "    checkpoint_ids = sorted([int(str(x).split(\"-\")[-1]) for x in Path(os.path.dirname(checkpoint_path)).glob(\"checkpoint-*\")])\n",
    "    return checkpoint_ids.index(int(str(checkpoint_path).split(\"-\")[-1]))\n",
    "\n",
    "\n",
    "import xxhash\n",
    "\n",
    "h = xxhash.xxh64()\n",
    "def get_seed_for_document(document, epoch):\n",
    "    \"\"\"Returns the seed to be used to set torch.manual_seed when doing dynamic masking\n",
    "\n",
    "    Args:\n",
    "        document: A string to get the seed for\n",
    "        epoch: The epoch to get the seed for\n",
    "\n",
    "    Returns:\n",
    "        An integer\n",
    "    \"\"\"\n",
    "    h.update(document.cpu().numpy())\n",
    "    h.update(bytes(epoch))\n",
    "    seed = h.intdigest()\n",
    "    h.reset()\n",
    "    return seed\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import torch\n",
    "class DeterministicDataCollatorForLanguageModeling (DataCollatorForLanguageModeling): \n",
    "    def torch_mask_tokens(self, inputs, special_tokens_mask = None):\n",
    "        \"\"\"\n",
    "        Adapted to make dynamic masking determinsitic based on (text, epoch). \n",
    "        Just wrapped the original implementation in a for loop where a seed based on (labels, epoch) is set for each individual example before masking.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        for i in range(0, labels.shape[0]):\n",
    "            torch.manual_seed(get_seed_for_document(labels[i], self.epoch))\n",
    "\n",
    "            # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "\n",
    "            probability_matrix = torch.full(labels[i:i+1].shape, self.mlm_probability)\n",
    "\n",
    "\n",
    "           \n",
    "            probability_matrix.masked_fill_(special_tokens_mask[i:i+1], value=0.0)\n",
    "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "            labels[i:i+1][~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "            indices_replaced = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.8)).bool() & masked_indices\n",
    "            inputs[i:i+1][indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            # 10% of the time, we replace masked input tokens with random word\n",
    "            indices_random = torch.bernoulli(torch.full(labels[i:i+1].shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "            random_words = torch.randint(len(self.tokenizer), labels[i:i+1].shape, dtype=torch.long)\n",
    "            inputs[i:i+1][indices_random] = random_words[indices_random]\n",
    "\n",
    "        ######################\n",
    "        \n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUs = 8\n",
    "per_device_batch_size = 16\n",
    "update_freq = 16\n",
    "NUM_GPUs*per_device_batch_size*update_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_GPUs = 2\n",
    "per_device_batch_size = 64\n",
    "update_freq = 16\n",
    "NUM_GPUs*per_device_batch_size*update_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pretrain.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pretrain.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "import torch\n",
    "from random import randrange\n",
    "import cloudpickle\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from transformers import RobertaForMaskedLM\n",
    "import evaluate\n",
    "from datasets import load_metric\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from datasets import load_metric\n",
    "import util\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.trainer_utils import (seed_worker)\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import datasets\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import SequentialSampler\n",
    "import json\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"pretraining\")\n",
    "parser.add_argument(\"config\", help=\"Path to a config.json file\")\n",
    "parser.add_argument(\"--per_device_train_batch_size\", help=\"per_device_train_batch_size\", type=int, nargs=\"?\", const=1, default=64) # TODO\n",
    "parser.add_argument(\"--cuda_visible_devices\", help=\"Comma seperated GPU ids to use\", nargs=\"?\", const=1, default=\"\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "config = None\n",
    "with open(args.config) as f:\n",
    "    config = json.load(f)\n",
    "    print(config)\n",
    "config[\"model_path\"] = os.path.join(\"./models/\",os.path.basename(config[\"curriculum_path\"]))   \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_visible_devices\n",
    "os.environ[\"WANDB_PROJECT\"]=\"babylm_pretraining\"\n",
    "\n",
    "if not os.path.exists(config[\"model_path\"]):\n",
    "    os.makedirs(config[\"model_path\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load or create tokenizer\n",
    "tokenizer = None\n",
    "try:\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(config[\"model_path\"], max_len=512)\n",
    "except:\n",
    "\n",
    "    dataset_tokenizer = datasets.load_from_disk(config[\"dataset_folder\"]) # without set_transform\n",
    "    # https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#train-tokenizer\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "    def batch_iterator(batch_size=1000):\n",
    "        for i in range(0, len(dataset_tokenizer), batch_size):\n",
    "            yield dataset_tokenizer[i: i + batch_size][\"text\"]\n",
    "\n",
    "    # Customized training\n",
    "    tokenizer.train_from_iterator(batch_iterator(), vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ])\n",
    "\n",
    "    # Save files to disk\n",
    "    tokenizer.save_model(config[\"model_path\"])\n",
    "    tokenizer = RobertaTokenizerFast.from_pretrained(config[\"model_path\"], max_len=512)\n",
    "\n",
    "# Setup custom data_collator:\n",
    "#   we still use dynamic masking (mask differently at each epoch) as in the original RoBERTa paper, but do so deterministically (by setting the torch seed based on a hash of the document and epoch).\n",
    "#       this is done to make the influence estimation more realistic\n",
    "#   Aside: we do not use sentence packing as that would defeat the purpouse of applying an influence estimation method on a per-document basis\n",
    "\n",
    "data_collator = util.DeterministicDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "data_collator.set_epoch(0)\n",
    "\n",
    "#   we need to change some classes so that they pass down the current epoch to this datacollator, as well as to the dataloader:\n",
    "\n",
    "\n",
    "class EpochVariableDataLoader(DataLoader):\n",
    "    \"\"\"A version of DataLoder that passes trough the epoch to a specified function when it's set_epoch is called.\n",
    "       To enable deterministic dynamic masking, the current epoch must be passed down to the data_collator but the Trainer only calls DataLoader.set_epoch().\n",
    "    \"\"\"\n",
    "    def __init__(self, train_dataset, passtrough_function, **dataloader_params):\n",
    "        self.passtrough_function = passtrough_function\n",
    "        super().__init__(train_dataset, **dataloader_params)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.sampler.epoch = epoch    \n",
    "        self.passtrough_function(epoch)    \n",
    " \n",
    "\n",
    "class OrderedSampler(SequentialSampler):\n",
    "    \"\"\"Loads the curriculum from config[\"curriculum_path\"]:\n",
    "       This file is either a tensor of shape (num_epochs, n), where each row is treated as an epoch, or a list of tensors where each element is treated as an epoch. \n",
    "       The curriculum (and the dataset) may vary in lenght by epoch. \n",
    "       *The huggingface Trainer is oblivious of this so keep that in mind when looking at tqdm runtime estimates!*\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, epoch):\n",
    "        self.data_source = data_source\n",
    "        self.epoch = epoch\n",
    "        self.curriculum = torch.load(config[\"curriculum_path\"], weights_only=True)\n",
    "       \n",
    "    def __iter__(self):\n",
    "        return iter(self.curriculum[self.epoch].tolist())\n",
    "\n",
    "# load and pre-tokenize the dataset (TODO unclear if that actually increases peformance with our custom dataloader and datacollator)\n",
    "\n",
    "t = lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = datasets.load_from_disk(config[\"dataset_folder\"])\n",
    "dataset = dataset.map(t)\n",
    "dataset = dataset.remove_columns([\"text\"]) \n",
    "dataset.set_format(\"torch\")\n",
    "\n",
    "dataset_eval = datasets.load_from_disk(config[\"eval_dataset_folder\"])\n",
    "dataset_eval = dataset_eval.map(t)\n",
    "dataset_eval = dataset_eval.remove_columns([\"text\"]) \n",
    "dataset_eval.set_format(\"torch\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CurriculumTrainer(Trainer):\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Adapted to use EpochVariableDataLoader defined below\n",
    "        Facilitates passing current epoch down to the data_collator (for deterministic dynamic masking) and dataloader (for loading the correct stage in the curriculum)\n",
    "        Skips accelerator (as that just re-instantiates with the default DataLoader class)!\n",
    "        \"\"\"\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "        }\n",
    "\n",
    "        if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "            dataloader_params[\"sampler\"] = OrderedSampler(self.train_dataset, self.state.epoch if self.state.epoch is not None else 0)\n",
    "            dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "            dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "            dataloader_params[\"prefetch_factor\"] = self.args.dataloader_prefetch_factor\n",
    "\n",
    "        return EpochVariableDataLoader(train_dataset, data_collator.set_epoch, **dataloader_params) # the Trainer class calls set_epoch on the dataloader, but we also need it in the data_collator\n",
    "\n",
    "\n",
    "# set up eval \n",
    "from collections import defaultdict\n",
    "batch_metrics = defaultdict(lambda:0) \n",
    "def compute_metrics(eval_pred, compute_result=True):\n",
    "    \"\"\"Computes accuracy and MLM loss (ignore masked tokens) as in https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py \n",
    "\n",
    "    Args:\n",
    "        eval_pred: Tuple of logits and labels\n",
    "        compute_result: Trainer will set this to true once all batches are complete. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Metrics that are logged to W&B\n",
    "    \"\"\"\n",
    "    global batch_metrics \n",
    "    logits, labels = eval_pred\n",
    "    if not torch.is_tensor(logits):\n",
    "        logits = torch.tensor(logits)\n",
    "    if not torch.is_tensor(labels):\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "    predictions = torch.argmax(logits, axis=-1)\n",
    "    label_mask = torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "    batch_metrics[\"accuracy\"] += ((torch.equal(predictions, labels))* label_mask).sum()\n",
    "    batch_metrics[\"mlm_loss\"] += (torch.nn.functional.cross_entropy(logits, torch.nn.functional.one_hot((labels).to(torch.int64), logits.shape[-1]).to(torch.float64))* label_mask).sum()\n",
    "    batch_metrics[\"normalizer\"] += label_mask.sum() # number of non-masked labels, divide this when compute_result to get mean \n",
    "\n",
    "    if compute_result:\n",
    "        result = {\n",
    "            \"accuracy\": batch_metrics[\"accuracy\"] / batch_metrics[\"normalizer\"],\n",
    "            \"mlm_perplexity\": math.exp(batch_metrics[\"mlm_loss\"] / batch_metrics[\"normalizer\"]),\n",
    "            \"mlm_loss\": batch_metrics[\"mlm_loss\"] / batch_metrics[\"normalizer\"],\n",
    "            \"normalizer\" : batch_metrics[\"normalizer\"]\n",
    "            }\n",
    "        batch_metrics = defaultdict(lambda:0) \n",
    "        return result\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "# configs\n",
    "roberta_config = RobertaConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-05,\n",
    "    attention_probs_dropout_prob = 0.1,\n",
    "    hidden_act = \"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size =768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    ")\n",
    "\n",
    "EPOCHS = len(torch.load(config[\"curriculum_path\"], weights_only=True))\n",
    "print(\"Detected {} epochs in the curriculum provided\".format(EPOCHS)) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    seed=42,\n",
    "    output_dir=config[\"model_path\"],\n",
    "    save_strategy=\"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=EPOCHS, # do not change this manually: see the custom OrderedSampler  \n",
    "    dataloader_num_workers=10,\n",
    "    fp16=False, # TODO was True in https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/config/pretraining/base.yaml but loss extreme at start\n",
    "    prediction_loss_only=False,\n",
    "    remove_unused_columns=True,\n",
    "\n",
    "    # https://github.com/facebookresearch/fairseq/blob/main/examples/roberta/README.pretraining.md\n",
    "    # for an effective batch size of  2048=16*64* 2 GPUS:\n",
    "        per_device_train_batch_size=64,\n",
    "        gradient_accumulation_steps=16,\n",
    "        learning_rate=5e-4, \n",
    "\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.98,\n",
    "        adam_epsilon=1e-06,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"polynomial\",\n",
    "        warmup_steps=10000, \n",
    "    # eval\n",
    "        eval_strategy=\"epoch\",\n",
    "        label_names=[\"labels\"], # of eval_dataset\n",
    "        batch_eval_metrics=True,\n",
    "        per_device_eval_batch_size=64,\n",
    "        eval_on_start = True,\n",
    "\n",
    "    # logging\n",
    "        report_to=\"wandb\", \n",
    "        logging_steps=50,   \n",
    "\n",
    "    # debug\n",
    "        no_cuda=True,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=roberta_config)\n",
    "trainer = CurriculumTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "trainer.train()  \n",
    "trainer.save_model(config[\"model_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up eval \n",
    "# from collections import defaultdict\n",
    "# batch_metrics = defaultdict(lambda:0) \n",
    "# def compute_metrics(eval_pred, compute_result=True):\n",
    "#     \"\"\"Computes accuracy and MLM loss (ignore masked tokens) as in https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_mlm_flax.py \n",
    "\n",
    "#     Args:\n",
    "#         eval_pred: Tuple of logits and labels\n",
    "#         compute_result: Trainer will set this to true once all batches are complete. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         Metrics that are logged to W&B\n",
    "#     \"\"\"\n",
    "#     global batch_metrics \n",
    "#     logits, labels = eval_pred\n",
    "#     logits = torch.tensor(logits)\n",
    "#     labels = torch.tensor(labels)\n",
    "\n",
    "#     predictions = torch.argmax(logits, axis=-1)\n",
    "#     label_mask = torch.where(labels > 0, 1.0, 0.0)\n",
    "\n",
    "#     batch_metrics[\"accuracy\"] += ((torch.equal(predictions, labels))* label_mask).sum()\n",
    "#     batch_metrics[\"mlm_loss\"] += (torch.nn.functional.cross_entropy(logits, torch.nn.functional.one_hot((labels).to(torch.int64), logits.shape[-1]).to(torch.float64))* label_mask).sum()\n",
    "#     batch_metrics[\"normalizer\"] += label_mask.sum() # number of non-masked labels, divide this when compute_result to get mean \n",
    "\n",
    "#     if compute_result:\n",
    "#         result = {\n",
    "#             \"accuracy\": batch_metrics[\"accuracy\"] / batch_metrics[\"normalizer\"],\n",
    "#             \"mlm_perplexity\": math.exp(batch_metrics[\"mlm_loss\"] / batch_metrics[\"normalizer\"]),\n",
    "#             \"mlm_loss\": batch_metrics[\"mlm_loss\"] / batch_metrics[\"normalizer\"],\n",
    "#             \"normalizer\" : batch_metrics[\"normalizer\"]\n",
    "#             }\n",
    "#         batch_metrics = defaultdict(lambda:0) \n",
    "#         return result\n",
    "#     else:\n",
    "#         return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
