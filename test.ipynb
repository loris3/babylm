{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trak.projectors import CudaProjector\n",
    "from trak.projectors import ProjectionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slow method runtime: 0.7527 seconds\n",
      "Fast method runtime: 0.0049 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "n_train, n_test, grad_size = 100, 200, 4000\n",
    "train_grads = torch.randn(n_train, grad_size)\n",
    "test_grads = torch.randn(n_test, grad_size)\n",
    "\n",
    "start = time.time()\n",
    "slow_res = []\n",
    "for i in range(len(train_grads)):\n",
    "    sims_for_train_i = []\n",
    "    for j in range(len(test_grads)):\n",
    "        sims_for_train_i.append(torch.nn.functional.cosine_similarity(train_grads[i], test_grads[j], dim=0))\n",
    "    slow_res.append(torch.Tensor(sims_for_train_i).mean())\n",
    "slow_res = torch.Tensor(slow_res)\n",
    "slow_time = time.time() - start\n",
    "print(f\"Slow method runtime: {slow_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "fast_res = []\n",
    "precomp_test_avg = torch.nn.functional.normalize(torch.nn.functional.normalize(test_grads, dim=1).mean(axis=0),dim=0)\n",
    "for i in range(len(train_grads)):\n",
    "    fast_res.append(torch.dot(torch.nn.functional.normalize(train_grads[i], dim=0), precomp_test_avg))\n",
    "fast_res = torch.Tensor(fast_res)\n",
    "fast_time = time.time() - start\n",
    "print(f\"Fast method runtime: {fast_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (100) must match the size of tensor b (200) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_grads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (100) must match the size of tensor b (200) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.cosine_similarity(train_grads, test_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precomputed_b torch.Size([4000])\n",
      "Batched method runtime: 0.0102 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def batched(a, precomputed_b):\n",
    "    results = []\n",
    "    for aa in torch.split(a, 100):\n",
    "        results.append(torch.mv(torch.nn.functional.normalize(aa), precomputed_b))\n",
    "  \n",
    "    result = torch.cat(results)\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "precomputed_b = torch.nn.functional.normalize(test_grads,dim=1).mean(axis=0)\n",
    "print(\"precomputed_b\", precomputed_b.shape)\n",
    "batched_res = batched(train_grads, precomputed_b)\n",
    "batched_time = time.time() - start\n",
    "print(f\"Batched method runtime: {batched_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "assert torch.allclose(slow_res, fast_res), \"Results fast wrong\"\n",
    "assert torch.allclose(slow_res, batched_res), \"Results batched wrong\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3856,  1.1161, -0.1036,  ...,  0.4223,  0.0343,  1.5029],\n",
       "         [-0.6620,  0.8604,  1.0990,  ...,  0.6454, -2.5239,  0.3107],\n",
       "         [ 0.1714, -0.3584, -0.1210,  ..., -0.2899, -1.2966, -1.7349],\n",
       "         ...,\n",
       "         [-0.4732, -0.6423, -1.1674,  ...,  0.4317,  0.6189, -1.6795],\n",
       "         [ 0.5421,  0.1864,  0.3507,  ...,  0.7997, -0.1297, -0.9185],\n",
       "         [ 0.5019,  1.6623, -0.5723,  ...,  0.9868,  0.8770, -1.0427]]),\n",
       " tensor([[ 3.4273e-01, -2.7250e-01, -4.6453e-01,  ..., -1.1107e+00,\n",
       "          -1.2405e+00,  7.8210e-01],\n",
       "         [-1.2946e+00, -3.9343e-01, -2.4400e+00,  ..., -3.2485e-01,\n",
       "          -1.4577e+00,  9.9926e-01],\n",
       "         [ 3.1344e-01,  2.3541e+00, -6.2582e-04,  ..., -1.5518e+00,\n",
       "          -5.5519e-01, -7.3342e-01],\n",
       "         ...,\n",
       "         [-8.8507e-03, -3.2516e-01, -1.5228e+00,  ..., -1.1632e+00,\n",
       "           1.0874e+00,  5.7149e-01],\n",
       "         [-6.3087e-01,  4.1980e-01,  1.1003e+00,  ...,  1.3482e+00,\n",
       "          -6.8238e-01,  3.9552e-01],\n",
       "         [ 2.2978e-01,  1.2527e+00, -1.4594e+00,  ..., -9.7319e-01,\n",
       "           1.6789e-01,  2.1723e+00]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(test_grads, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched chunked mean method runtime: 0.0045 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def batched_chunked_mean(a, precomputed_b):\n",
    "    results = []\n",
    "    for aa in torch.split(a, 100):\n",
    "        results.append(torch.mv(torch.nn.functional.normalize(aa), precomputed_b))\n",
    "  \n",
    "    result = torch.cat(results)\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "precomputed_b = []\n",
    "for bb in torch.split(test_grads, 123):\n",
    "  precomputed_b.append(torch.nn.functional.normalize(bb,dim=1).sum(axis=0))\n",
    "precomputed_b = torch.stack(precomputed_b).sum(axis=0) / test_grads.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "batched_res_chunked_mean = batched_chunked_mean(train_grads, precomputed_b)\n",
    "batched_time = time.time() - start\n",
    "print(f\"Batched chunked mean method runtime: {batched_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assert torch.allclose(slow_res, batched_res_chunked_mean), \"Results batched mean wrong\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched precomp avg method runtime: 0.0020 seconds\n"
     ]
    }
   ],
   "source": [
    "def batched_precomp_avg(a, precomputed_b_avg):\n",
    "    results = []\n",
    "    for aa in torch.split(a, 100):\n",
    "        results.append(torch.mv(torch.nn.functional.normalize(aa, dim = 1), precomputed_b_avg))\n",
    "    result = torch.cat(results)\n",
    "    return result\n",
    "\n",
    "start = time.time()\n",
    "precomputed_b_avg = torch.nn.functional.normalize(test_grads, dim = 1).mean(axis = 0)\n",
    "batched_precomp_avg_res = batched_precomp_avg(train_grads, precomputed_b_avg)\n",
    "batched_precomp_avg_time = time.time() - start\n",
    "print(f\"Batched precomp avg method runtime: {batched_precomp_avg_time:.4f} seconds\")\n",
    "assert torch.allclose(slow_res, batched_precomp_avg_res), \"Results batched wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slow_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1000) must match the size of tensor b (200) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_grads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1000) must match the size of tensor b (200) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "torch.cosine_similarity(train_grads,test_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(11788)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "johnson_lindenstrauss_min_dim(n_samples=939344, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16777216])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.load(\"gradients/OLMo-2-1124-7B-SFT/tulu_3_formatting_errors/train[0%:100%]/main/mean\",map_location=\"cuda\").half().flatten().unsqueeze(0)\n",
    "b = torch.load(\"gradients/OLMo-2-1124-7B-SFT/tulu_3_no_errors/train[0%:100%]/main/mean\",map_location=\"cuda\").half().flatten().unsqueeze(0)\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.path.getsize(\"gradients/OLMo-2-1124-7B-SFT/tulu_3_formatting_errors/train[0%:100%]/main/mean\") >> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector = CudaProjector(grad_dim=a.shape[-1], proj_dim=2**18,seed=42, proj_type=ProjectionType.normal,device=\"cuda\", max_batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trak.projectors.CudaProjector at 0x7f0dc77654f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2388,  0.3356, -0.2365,  ...,  0.0204, -0.0943, -0.1738]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projector.project(a,model_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save( projector.project(a,model_id=0),\"test.grad\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector.project(a,model_id=0).shape[-1] / a.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getsize(\"test.grad\") >> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[8192*k for k in range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "import pandas as pd\n",
    "for proj_dim in [2**k for k in range(9,18)]:\n",
    "    print(proj_dim)\n",
    "    projector = CudaProjector(grad_dim=a.shape[-1], proj_dim=proj_dim,seed=42, proj_type=ProjectionType.normal,device=\"cuda\", max_batch_size=8)\n",
    "    r.append((proj_dim, float(torch.abs(torch.cosine_similarity(projector.project(a,model_id=0), projector.project(b,model_id=0)) - torch.cosine_similarity(a,b))[0])))\n",
    "\n",
    "df = pd.DataFrame(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(df, x=0, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector.project(a,model_id=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# torch.allclose(torch.cosine_similarity(a,b), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(((a * b).sum(dim=1) / (torch.norm(a, p=2, dim=1) * torch.norm(b, p=2, dim=1))),(torch.nn.functional.normalize(a) *torch.nn.functional.normalize(b)).sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert torch.allclose(torch.cosine_similarity(a,b),(torch.nn.functional.normalize(a) *torch.nn.functional.normalize(b)).sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(torch.nn.functional.normalize(a, p=2, dim=1),torch.nn.functional.normalize(b, p=2, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(a, p=2, dim=1) * torch.nn.functional.normalize(b, p=2, dim=1)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((torch.nn.functional.normalize(a, p=2, dim=1) * torch.nn.functional.normalize(b, p=2, dim=1)).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.normalize(b, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(a, p=2, dim=1) * torch.nn.functional.normalize(b, p=2, dim=1)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(torch.nn.functional.normalize(b, p=2, dim=1).mean() * torch.norm(b,dim=1).unsqueeze(1), b.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(b, p=2, dim=1).mean() * torch.norm(b,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(b,dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.normalize(b, p=2, dim=1) * torch.norm(b,dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batched(a, b):\n",
    "    # Normalize tensor b once\n",
    "    mean_b = torch.nn.functional.normalize(b, p=2, dim=1).mean(axis=0)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Split tensor a into batches\n",
    "    gradients_a = torch.split(a, 2)\n",
    "    \n",
    "    for aa in gradients_a:\n",
    "        # Normalize each batch of tensor a\n",
    "        normalized_aa = torch.nn.functional.normalize(aa, p=2, dim=1)\n",
    "       # print(normalized_aa.shape, mean_b.shape)\n",
    "        # Compute the cosine similarity: dot product between each normalized row of a and b\n",
    "        similarity = (normalized_aa*mean_b).sum(dim=1)  # Use matrix multiplication to calculate dot product\n",
    "        \n",
    "        # Append the results\n",
    "        results.append(similarity)\n",
    "    \n",
    "    # Concatenate all batch results\n",
    "    result = torch.cat(results)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "\n",
    "similarity = batched(a, b)\n",
    "similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m b = torch.rand((\u001b[32m1113\u001b[39m,\u001b[32m1111\u001b[39m))\n\u001b[32m     15\u001b[39m precomputed_b = torch.nn.functional.normalize(b)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(slow_res,batched(\u001b[43mtrain_gradients\u001b[49m,precomputed_b))\n",
      "\u001b[31mNameError\u001b[39m: name 'train_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched(a,precomputed_b):\n",
    "    \n",
    "    results = []\n",
    "    for aa in torch.split(a, a.shape[0]):\n",
    "        results.append((torch.nn.functional.normalize(aa) * precomputed_b).sum(dim=1))\n",
    "    result = torch.cat(results)\n",
    "    return result\n",
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.rand((113,1111))\n",
    "b = torch.rand((1113,1111))\n",
    "precomputed_b = torch.nn.functional.normalize(b)\n",
    "\n",
    "assert torch.allclose(slow_res,batched(train_gradients,precomputed_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched(a,precomputed_b):\n",
    "    \n",
    "    results = []\n",
    "    for aa in torch.split(a, a.shape[0]):\n",
    "        results.append((torch.nn.functional.normalize(aa) * precomputed_b).sum(dim=1))\n",
    "    result = torch.cat(results)\n",
    "    return result\n",
    "\n",
    "torch.manual_seed(0)\n",
    "a = torch.rand((11,1111))\n",
    "b = torch.rand((11,1111))\n",
    "precomputed_b = torch.nn.functional.normalize(b).mean(axis=0)\n",
    "print(batched(a,precomputed_b))\n",
    "print(torch.cosine_similarity(a,b))\n",
    "assert torch.allclose(torch.cosine_similarity(a,b).mean(axis = 1), batched(a,precomputed_b))\n",
    "\n",
    "\n",
    "assert torch.allclose(torch.cosine_similarity(a,b),batched(a,precomputed_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched(a,b).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(results)/len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(a) *(torch.nn.functional.normalize(b).mean())).sum(dim=1).mean()############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "Y = bb.unsqueeze(dim=0)\n",
    "x = a\n",
    "similarities = 1 - cdist(x, Y, metric='cosine')\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(a) * torch.nn.functional.normalize(b).mean(axis=0)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((a * b).sum(dim=1) / (torch.norm(a, p=2, dim=1) * torch.norm(b, p=2, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.nn.functional.normalize(a) *torch.nn.functional.normalize(b)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(a, p=2, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((a * b.mean(axis=0)).sum(dim=1) / (torch.norm(a, p=2, dim=1) * torch.norm(b, p=2, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_a = torch.split(a, a.shape[0] // 2)\n",
    "gradients_b = torch.split(b, b.shape[0] // 2)\n",
    "\n",
    "\n",
    "results = []\n",
    "for a in gradients_a\n",
    "((a * b).sum(dim=1) / (torch.norm(a, p=2, dim=1) * torch.norm(b, p=2, dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"loris3/stratified_equitoken_10m_curriculum_random\"\n",
    "\n",
    "\n",
    "dataset_train_name =\"loris3/stratified_equitoken_10m_curriculum\"\n",
    "dataset_train_split_name = \"validation\"\n",
    "\n",
    "dataset_test_name = \"loris3/stratified_equitoken_10m_curriculum\"\n",
    "dataset_test_split_name = \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "dataset_test = load_dataset(dataset_test_name)[dataset_test_split_name] \n",
    "\n",
    "\n",
    "\n",
    "len_ds = len(dataset_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests on wether implementaiton of\n",
    "\n",
    "$\n",
    "        \\phi(f;z,z') = \\frac{\\sum_{\\forall z' \\in D_{test}} \\nabla \\ell(z) \\cdot \\nabla \\ell(z')}{ |D_{test}|}\n",
    "$\n",
    "works out with sufficent accuracy via taking the mean of the test gradients first\n",
    "$\n",
    "      = \\frac{1}{ |D_{test}|}\\cdot(\\nabla \\ell(z) \\cdot\\sum_{\\forall z' \\in D_{test}} \\nabla \\ell(z'))\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extract_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_split=validation[0%:10%] --paradigm=mlm --gradients_per_file=1000 --mode=store\n",
    "%run extract_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_split=validation[0%:10%] --paradigm=mlm --gradients_per_file=1000 --mode=store_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_mean_extraction():\n",
    "    cp = \"/data/loriss21dm/babylm/gradients/stratified_equitoken_10m_curriculum_random/stratified_equitoken_10m_curriculum/validation[0%:10%]/checkpoint-6174/\"\n",
    "    paths = os.listdir(cp)\n",
    "    mean_pw = torch.cat([torch.load(os.path.join(cp,p),weights_only=True) for p in paths if p != \"mean\"],axis=0).mean(axis=0, dtype=torch.float64)\n",
    "    mean_script = torch.load(\"/data/loriss21dm/babylm/gradients/stratified_equitoken_10m_curriculum_random/stratified_equitoken_10m_curriculum/validation[0%:10%]/checkpoint-6174/mean\", weights_only=True)\n",
    "    assert torch.cosine_similarity(mean_pw, mean_script).mean().float() == torch.tensor(1.0).float()\n",
    "    assert torch.allclose(mean_pw, mean_script, atol=0.000001)\n",
    "validate_mean_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run extract_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_split=validation[10%:15%] --paradigm=mlm --gradients_per_file=1000 --mode=store\n",
    "%run extract_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_split=validation[10%:15%] --paradigm=mlm --gradients_per_file=1000 --mode=store_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_train_split=validation[0%:10%] --dataset_test=\"loris3/stratified_equitoken_10m_curriculum\" --dataset_test_split=validation[10%:15%] --mode=single --gradients_per_file=1000 --batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_single = torch.load(\"/data/loriss21dm/babylm/influence/stratified_equitoken_10m_curriculum_random/stratified_equitoken_10m_curriculum_validation[0%:10%]_stratified_equitoken_10m_curriculum_validation[10%:15%]/checkpoint-6174\", weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_gradients.py loris3/stratified_equitoken_10m_curriculum_random loris3/stratified_equitoken_10m_curriculum 0 --dataset_train_split=validation[0%:10%] --dataset_test=\"loris3/stratified_equitoken_10m_curriculum\" --dataset_test_split=validation[10%:15%] --mode=mean --gradients_per_file=1000 --batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_mean = torch.load(\"/data/loriss21dm/babylm/mean_influence/stratified_equitoken_10m_curriculum_random/stratified_equitoken_10m_curriculum_validation[0%:10%]_stratified_equitoken_10m_curriculum_validation[10%:15%]/checkpoint-6174\", weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(influence_single.mean(-1),influence_mean.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = (torch.cosine_similarity(influence_single.mean(-1), influence_mean.squeeze())).unsqueeze(1)\n",
    "\n",
    "\n",
    "array = tensor.numpy()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(array, cmap='viridis', aspect='auto', interpolation=None)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests of the two modes \"single\" and \"mean\" against eachother for getting the mean influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_debug(start, stop):\n",
    "    return torch.arange(start*393216, stop*393216, 1, dtype=torch.float64).reshape(-1,393216) /  (len_ds*393216)\n",
    "\n",
    "gradient_dir = \"./gradients/test/test/test/test\"\n",
    "if not os.path.exists(gradient_dir):\n",
    "    os.makedirs(gradient_dir)\n",
    "    chunks_test = [ (i, min(i+10000, len_ds), os.path.join(gradient_dir, str(i) + \"_\" + str(i + 10000))) for i in range(0, len(dataset_test),10000)]\n",
    "    for start, stop, chunk in chunks_test:\n",
    "        torch.save(load_debug(start, stop), chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_train_train():\n",
    "    s = None\n",
    "    if not os.path.exists(\"test\"):\n",
    "        data = load_debug(0, len_ds).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            s = torch.matmul(data, data.T).sum(dim=1)\n",
    "        s = s / len_ds\n",
    "        s = s.unsqueeze(0)\n",
    "        torch.save(s, \"test\")\n",
    "    else:\n",
    "        s = torch.load(\"test\")\n",
    "\n",
    "    slurm = torch.load(\"/data/loriss21dm/babylm/mean_influence/test/test_test_test_test/test\")\n",
    "    assert torch.allclose(slurm, s.float())\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    tensor = (s / slurm).float()\n",
    "\n",
    "\n",
    "    array = tensor.numpy()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.imshow(array, cmap='viridis', aspect='auto', interpolation=None)\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_gradients.py test test 0 --mode=mean --dataset_test_split=test --dataset_train_split=test --test=True --test_dataset_size=53457 --gradients_per_file=10000 --batch_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_train_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_gradients.py test test 0 --mode=mean --dataset_test_split=test --dataset_train_split=test --test=True --test_dataset_size=53457 --gradients_per_file=10000 --batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_train_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run process_gradients.py test test 0 --mode=mean --dataset_test_split=test --dataset_train_split=test --test=True --test_dataset_size=53457 --gradients_per_file=10000 --batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_train_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
