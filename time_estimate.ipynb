{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset_folder\": \"./train_10M\",\n",
    "    \"gradients_per_file\" : 10000,\n",
    "    \"epochs\" : 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_in_h = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/loriss21dm/TracInVenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./TestModel\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "t = lambda x : tokenizer(x[\"text\"], return_special_tokens_mask=True, truncation=True, max_length=512)\n",
    "\n",
    "dataset = datasets.load_from_disk(\"curricula/datasets/test_eval\")\n",
    "dataset.set_transform(t)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "data_collator = util.DeterministicDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "data_collator.set_epoch(1000)\n",
    "# https://github.com/ayoolaolafenwa/TrainNLP\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "\n",
    "\n",
    "eval_dataset = dataset.map(insert_random_mask,batched=True,\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.rename_columns({\"masked_input_ids\": \"input_ids\",\n",
    "\"masked_attention_mask\": \"attention_mask\",\"masked_labels\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 117\n",
       "})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 117\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dataset.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"num_processes\"] = 4\n",
    "TIME_PER_BATCH_IN_MIN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.650233333333333 hours\n"
     ]
    }
   ],
   "source": [
    "time_if = ((((len(dataset[\"train\"]) / args[\"gradients_per_file\"]) *args[\"epochs\"]) /args[\"num_processes\"]) * TIME_PER_BATCH_IN_MIN)/60\n",
    "print(time_if, \"hours\")\n",
    "total_in_h+=time_if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_IN_GB_OF_CHUNK = 7.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.7247036 TB disk space\n"
     ]
    }
   ],
   "source": [
    "print((len(dataset[\"train\"]) / args[\"gradients_per_file\"]) *args[\"epochs\"] * SIZE_IN_GB_OF_CHUNK/1000, \"TB disk space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args[\"batch_size\"] = 20\n",
    "args[\"num_processes\"] = 4\n",
    "\n",
    "SIZE_IN_GB_OF_CHUNK = 7.4 # ls\n",
    "time_single_chunk = 15.36 # get from log\n",
    "time_load_task = 110 # get from log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM requirements 621.6 GB\n"
     ]
    }
   ],
   "source": [
    "print(\"RAM requirements\", (args[\"batch_size\"]+1)*args[\"num_processes\"]*SIZE_IN_GB_OF_CHUNK, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for a single epoch and one process 3.1456183649070217 hours\n",
      "Total time with multiprocessing 3.9320229561337774 hours\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_chunks =(((len(dataset[\"train\"]) / args[\"gradients_per_file\"])))\n",
    "num_batches = num_chunks / args[\"batch_size\"]\n",
    "\n",
    "time_single_batch = (num_chunks * time_single_chunk) + time_load_task\n",
    "time_single_epoch = (num_batches * time_single_batch) \n",
    "print(\"Time for a single epoch and one process\", time_single_epoch /60/60, \"hours\")\n",
    "total_merge = (time_single_epoch * args[\"epochs\"] /args[\"num_processes\"])/60/60\n",
    "print(\"Total time with multiprocessing\", total_merge, \"hours\")\n",
    "total_in_h += total_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.757139622800445 hours\n"
     ]
    }
   ],
   "source": [
    "print(total_in_h, \"hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
